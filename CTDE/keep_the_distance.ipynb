{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep average distance\n",
    "\n",
    "the agents goal is to position close to each others at a distance previously defined\n",
    "\n",
    "challenges:\n",
    "- deal with continuous space environment\n",
    "- limited vision of an agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.vectors import Vector2D\n",
    "from utils.canvas import CanvasWithBorders\n",
    "from utils.algo_utils import (save_algo, load_algo)\n",
    "from utils.simulations import (simulate_episode, simulate_random_episode, ppo_result_format)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### environment definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "import random as rnd\n",
    "from gymnasium.spaces import Discrete, Box, Dict, Tuple, MultiDiscrete\n",
    "from gymnasium.spaces.utils import flatten, flatten_space\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import math\n",
    "from ipycanvas import Canvas, hold_canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvironmentConfiguration: \n",
    "    def __init__(self, n_agents, target_distance, speed, spawn_area=100, visible_nbrs=1, max_steps=None, spawn_area_schedule=None):\n",
    "        self.n_agents = n_agents\n",
    "        self.visible_nbrs = visible_nbrs\n",
    "        self.target_distance = target_distance\n",
    "        self.max_steps = max_steps\n",
    "        self.speed = speed\n",
    "        self.spawn_area = spawn_area\n",
    "        self.spawn_area_schedule = spawn_area_schedule\n",
    "\n",
    "class KeepTheDistance(MultiAgentEnv):\n",
    "\n",
    "    canvas = None\n",
    "    CANVAS_WIDTH, CANVAS_HEIGHT = 300.0, 300.0\n",
    "\n",
    "    def __init__(self, config: EnvironmentConfiguration):\n",
    "        assert config.n_agents > config.visible_nbrs # just base case implemented \n",
    "             \n",
    "        self.n_agents = config.n_agents\n",
    "        self.visible_nbrs = config.visible_nbrs\n",
    "        self.target_distance = config.target_distance\n",
    "        self.max_steps = config.max_steps\n",
    "        self.speed = config.speed\n",
    "        self.spawn_area = config.spawn_area\n",
    "        self.spawn_area_schedule = config.spawn_area_schedule\n",
    "        if self.spawn_area_schedule != None:\n",
    "            self.spawn_area_schedule_index = 0\n",
    "            self.n_reset = 0\n",
    "            self.spawn_area = self.spawn_area_schedule[0][1]\n",
    "        \n",
    "        self.agents_ids = ['agent-' + str(i) for i in range(self.n_agents)]\n",
    "        self.agent_colors = {agent: self.rgb_to_hex(rnd.randint(0, 255), rnd.randint(0, 255), rnd.randint(0, 255)) for agent in self.agents_ids}\n",
    "        self.observation_space = self.observation_space('agent-0')\n",
    "        self.action_space = self.action_space(\"\")\n",
    "\n",
    "    def unflatten_observation_space(self, agent):\n",
    "        #distance_vector = Box(low=-np.inf, high=np.inf, shape=(2,1), dtype=np.float32)\n",
    "        #obs_space = Dict({\"nbr-1\": distance_vector})\n",
    "        direction = Box(low=-1, high=1, shape=(2,1), dtype=np.float32)\n",
    "        distance = Box(low=-np.inf, high=np.inf, shape=(1,1), dtype=np.float32)\n",
    "        return Dict({f\"nbr-{i}\": Dict({'direction': direction, 'distance': distance}) for i in range(self.visible_nbrs)})\n",
    "\n",
    "    def observation_space(self, agent):\n",
    "        return flatten_space(self.unflatten_observation_space(agent))\n",
    "\n",
    "    def action_space(self, agent):\n",
    "        direction = Box(low=-1.0, high=1.0, shape=(2,1), dtype=np.float32)\n",
    "        speed = Box(0.0, 1.0, dtype=np.float32)\n",
    "        return flatten_space(Tuple([direction, speed]))\n",
    "    \n",
    "    def __get_observation(self, agent):\n",
    "        distance_vectors = [Vector2D.distance_vector(self.agents_pos[agent], self.agents_pos[nbr])  \n",
    "                            for nbr in self.__get_n_closest_neighbours(agent, self.visible_nbrs)]\n",
    "\n",
    "        obs = {\n",
    "            f\"nbr-{i}\": {\n",
    "                \"direction\": Vector2D.unit_vector(distance_vectors[i]).to_np_array(),\n",
    "                \"distance\": np.log(1 + Vector2D.norm(distance_vectors[i])) #1 - np.exp(-alpha * x)\n",
    "            }\n",
    "            for i in range(len(distance_vectors))\n",
    "            }\n",
    "        return flatten(self.unflatten_observation_space(agent), obs)\n",
    "\n",
    "    def rgb_to_hex(self, r, g, b):\n",
    "        return f'#{r:02x}{g:02x}{b:02x}'\n",
    "\n",
    "    def __total_distance_from_closest_neighbours(self, agent):\n",
    "        return sum([abs(Vector2D.distance(self.agents_pos[agent], self.agents_pos[nbr]) - self.target_distance) for nbr in self.__get_n_closest_neighbours(agent, self.visible_nbrs)])\n",
    "\n",
    "    def __get_local_reward(self, agent, action):\n",
    "        last_action = self.last_actions[agent]\n",
    "        self.last_actions[agent] = action\n",
    "\n",
    "        # r0: negative of the distance from the closest neighbours\n",
    "        reward_0 = -self.__total_distance_from_closest_neighbours(agent)\n",
    "\n",
    "        # r1: improvement of the distance from the closest neighbours\n",
    "        newDistance = self.__total_distance_from_closest_neighbours(agent)\n",
    "        reward_1 = self.last_step_distances[agent] - newDistance\n",
    "        self.last_step_distances[agent] = newDistance\n",
    "\n",
    "        worst_possible_reward_1 = self.speed * (self.visible_nbrs + 1) # I move away from my nbrs and my nbrs are moving away from me at a speed = speed\n",
    "        reward_1_normalized = reward_1 / worst_possible_reward_1 # [-1,+1] \n",
    "\n",
    "        # r2: bonus if the agent is very close to the target distance\n",
    "        closest_nbrs = self.__get_n_closest_neighbours(agent, self.visible_nbrs)\n",
    "        #reward_2 = sum([100 if abs(Vector2D.distance(self.agents_pos[agent], self.agents_pos[nbr]) - self.target_distance) < 0.5 else 0 for nbr in closest_nbrs])\n",
    "        reward_2 = sum([max(0, 1 - (abs(Vector2D.distance(self.agents_pos[agent], self.agents_pos[nbr]) - self.target_distance))) for nbr in closest_nbrs])\n",
    "\n",
    "        # r5: penalize when the distance between two agents is lower than the target one\n",
    "        if self.target_distance > 0:\n",
    "            reward_5 = sum([min(0, (Vector2D.distance(self.agents_pos[agent], self.agents_pos[nbr]) - self.target_distance)/self.target_distance) for nbr in closest_nbrs])\n",
    "        else:\n",
    "            reward_5 = 0\n",
    "\n",
    "        # r3: penalize rapid changes of direction\n",
    "        reward_3 = -(1-Vector2D.similarity(Vector2D(action[0],action[1]), Vector2D(last_action[0],last_action[1])))\n",
    "\n",
    "        #reward_4 = -action[2]*10\n",
    "        return reward_1 + reward_2 + reward_5 # + reward_3 #+ reward_3# working for two agents using value for reward_2 equals to one\n",
    "\n",
    "    def __get_global_reward(self):\n",
    "        return 0\n",
    "    \n",
    "    def __get_other_agents(self, agent):\n",
    "        return [other for other in self.agents_ids if other != agent]\n",
    "\n",
    "    def __get_n_closest_neighbours(self, agent, n=1):\n",
    "        distances = {other: Vector2D.distance(self.agents_pos[agent], self.agents_pos[other]) for other in self.__get_other_agents(agent)}\n",
    "        return [neighbour[0] for neighbour in sorted(list(distances.items()), key=lambda d: d[1])[:n]]\n",
    "        # return {neighbour[0]: neighbour[1] for neighbour in sorted(list(dst.items()), key=lambda d: d[0])[:n]}\n",
    "\n",
    "    def __update_agent_position(self, agent, action):\n",
    "        unit_movement = Vector2D(action[0], action[1])\n",
    "        self.agents_pos[agent] = Vector2D.sum(self.agents_pos[agent], Vector2D.mul(unit_movement, action[2]*self.speed))\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        if self.spawn_area_schedule != None:\n",
    "            self.n_reset += 1\n",
    "            if (self.spawn_area_schedule_index < len(self.spawn_area_schedule)-1 and \n",
    "                self.n_reset >= self.spawn_area_schedule[self.spawn_area_schedule_index+1][0]):\n",
    "                self.spawn_area_schedule_index += 1\n",
    "                self.spawn_area = self.spawn_area_schedule[self.spawn_area_schedule_index][1]\n",
    "\n",
    "        self.steps = 0\n",
    "        self.agents_pos = {agent: Vector2D.get_random_point(max_x=self.spawn_area, max_y=self.spawn_area) for agent in self.agents_ids}\n",
    "        self.last_step_distances = {agent: self.__total_distance_from_closest_neighbours(agent) for agent in self.agents_ids}\n",
    "        self.last_actions = {agent: [0]*3 for agent in self.agents_ids}\n",
    "        return {agent: self.__get_observation(agent) for agent in self.agents_ids}, {}\n",
    "     \n",
    "    def step(self, actions):\n",
    "        self.steps += 1\n",
    "        observations, rewards, terminated, truncated, infos = {}, {}, {}, {}, {}\n",
    "\n",
    "        for agent, action in actions.items():\n",
    "            self.__update_agent_position(agent, action)\n",
    "\n",
    "        for agent, action in actions.items():\n",
    "            observations[agent] = self.__get_observation(agent)\n",
    "            rewards[agent] = self.__get_local_reward(agent, action) + self.__get_global_reward()\n",
    "            terminated[agent] = False\n",
    "            truncated[agent] = False\n",
    "            infos[agent] = {}\n",
    "\n",
    "        truncated['__all__'] = False\n",
    "        if self.max_steps != None and self.steps == self.max_steps:\n",
    "            terminated['__all__'] = True\n",
    "        else:\n",
    "            terminated['__all__'] = False\n",
    "\n",
    "        return observations, rewards, terminated, truncated, infos\n",
    "     \n",
    "    def rgb_to_hex(self, r, g, b):\n",
    "        return f'#{r:02x}{g:02x}{b:02x}'\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    def get_agent_ids(self):\n",
    "       return self.agents\n",
    "\n",
    "\n",
    "class RenderableKeepTheDistance(KeepTheDistance):\n",
    "    def render(self):\n",
    "        if self.canvas is None:\n",
    "            self.canvas = CanvasWithBorders(width=self.CANVAS_WIDTH, height=self.CANVAS_HEIGHT)\n",
    "            display(self.canvas)\n",
    "        \n",
    "        with hold_canvas():\n",
    "            agent_size = max(self.CANVAS_WIDTH/float(self.spawn_area),1)\n",
    "            target_distance_size = (self.CANVAS_WIDTH/float(self.spawn_area))*self.target_distance\n",
    "            top_left = (0.0,0.0)\n",
    "            bottom_right = (self.spawn_area, self.spawn_area)\n",
    "            self.canvas.clear()\n",
    "\n",
    "            for agent in self.agents_ids:\n",
    "                raw_pos = self.agents_pos[agent].to_np_array()\n",
    "                color = self.agent_colors[agent]\n",
    "                \n",
    "                agent_pos_in_frame = [((raw_pos[0]-top_left[0])/(bottom_right[0]-top_left[0]))*self.CANVAS_WIDTH,\n",
    "                            ((raw_pos[1]-top_left[1])/(bottom_right[1]-top_left[1]))*self.CANVAS_HEIGHT,]\n",
    "\n",
    "                self.canvas.fill_style = color\n",
    "                self.canvas.fill_circle(\n",
    "                    agent_pos_in_frame[0],\n",
    "                    agent_pos_in_frame[1],\n",
    "                    agent_size/2.0\n",
    "                )\n",
    "                \n",
    "                self.canvas.stroke_style = \"black\"\n",
    "                self.canvas.stroke_circle(\n",
    "                    agent_pos_in_frame[0],\n",
    "                    agent_pos_in_frame[1],\n",
    "                    agent_size/2.0\n",
    "                )\n",
    "\n",
    "                if self.target_distance > 0:\n",
    "                    self.canvas.stroke_style = \"red\"\n",
    "                    self.canvas.stroke_circle(\n",
    "                        agent_pos_in_frame[0],\n",
    "                        agent_pos_in_frame[1],\n",
    "                        target_distance_size/2.0\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent-0': array([0.37139067, 0.9284767 , 1.8539773 ], dtype=float32), 'agent-1': array([-0.37139067, -0.9284767 ,  1.8539773 ], dtype=float32)}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2877a9e0041647afa2ac117963958dd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CanvasWithBorders(height=300, width=300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env_config = EnvironmentConfiguration(n_agents=2, target_distance=0, max_steps=500, speed=1, spawn_area=10)\n",
    "env = RenderableKeepTheDistance(env_config)\n",
    "\n",
    "print(env.reset()[0])\n",
    "#env.render()\n",
    "simulate_random_episode(env, 10, print_info=False)\n",
    "#env.step({'agent-1': (1,1,1)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## policy training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KeepTheDistance?dst=0&agent=2&spawn_area=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "\n",
    "env_config = EnvironmentConfiguration(n_agents=2, target_distance=0, max_steps=300, speed=1, spawn_area=100)\n",
    "register_env(\"KeepTheDistance?dst=0&agent=2&spawn_area=100\", lambda _: KeepTheDistance(env_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 13:32:36,042\tWARNING deprecation.py:50 -- DeprecationWarning: `_enable_new_api_stack` has been deprecated. Use `AlgorithmConfig._enable_new_api_stack` instead. This will raise an error in the future!\n",
      "/home/nicolo/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:521: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/nicolo/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/nicolo/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/nicolo/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2024-06-24 13:32:36,103\tWARNING deprecation.py:50 -- DeprecationWarning: `WorkerSet(num_workers=... OR local_worker=...)` has been deprecated. Use `EnvRunnerGroup(num_env_runners=... AND local_env_runner=...)` instead. This will raise an error in the future!\n",
      "2024-06-24 13:32:36,103\tWARNING deprecation.py:50 -- DeprecationWarning: `max_num_worker_restarts` has been deprecated. Use `AlgorithmConfig.max_num_env_runner_restarts` instead. This will raise an error in the future!\n",
      "2024-06-24 13:32:39,201\tINFO worker.py:1749 -- Started a local Ray instance.\n",
      "2024-06-24 13:32:49,554\tINFO trainable.py:161 -- Trainable.setup took 13.451 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2024-06-24 13:32:49,556\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "algo = load_algo(\"KeepTheDistance?dst=0&agent=2&spawn_area=100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration [1] => episode_reward_mean: -8.2491505251911, episode_len_mean: 300.0, agent_steps_trained: 8192, env_steps_trained: 4096, entropy: 4.260479648411274, learning_rate: 0.0010000000000000005\n",
      "iteration [2] => episode_reward_mean: 26.74030357421306, episode_len_mean: 300.0, agent_steps_trained: 16384, env_steps_trained: 8192, entropy: 4.282991732160251, learning_rate: 0.0010000000000000005\n",
      "iteration [3] => episode_reward_mean: 47.404754154425326, episode_len_mean: 300.0, agent_steps_trained: 24576, env_steps_trained: 12288, entropy: 4.188545287897189, learning_rate: 0.0010000000000000005\n",
      "iteration [4] => episode_reward_mean: 61.9642279714821, episode_len_mean: 300.0, agent_steps_trained: 32768, env_steps_trained: 16384, entropy: 4.28954379533728, learning_rate: 0.0010000000000000005\n",
      "iteration [5] => episode_reward_mean: 73.0602125192637, episode_len_mean: 300.0, agent_steps_trained: 40960, env_steps_trained: 20480, entropy: 4.296951741228501, learning_rate: 0.0010000000000000005\n",
      "iteration [6] => episode_reward_mean: 83.6019659784346, episode_len_mean: 300.0, agent_steps_trained: 49152, env_steps_trained: 24576, entropy: 4.224251094460487, learning_rate: 0.0010000000000000005\n",
      "iteration [7] => episode_reward_mean: 95.12385056444383, episode_len_mean: 300.0, agent_steps_trained: 57344, env_steps_trained: 28672, entropy: 4.176411455869674, learning_rate: 0.0010000000000000005\n",
      "iteration [8] => episode_reward_mean: 112.53530194446253, episode_len_mean: 300.0, agent_steps_trained: 65536, env_steps_trained: 32768, entropy: 4.105881314973036, learning_rate: 0.0010000000000000005\n",
      "iteration [9] => episode_reward_mean: 127.52346210013815, episode_len_mean: 300.0, agent_steps_trained: 73728, env_steps_trained: 36864, entropy: 4.109167362749576, learning_rate: 0.0010000000000000005\n",
      "iteration [10] => episode_reward_mean: 140.98805518626784, episode_len_mean: 300.0, agent_steps_trained: 81920, env_steps_trained: 40960, entropy: 4.024059564371904, learning_rate: 0.0010000000000000005\n",
      "iteration [11] => episode_reward_mean: 154.297613436139, episode_len_mean: 300.0, agent_steps_trained: 90112, env_steps_trained: 45056, entropy: 4.009137526651224, learning_rate: 0.0010000000000000005\n",
      "iteration [12] => episode_reward_mean: 165.7035849723156, episode_len_mean: 300.0, agent_steps_trained: 98304, env_steps_trained: 49152, entropy: 3.9784682005643845, learning_rate: 0.0010000000000000005\n",
      "iteration [13] => episode_reward_mean: 174.37532050825416, episode_len_mean: 300.0, agent_steps_trained: 106496, env_steps_trained: 53248, entropy: 3.9138338011999925, learning_rate: 0.0010000000000000005\n",
      "iteration [14] => episode_reward_mean: 181.30241480844444, episode_len_mean: 300.0, agent_steps_trained: 114688, env_steps_trained: 57344, entropy: 3.912434645742178, learning_rate: 0.0010000000000000005\n",
      "iteration [15] => episode_reward_mean: 191.5792047513965, episode_len_mean: 300.0, agent_steps_trained: 122880, env_steps_trained: 61440, entropy: 3.8528323367238047, learning_rate: 0.0010000000000000005\n",
      "iteration [16] => episode_reward_mean: 203.33632575991228, episode_len_mean: 300.0, agent_steps_trained: 131072, env_steps_trained: 65536, entropy: 3.8606174123783905, learning_rate: 0.0010000000000000005\n",
      "iteration [17] => episode_reward_mean: 213.5774174893999, episode_len_mean: 300.0, agent_steps_trained: 139264, env_steps_trained: 69632, entropy: 3.804281808435917, learning_rate: 0.0010000000000000005\n",
      "iteration [18] => episode_reward_mean: 223.98664739834356, episode_len_mean: 300.0, agent_steps_trained: 147456, env_steps_trained: 73728, entropy: 3.7871425792574884, learning_rate: 0.0010000000000000005\n",
      "iteration [19] => episode_reward_mean: 232.72368147100525, episode_len_mean: 300.0, agent_steps_trained: 155648, env_steps_trained: 77824, entropy: 3.8307312014202277, learning_rate: 0.0010000000000000005\n",
      "iteration [20] => episode_reward_mean: 244.16480983564244, episode_len_mean: 300.0, agent_steps_trained: 163840, env_steps_trained: 81920, entropy: 3.750966937839985, learning_rate: 0.0010000000000000005\n",
      "iteration [21] => episode_reward_mean: 251.0515715100132, episode_len_mean: 300.0, agent_steps_trained: 172032, env_steps_trained: 86016, entropy: 3.7542345715065797, learning_rate: 0.0010000000000000005\n",
      "iteration [22] => episode_reward_mean: 258.7686113477273, episode_len_mean: 300.0, agent_steps_trained: 180224, env_steps_trained: 90112, entropy: 3.7623036896189053, learning_rate: 0.0010000000000000005\n",
      "iteration [23] => episode_reward_mean: 262.1224849266892, episode_len_mean: 300.0, agent_steps_trained: 188416, env_steps_trained: 94208, entropy: 3.764138208826383, learning_rate: 0.0010000000000000005\n",
      "iteration [24] => episode_reward_mean: 265.3873132690426, episode_len_mean: 300.0, agent_steps_trained: 196608, env_steps_trained: 98304, entropy: 3.7438347304860753, learning_rate: 0.0010000000000000005\n",
      "iteration [25] => episode_reward_mean: 270.40405874820704, episode_len_mean: 300.0, agent_steps_trained: 204800, env_steps_trained: 102400, entropy: 3.6615439449747402, learning_rate: 0.0010000000000000005\n",
      "iteration [26] => episode_reward_mean: 276.86704342735084, episode_len_mean: 300.0, agent_steps_trained: 212992, env_steps_trained: 106496, entropy: 3.7221683762967586, learning_rate: 0.0010000000000000005\n",
      "iteration [27] => episode_reward_mean: 280.89426372365443, episode_len_mean: 300.0, agent_steps_trained: 221184, env_steps_trained: 110592, entropy: 3.7527961378296215, learning_rate: 0.0010000000000000005\n",
      "iteration [28] => episode_reward_mean: 283.7973033560783, episode_len_mean: 300.0, agent_steps_trained: 229376, env_steps_trained: 114688, entropy: 3.714443445702394, learning_rate: 0.0010000000000000005\n",
      "iteration [29] => episode_reward_mean: 290.1478247673741, episode_len_mean: 300.0, agent_steps_trained: 237568, env_steps_trained: 118784, entropy: 3.649320044865211, learning_rate: 0.0010000000000000005\n",
      "iteration [30] => episode_reward_mean: 295.590107097121, episode_len_mean: 300.0, agent_steps_trained: 245760, env_steps_trained: 122880, entropy: 3.6460072847704095, learning_rate: 0.0010000000000000005\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbe0405783e14284b6be5a1cb773d0c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CanvasWithBorders(height=300, width=300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs:  {'agent-0': array([-0.48564294,  0.87415725,  4.1395373 ], dtype=float32), 'agent-1': array([ 0.48564294, -0.87415725,  4.1395373 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.       , -1.       ,  0.5872913], dtype=float32), 'agent-1': array([-0.43972194,  0.21780336,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.2004210590646451, 'agent-1': 1.2004210590646451} \n",
      "\n",
      "obs:  {'agent-0': array([-0.47831237,  0.8781898 ,  4.1202292 ], dtype=float32), 'agent-1': array([ 0.47831237, -0.8781898 ,  4.1202292 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.       , -0.7112257,  0.5463229], dtype=float32), 'agent-1': array([-0.9660263 ,  1.        ,  0.54831016], dtype=float32)}\n",
      "reward:  {'agent-0': 1.3353318421749094, 'agent-1': 1.3353318421749094} \n",
      "\n",
      "obs:  {'agent-0': array([-0.47093028,  0.88217044,  4.098304  ], dtype=float32), 'agent-1': array([ 0.47093028, -0.88217044,  4.098304  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.       , -0.9942838,  0.9115002], dtype=float32), 'agent-1': array([-0.940517 , -1.       ,  0.7365478], dtype=float32)}\n",
      "reward:  {'agent-0': 0.8899441509801704, 'agent-1': 0.8899441509801704} \n",
      "\n",
      "obs:  {'agent-0': array([-0.45061883,  0.89271647,  4.08342   ], dtype=float32), 'agent-1': array([ 0.45061883, -0.89271647,  4.08342   ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -1.        ,  0.72056466], dtype=float32), 'agent-1': array([-0.23176408,  0.6702969 ,  0.7057139 ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.463395869785046, 'agent-1': 1.463395869785046} \n",
      "\n",
      "obs:  {'agent-0': array([-0.44666892,  0.89469934,  4.058453  ], dtype=float32), 'agent-1': array([ 0.44666892, -0.89469934,  4.058453  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.98155093, -0.5273556 ,  0.8488622 ], dtype=float32), 'agent-1': array([0.74215865, 0.7853645 , 1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.1418697195326075, 'agent-1': 1.1418697195326075} \n",
      "\n",
      "obs:  {'agent-0': array([-0.4541855 ,  0.89090717,  4.038529  ], dtype=float32), 'agent-1': array([ 0.4541855 , -0.89090717,  4.038529  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.58604944, -0.7935844 ,  1.        ], dtype=float32), 'agent-1': array([-1.        , -0.03885823,  0.6319115 ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.2333475561386535, 'agent-1': 1.2333475561386535} \n",
      "\n",
      "obs:  {'agent-0': array([-0.442118 ,  0.8969569,  4.016554 ], dtype=float32), 'agent-1': array([ 0.442118 , -0.8969569,  4.016554 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.7679327 ,  0.59012634], dtype=float32), 'agent-1': array([-1.       ,  1.       ,  0.5566617], dtype=float32)}\n",
      "reward:  {'agent-0': 1.4096064839793883, 'agent-1': 1.4096064839793883} \n",
      "\n",
      "obs:  {'agent-0': array([-0.4322578,  0.9017501,  3.9908316], dtype=float32), 'agent-1': array([ 0.4322578, -0.9017501,  3.9908316], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.28188032, -0.37951082,  0.54502684], dtype=float32), 'agent-1': array([-0.73791856,  1.        ,  0.92800355], dtype=float32)}\n",
      "reward:  {'agent-0': 1.2529446834595106, 'agent-1': 1.2529446834595106} \n",
      "\n",
      "obs:  {'agent-0': array([-0.4324591,  0.9016535,  3.9673994], dtype=float32), 'agent-1': array([ 0.4324591, -0.9016535,  3.9673994], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.52240276, -0.3506841 ,  0.79974973], dtype=float32), 'agent-1': array([-1.,  1.,  1.], dtype=float32)}\n",
      "reward:  {'agent-0': 1.7624256694149665, 'agent-1': 1.7624256694149665} \n",
      "\n",
      "obs:  {'agent-0': array([-0.41936892,  0.9078159 ,  3.933481  ], dtype=float32), 'agent-1': array([ 0.41936892, -0.9078159 ,  3.933481  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.30255103, -0.06009245,  0.80895066], dtype=float32), 'agent-1': array([0.29671192, 1.        , 0.8633919 ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.8215717925070791, 'agent-1': 0.8215717925070791} \n",
      "\n",
      "obs:  {'agent-0': array([-0.42659488,  0.90444285,  3.9172676 ], dtype=float32), 'agent-1': array([ 0.42659488, -0.90444285,  3.9172676 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.68519855, 0.05688906, 0.4129481 ], dtype=float32), 'agent-1': array([-1.       ,  1.       ,  0.8770632], dtype=float32)}\n",
      "reward:  {'agent-0': 1.2619739985645495, 'agent-1': 1.2619739985645495} \n",
      "\n",
      "obs:  {'agent-0': array([-0.4136438,  0.9104388,  3.8918397], dtype=float32), 'agent-1': array([ 0.4136438, -0.9104388,  3.8918397], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.       , -1.       ,  0.4348044], dtype=float32), 'agent-1': array([-0.9707023 ,  0.19715858,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.1456778997329096, 'agent-1': 1.1456778997329096} \n",
      "\n",
      "obs:  {'agent-0': array([-0.39376122,  0.91921276,  3.8681812 ], dtype=float32), 'agent-1': array([ 0.39376122, -0.91921276,  3.8681812 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.6015068 ,  0.69748557], dtype=float32), 'agent-1': array([-0.4029867 ,  0.60018504,  0.7201385 ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.1682749450473366, 'agent-1': 1.1682749450473366} \n",
      "\n",
      "obs:  {'agent-0': array([-0.38221154,  0.9240748 ,  3.8434656 ], dtype=float32), 'agent-1': array([ 0.38221154, -0.9240748 ,  3.8434656 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.15404725, -0.2961796 ,  0.84508276], dtype=float32), 'agent-1': array([-0.7864614 ,  0.20116246,  0.16821635], dtype=float32)}\n",
      "reward:  {'agent-0': 0.2632444991681311, 'agent-1': 0.2632444991681311} \n",
      "\n",
      "obs:  {'agent-0': array([-0.38438004,  0.9231749 ,  3.8378112 ], dtype=float32), 'agent-1': array([ 0.38438004, -0.9231749 ,  3.8378112 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -1.        ,  0.51278913], dtype=float32), 'agent-1': array([-1.        , -0.29818034,  0.8157855 ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.7453950708241095, 'agent-1': 0.7453950708241095} \n",
      "\n",
      "obs:  {'agent-0': array([-0.36105648,  0.93254393,  3.8216245 ], dtype=float32), 'agent-1': array([ 0.36105648, -0.93254393,  3.8216245 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.60695314, -0.8283529 ,  0.88658214], dtype=float32), 'agent-1': array([-1.        ,  0.92930484,  0.97471625], dtype=float32)}\n",
      "reward:  {'agent-0': 2.0679229819077207, 'agent-1': 2.0679229819077207} \n",
      "\n",
      "obs:  {'agent-0': array([-0.34307513,  0.939308  ,  3.7752964 ], dtype=float32), 'agent-1': array([ 0.34307513, -0.939308  ,  3.7752964 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.17108381, 0.23193479, 0.68017995], dtype=float32), 'agent-1': array([-1.,  1.,  1.], dtype=float32)}\n",
      "reward:  {'agent-0': 1.1671602726462993, 'agent-1': 1.1671602726462993} \n",
      "\n",
      "obs:  {'agent-0': array([-0.32579982,  0.94543874,  3.7481685 ], dtype=float32), 'agent-1': array([ 0.32579982, -0.94543874,  3.7481685 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.2508071 , -0.9916171 ,  0.59387755], dtype=float32), 'agent-1': array([-1.,  1.,  1.], dtype=float32)}\n",
      "reward:  {'agent-0': 1.7784412139557944, 'agent-1': 1.7784412139557944} \n",
      "\n",
      "obs:  {'agent-0': array([-0.31895158,  0.947771  ,  3.7053635 ], dtype=float32), 'agent-1': array([ 0.31895158, -0.947771  ,  3.7053635 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.23164117, -0.57426727,  0.68249476], dtype=float32), 'agent-1': array([-0.01108646,  0.9609904 ,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.3352640212736944, 'agent-1': 1.3352640212736944} \n",
      "\n",
      "obs:  {'agent-0': array([-0.32564884,  0.9454908 ,  3.6719766 ], dtype=float32), 'agent-1': array([ 0.32564884, -0.9454908 ,  3.6719766 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.       , -1.       ,  0.6918043], dtype=float32), 'agent-1': array([-0.23160398,  1.        ,  0.96395975], dtype=float32)}\n",
      "reward:  {'agent-0': 1.862041256754722, 'agent-1': 1.862041256754722} \n",
      "\n",
      "obs:  {'agent-0': array([-0.31718406,  0.948364  ,  3.6234746 ], dtype=float32), 'agent-1': array([ 0.31718406, -0.948364  ,  3.6234746 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.06104517,  0.13927114,  0.66743886], dtype=float32), 'agent-1': array([-1.,  1.,  1.], dtype=float32)}\n",
      "reward:  {'agent-0': 1.1589901350799323, 'agent-1': 1.1589901350799323} \n",
      "\n",
      "obs:  {'agent-0': array([-0.3004277,  0.9538046,  3.592053 ], dtype=float32), 'agent-1': array([ 0.3004277, -0.9538046,  3.592053 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.83612597, -0.90675163,  1.        ], dtype=float32), 'agent-1': array([-0.701229,  1.      ,  1.      ], dtype=float32)}\n",
      "reward:  {'agent-0': 2.2684489761409026, 'agent-1': 2.2684489761409026} \n",
      "\n",
      "obs:  {'agent-0': array([-0.27452433,  0.96158016,  3.5275388 ], dtype=float32), 'agent-1': array([ 0.27452433, -0.96158016,  3.5275388 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.03149951, -0.9094422 ,  0.902678  ], dtype=float32), 'agent-1': array([-1.        ,  1.        ,  0.97413635], dtype=float32)}\n",
      "reward:  {'agent-0': 1.9829278699675044, 'agent-1': 1.9829278699675044} \n",
      "\n",
      "obs:  {'agent-0': array([-0.2616017,  0.9651759,  3.4675205], dtype=float32), 'agent-1': array([ 0.2616017, -0.9651759,  3.4675205], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.50684476, -0.88949835,  1.        ], dtype=float32), 'agent-1': array([-0.4697832,  1.       ,  1.       ], dtype=float32)}\n",
      "reward:  {'agent-0': 2.075718012656335, 'agent-1': 2.075718012656335} \n",
      "\n",
      "obs:  {'agent-0': array([-0.2466398,  0.9691072,  3.4005785], dtype=float32), 'agent-1': array([ 0.2466398, -0.9691072,  3.4005785], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.8616024 , -0.8624813 ,  0.41598666], dtype=float32), 'agent-1': array([-1.       ,  0.6072551,  1.       ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.2502636625120083, 'agent-1': 1.2502636625120083} \n",
      "\n",
      "obs:  {'agent-0': array([-0.20877448,  0.9779638 ,  3.3579829 ], dtype=float32), 'agent-1': array([ 0.20877448, -0.9779638 ,  3.3579829 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.3847736 , -0.4811294 ,  0.56506187], dtype=float32), 'agent-1': array([-1.,  1.,  1.], dtype=float32)}\n",
      "reward:  {'agent-0': 1.4024789916236564, 'agent-1': 1.4024789916236564} \n",
      "\n",
      "obs:  {'agent-0': array([-0.19017206,  0.9817508 ,  3.3079374 ], dtype=float32), 'agent-1': array([ 0.19017206, -0.9817508 ,  3.3079374 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.24658298, -0.55531275,  0.6529535 ], dtype=float32), 'agent-1': array([-1.,  1.,  1.], dtype=float32)}\n",
      "reward:  {'agent-0': 1.5428669041211656, 'agent-1': 1.5428669041211656} \n",
      "\n",
      "obs:  {'agent-0': array([-0.15516831,  0.98788804,  3.2498252 ], dtype=float32), 'agent-1': array([ 0.15516831, -0.98788804,  3.2498252 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.93128645, -0.62897265,  0.27073476], dtype=float32), 'agent-1': array([-1.        ,  0.06051743,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.39270667373356716, 'agent-1': 0.39270667373356716} \n",
      "\n",
      "obs:  {'agent-0': array([-0.10633504,  0.99433035,  3.2344785 ], dtype=float32), 'agent-1': array([ 0.10633504, -0.99433035,  3.2344785 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.23283195, -0.7337396 ,  0.96151555], dtype=float32), 'agent-1': array([-0.42195666,  0.13888383,  0.635552  ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.8381119295017818, 'agent-1': 0.8381119295017818} \n",
      "\n",
      "obs:  {'agent-0': array([-0.08922929,  0.99601114,  3.200916  ], dtype=float32), 'agent-1': array([ 0.08922929, -0.99601114,  3.200916  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.5400316 , -0.7181524 ,  0.12320012], dtype=float32), 'agent-1': array([-0.24215555,  1.        ,  0.8724295 ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.9699117704044298, 'agent-1': 0.9699117704044298} \n",
      "\n",
      "obs:  {'agent-0': array([-0.08665293,  0.9962386 ,  3.1606152 ], dtype=float32), 'agent-1': array([ 0.08665293, -0.9962386 ,  3.1606152 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.20164418, 0.19561481, 0.5034573 ], dtype=float32), 'agent-1': array([-0.2809956 ,  1.        ,  0.72092485], dtype=float32)}\n",
      "reward:  {'agent-0': 0.645037490988333, 'agent-1': 0.645037490988333} \n",
      "\n",
      "obs:  {'agent-0': array([-0.07534023,  0.9971579 ,  3.132885  ], dtype=float32), 'agent-1': array([ 0.07534023, -0.9971579 ,  3.132885  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.5347786, -0.5572752,  0.878322 ], dtype=float32), 'agent-1': array([-0.44313443,  0.08954775,  0.9391286 ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.6221139680317762, 'agent-1': 0.6221139680317762} \n",
      "\n",
      "obs:  {'agent-0': array([-0.03598382,  0.9993524 ,  3.1053913 ], dtype=float32), 'agent-1': array([ 0.03598382, -0.9993524 ,  3.1053913 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.39089358, -0.13372242,  0.60416996], dtype=float32), 'agent-1': array([-0.3339253 ,  1.        ,  0.77831113], dtype=float32)}\n",
      "reward:  {'agent-0': 0.8711118202539829, 'agent-1': 0.8711118202539829} \n",
      "\n",
      "obs:  {'agent-0': array([-0.01325571,  0.99991214,  3.0655773 ], dtype=float32), 'agent-1': array([ 0.01325571, -0.99991214,  3.0655773 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.6037581 , -0.35234177,  0.540828  ], dtype=float32), 'agent-1': array([-0.02504939,  0.45257306,  0.86045647], dtype=float32)}\n",
      "reward:  {'agent-0': 0.5816227755118852, 'agent-1': 0.5816227755118852} \n",
      "\n",
      "obs:  {'agent-0': array([0.00387843, 0.9999925 , 3.0380833 ], dtype=float32), 'agent-1': array([-0.00387843, -0.9999925 ,  3.0380833 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.39688247,  0.8183622 ], dtype=float32), 'agent-1': array([-1.       ,  0.2839527,  1.       ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.5158376626153682, 'agent-1': 0.5158376626153682} \n",
      "\n",
      "obs:  {'agent-0': array([0.09795705, 0.9951906 , 3.0130503 ], dtype=float32), 'agent-1': array([-0.09795705, -0.9951906 ,  3.0130503 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.4263171, -0.2164309,  0.5423053], dtype=float32), 'agent-1': array([-0.43586177,  0.48285067,  0.7963972 ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.48742536555383964, 'agent-1': 0.48742536555383964} \n",
      "\n",
      "obs:  {'agent-0': array([0.10663441, 0.9942983 , 2.988806  ], dtype=float32), 'agent-1': array([-0.10663441, -0.9942983 ,  2.988806  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.70362  , -0.8692855,  0.9773586], dtype=float32), 'agent-1': array([0.61753273, 1.        , 1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.942408256575959, 'agent-1': 1.942408256575959} \n",
      "\n",
      "obs:  {'agent-0': array([0.04173347, 0.99912876, 2.885892  ], dtype=float32), 'agent-1': array([-0.04173347, -0.99912876,  2.885892  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        , -0.6393318 ,  0.40330684], dtype=float32), 'agent-1': array([0.54640865, 0.8014624 , 0.53128356], dtype=float32)}\n",
      "reward:  {'agent-0': 0.6983865340927728, 'agent-1': 0.6983865340927728} \n",
      "\n",
      "obs:  {'agent-0': array([7.7099854e-04, 9.9999970e-01, 2.8461387e+00], dtype=float32), 'agent-1': array([-7.7099854e-04, -9.9999970e-01,  2.8461387e+00], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.9805076 , -0.0748657 ,  0.72745085], dtype=float32), 'agent-1': array([-1.,  1.,  1.], dtype=float32)}\n",
      "reward:  {'agent-0': 0.9565958847358118, 'agent-1': 0.9565958847358118} \n",
      "\n",
      "obs:  {'agent-0': array([0.11305779, 0.9935884 , 2.7889886 ], dtype=float32), 'agent-1': array([-0.11305779, -0.9935884 ,  2.7889886 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        , -0.07589638,  0.8166733 ], dtype=float32), 'agent-1': array([-0.3178894 ,  1.        ,  0.69336855], dtype=float32)}\n",
      "reward:  {'agent-0': 0.8090247289332098, 'agent-1': 0.8090247289332098} \n",
      "\n",
      "obs:  {'agent-0': array([0.07813746, 0.9969426 , 2.7379673 ], dtype=float32), 'agent-1': array([-0.07813746, -0.9969426 ,  2.7379673 ], dtype=float32)}\n",
      "action:  {'agent-0': array([1.        , 0.00157404, 1.        ], dtype=float32), 'agent-1': array([-0.02233607,  1.        ,  0.88940644], dtype=float32)}\n",
      "reward:  {'agent-0': 0.7622858214035784, 'agent-1': 0.7622858214035784} \n",
      "\n",
      "obs:  {'agent-0': array([0.15696672, 0.9876039 , 2.6873882 ], dtype=float32), 'agent-1': array([-0.15696672, -0.9876039 ,  2.6873882 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.0946188 , -0.4984972 ,  0.48495874], dtype=float32), 'agent-1': array([1.       , 1.       , 0.9308549], dtype=float32)}\n",
      "reward:  {'agent-0': 1.2777952882460717, 'agent-1': 1.2777952882460717} \n",
      "\n",
      "obs:  {'agent-0': array([0.10184209, 0.99480057, 2.5964074 ], dtype=float32), 'agent-1': array([-0.10184209, -0.99480057,  2.5964074 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.24792993, -0.6646574 ,  0.48483336], dtype=float32), 'agent-1': array([-0.4160046 ,  0.14503062,  0.873943  ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.3858457622817397, 'agent-1': 0.3858457622817397} \n",
      "\n",
      "obs:  {'agent-0': array([0.1453235, 0.9893842, 2.5672245], dtype=float32), 'agent-1': array([-0.1453235, -0.9893842,  2.5672245], dtype=float32)}\n",
      "action:  {'agent-0': array([0.2565905 , 0.21515512, 0.92750084], dtype=float32), 'agent-1': array([0.9332813, 1.       , 1.       ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.8783297494335045, 'agent-1': 0.8783297494335045} \n",
      "\n",
      "obs:  {'agent-0': array([0.0944189, 0.9955326, 2.4974346], dtype=float32), 'agent-1': array([-0.0944189, -0.9955326,  2.4974346], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.37975377,  0.6843587 ], dtype=float32), 'agent-1': array([-0.9681995,  1.       ,  1.       ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.9446132130215315, 'agent-1': 0.9446132130215315} \n",
      "\n",
      "obs:  {'agent-0': array([0.26506692, 0.96423   , 2.416509  ], dtype=float32), 'agent-1': array([-0.26506692, -0.96423   ,  2.416509  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.12176377,  0.1254158 ,  1.        ], dtype=float32), 'agent-1': array([-0.6293472 ,  0.44639897,  0.22649431], dtype=float32)}\n",
      "reward:  {'agent-0': -0.028956504543698358, 'agent-1': -0.028956504543698358} \n",
      "\n",
      "obs:  {'agent-0': array([0.2663472 , 0.96387714, 2.4190893 ], dtype=float32), 'agent-1': array([-0.2663472 , -0.96387714,  2.4190893 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.4191389 , -0.12479806,  1.        ], dtype=float32), 'agent-1': array([1.        , 0.40915668, 0.18663526], dtype=float32)}\n",
      "reward:  {'agent-0': 0.12815277562257776, 'agent-1': 0.12815277562257776} \n",
      "\n",
      "obs:  {'agent-0': array([0.29272738, 0.95619595, 2.4076178 ], dtype=float32), 'agent-1': array([-0.29272738, -0.95619595,  2.4076178 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.2005781 , -0.11111289,  1.        ], dtype=float32), 'agent-1': array([-0.97417736,  0.39295733,  0.8569778 ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.2150457839468931, 'agent-1': 0.2150457839468931} \n",
      "\n",
      "obs:  {'agent-0': array([0.36320758, 0.9317083 , 2.3880675 ], dtype=float32), 'agent-1': array([-0.36320758, -0.9317083 ,  2.3880675 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        , -0.87572396,  1.        ], dtype=float32), 'agent-1': array([1.        , 1.        , 0.53723544], dtype=float32)}\n",
      "reward:  {'agent-0': 1.822297768279105, 'agent-1': 1.822297768279105} \n",
      "\n",
      "obs:  {'agent-0': array([0.254738 , 0.9670101, 2.2049863], dtype=float32), 'agent-1': array([-0.254738 , -0.9670101,  2.2049863], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.81344867, -0.00611073,  0.6304534 ], dtype=float32), 'agent-1': array([-1.       ,  0.8656869,  0.9196356], dtype=float32)}\n",
      "reward:  {'agent-0': 0.24562542245632724, 'agent-1': 0.24562542245632724} \n",
      "\n",
      "obs:  {'agent-0': array([0.44581053, 0.89512736, 2.1775322 ], dtype=float32), 'agent-1': array([-0.44581053, -0.89512736,  2.1775322 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.33777487, -0.6706242 ,  0.673509  ], dtype=float32), 'agent-1': array([1.       , 0.6706512, 0.5273025], dtype=float32)}\n",
      "reward:  {'agent-0': 1.0499460259936848, 'agent-1': 1.0499460259936848} \n",
      "\n",
      "obs:  {'agent-0': array([0.40348747, 0.9149852 , 2.0508564 ], dtype=float32), 'agent-1': array([-0.40348747, -0.9149852 ,  2.0508564 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.2790736, -1.       ,  1.       ], dtype=float32), 'agent-1': array([-0.6063558 , -0.10302067,  0.63929105], dtype=float32)}\n",
      "reward:  {'agent-0': 0.5075135661827233, 'agent-1': 0.5075135661827233} \n",
      "\n",
      "obs:  {'agent-0': array([0.5425462 , 0.84002596, 1.9833493 ], dtype=float32), 'agent-1': array([-0.5425462 , -0.84002596,  1.9833493 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.       , -0.7366769,  0.8755615], dtype=float32), 'agent-1': array([-0.6487361 ,  0.00656104,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.6680242159303296, 'agent-1': 0.6680242159303296} \n",
      "\n",
      "obs:  {'agent-0': array([0.56676626, 0.82387865, 1.8869208 ], dtype=float32), 'agent-1': array([-0.56676626, -0.82387865,  1.8869208 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.39915383, -1.        ,  0.5207567 ], dtype=float32), 'agent-1': array([0.5843092, 0.0716989, 1.       ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.7014019928975079, 'agent-1': 0.7014019928975079} \n",
      "\n",
      "obs:  {'agent-0': array([0.5710712 , 0.82090056, 1.7745482 ], dtype=float32), 'agent-1': array([-0.5710712 , -0.82090056,  1.7745482 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        , -0.93603265,  1.        ], dtype=float32), 'agent-1': array([-0.17281234,  1.        ,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.3279564374769084, 'agent-1': 1.3279564374769084} \n",
      "\n",
      "obs:  {'agent-0': array([0.5033778, 0.8640664, 1.5194386], dtype=float32), 'agent-1': array([-0.5033778, -0.8640664,  1.5194386], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.69136095, -1.        ,  0.9373125 ], dtype=float32), 'agent-1': array([1.       , 0.1313572, 0.7682943], dtype=float32)}\n",
      "reward:  {'agent-0': 1.4883794980585554, 'agent-1': 1.4883794980585554} \n",
      "\n",
      "obs:  {'agent-0': array([0.18285461, 0.98314   , 1.125345  ], dtype=float32), 'agent-1': array([-0.18285461, -0.98314   ,  1.125345  ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.20223403, 0.3160298 , 0.53956795], dtype=float32), 'agent-1': array([1.        , 1.        , 0.83288807], dtype=float32)}\n",
      "reward:  {'agent-0': 0.6555362631178197, 'agent-1': 0.6555362631178197} \n",
      "\n",
      "obs:  {'agent-0': array([-0.24071477,  0.9705959 ,  0.886138  ], dtype=float32), 'agent-1': array([ 0.24071477, -0.9705959 ,  0.886138  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.17980331, -0.7437148 ,  1.        ], dtype=float32), 'agent-1': array([1.        , 1.        , 0.33784053], dtype=float32)}\n",
      "reward:  {'agent-0': 0.6010109061883266, 'agent-1': 0.6010109061883266} \n",
      "\n",
      "obs:  {'agent-0': array([-0.9435261 ,  0.33129823,  0.64834136], dtype=float32), 'agent-1': array([ 0.9435261 , -0.33129823,  0.64834136], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.       , -1.       ,  0.9527235], dtype=float32), 'agent-1': array([-0.24993956,  1.        ,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.598534925287536, 'agent-1': 0.598534925287536} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.13986905, -0.99017   ,  0.50495785], dtype=float32), 'agent-1': array([-0.13986905,  0.99017   ,  0.50495785], dtype=float32)}\n",
      "action:  {'agent-0': array([0.03056049, 0.01245213, 0.        ], dtype=float32), 'agent-1': array([ 0.2513218 , -0.5556167 ,  0.03996229], dtype=float32)}\n",
      "reward:  {'agent-0': 0.3897909475106517, 'agent-1': 0.3897909475106517} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.1291724 , -0.99162215,  0.49076313], dtype=float32), 'agent-1': array([-0.1291724 ,  0.99162215,  0.49076313], dtype=float32)}\n",
      "action:  {'agent-0': array([1.        , 0.7098665 , 0.01840988], dtype=float32), 'agent-1': array([ 1., -1.,  0.], dtype=float32)}\n",
      "reward:  {'agent-0': 0.38696138774392386, 'agent-1': 0.38696138774392386} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.16083518, -0.9869813 ,  0.4844614 ], dtype=float32), 'agent-1': array([-0.16083518,  0.9869813 ,  0.4844614 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.25915468, 1.        , 0.40829813], dtype=float32), 'agent-1': array([ 0.11488092, -1.        ,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.039301531671069, 'agent-1': 1.039301531671069} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.70568967, -0.70852107,  0.256191  ], dtype=float32), 'agent-1': array([-0.70568967,  0.70852107,  0.256191  ], dtype=float32)}\n",
      "action:  {'agent-0': array([1.        , 0.91632545, 0.17468816], dtype=float32), 'agent-1': array([ 0.1984253 , -1.        ,  0.05515036], dtype=float32)}\n",
      "reward:  {'agent-0': 0.5521997869238293, 'agent-1': 0.5521997869238293} \n",
      "\n",
      "obs:  {'agent-0': array([0.99974614, 0.02252987, 0.31473762], dtype=float32), 'agent-1': array([-0.99974614, -0.02252987,  0.31473762], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        ,  0.11011457,  0.35593724], dtype=float32), 'agent-1': array([ 1.        , -0.25408453,  0.19473496], dtype=float32)}\n",
      "reward:  {'agent-0': 0.959422451457395, 'agent-1': 0.959422451457395} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8812483 ,  0.47265363,  0.18667763], dtype=float32), 'agent-1': array([ 0.8812483 , -0.47265363,  0.18667763], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.9422668 , -0.60449773,  0.        ], dtype=float32), 'agent-1': array([-1.,  1.,  0.], dtype=float32)}\n",
      "reward:  {'agent-0': 0.7947613036262495, 'agent-1': 0.7947613036262495} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8812483 ,  0.47265363,  0.18667763], dtype=float32), 'agent-1': array([ 0.8812483 , -0.47265363,  0.18667763], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.26177287, -0.537583  ,  0.32349798], dtype=float32), 'agent-1': array([-1.        ,  0.40741324,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.9589473298338522, 'agent-1': 0.9589473298338522} \n",
      "\n",
      "obs:  {'agent-0': array([-0.7810526 , -0.6244652 ,  0.11613339], dtype=float32), 'agent-1': array([0.7810526 , 0.6244652 , 0.11613339], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.6975796 ,  0.45215666,  0.40740556], dtype=float32), 'agent-1': array([-0.6399204, -0.5029012,  1.       ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.20308132462943906, 'agent-1': -0.20308132462943906} \n",
      "\n",
      "obs:  {'agent-0': array([0.39139506, 0.92022276, 0.50869143], dtype=float32), 'agent-1': array([-0.39139506, -0.92022276,  0.50869143], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.5934944, -0.4868462,  0.       ], dtype=float32), 'agent-1': array([0.30953526, 1.        , 1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.8771512324897007, 'agent-1': 0.8771512324897007} \n",
      "\n",
      "obs:  {'agent-0': array([-0.1272222 , -0.9918742 ,  0.33144614], dtype=float32), 'agent-1': array([0.1272222 , 0.9918742 , 0.33144614], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.34420973,  1.        ,  0.03221449], dtype=float32), 'agent-1': array([0.29589128, 0.36512148, 0.45119452], dtype=float32)}\n",
      "reward:  {'agent-0': 0.27821302277811066, 'agent-1': 0.27821302277811066} \n",
      "\n",
      "obs:  {'agent-0': array([-0.34911114, -0.93708134,  0.44300753], dtype=float32), 'agent-1': array([0.34911114, 0.93708134, 0.44300753], dtype=float32)}\n",
      "action:  {'agent-0': array([0.24355412, 0.93819845, 0.18606341], dtype=float32), 'agent-1': array([ 0.39022493, -0.07078433,  0.434898  ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.6580155986641709, 'agent-1': 0.6580155986641709} \n",
      "\n",
      "obs:  {'agent-0': array([-0.7093434 , -0.70486313,  0.37134576], dtype=float32), 'agent-1': array([0.7093434 , 0.70486313, 0.37134576], dtype=float32)}\n",
      "action:  {'agent-0': array([0.11551714, 0.8029332 , 0.3395179 ], dtype=float32), 'agent-1': array([ 0.36991024, -0.83732736,  0.10176802], dtype=float32)}\n",
      "reward:  {'agent-0': 0.809635686100746, 'agent-1': 0.809635686100746} \n",
      "\n",
      "obs:  {'agent-0': array([-0.9918167 ,  0.12767005,  0.27765012], dtype=float32), 'agent-1': array([ 0.9918167 , -0.12767005,  0.27765012], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.6582211 , -0.61368346,  0.        ], dtype=float32), 'agent-1': array([-0.7424573, -0.2774048,  0.       ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.6799757285974299, 'agent-1': 0.6799757285974299} \n",
      "\n",
      "obs:  {'agent-0': array([-0.9918167 ,  0.12767005,  0.27765012], dtype=float32), 'agent-1': array([ 0.9918167 , -0.12767005,  0.27765012], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.5091212, -1.       ,  0.6461404], dtype=float32), 'agent-1': array([ 0.6114805 , -0.86453694,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.45103245427169425, 'agent-1': -0.45103245427169425} \n",
      "\n",
      "obs:  {'agent-0': array([-0.72992486, -0.6835274 ,  0.6342081 ], dtype=float32), 'agent-1': array([0.72992486, 0.6835274 , 0.6342081 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.11911792,  1.        ,  0.        ], dtype=float32), 'agent-1': array([-0.7984566 ,  0.8477657 ,  0.37996015], dtype=float32)}\n",
      "reward:  {'agent-0': -0.09205732132336497, 'agent-1': -0.09205732132336497} \n",
      "\n",
      "obs:  {'agent-0': array([-0.34687495, -0.9379114 ,  0.68752784], dtype=float32), 'agent-1': array([0.34687495, 0.9379114 , 0.68752784], dtype=float32)}\n",
      "action:  {'agent-0': array([1., 1., 1.], dtype=float32), 'agent-1': array([-0.44324136, -0.14871788,  0.90650225], dtype=float32)}\n",
      "reward:  {'agent-0': -0.09014302258986318, 'agent-1': -0.09014302258986318} \n",
      "\n",
      "obs:  {'agent-0': array([0.9813482 , 0.19223852, 0.73185617], dtype=float32), 'agent-1': array([-0.9813482 , -0.19223852,  0.73185617], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.3953966 ,  1.        ,  0.30449662], dtype=float32), 'agent-1': array([ 1.       , -0.4617015,  0.       ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.009976569756333653, 'agent-1': 0.009976569756333653} \n",
      "\n",
      "obs:  {'agent-0': array([0.8778771, 0.478886 , 0.7270457], dtype=float32), 'agent-1': array([-0.8778771, -0.478886 ,  0.7270457], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.20261967,  0.16676831,  0.32712758], dtype=float32), 'agent-1': array([-0.26400125,  0.42496657,  0.41647437], dtype=float32)}\n",
      "reward:  {'agent-0': 0.012466380344379191, 'agent-1': 0.012466380344379191} \n",
      "\n",
      "obs:  {'agent-0': array([0.9295681 , 0.36865035, 0.72100204], dtype=float32), 'agent-1': array([-0.9295681 , -0.36865035,  0.72100204], dtype=float32)}\n",
      "action:  {'agent-0': array([0.12149823, 0.94942975, 0.41522983], dtype=float32), 'agent-1': array([-0.37872875,  0.49731684,  0.3297938 ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.25639594582934677, 'agent-1': -0.25639594582934677} \n",
      "\n",
      "obs:  {'agent-0': array([0.88159347, 0.4720095 , 0.83849734], dtype=float32), 'agent-1': array([-0.88159347, -0.4720095 ,  0.83849734], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.01170158, -1.        ,  0.48711586], dtype=float32), 'agent-1': array([ 0.6794157 , -0.78325033,  0.45833603], dtype=float32)}\n",
      "reward:  {'agent-0': 0.34607102104849075, 'agent-1': 0.34607102104849075} \n",
      "\n",
      "obs:  {'agent-0': array([0.8661032, 0.4998653, 0.684817 ], dtype=float32), 'agent-1': array([-0.8661032, -0.4998653,  0.684817 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.97248435, -0.9690568 ,  0.90497535], dtype=float32), 'agent-1': array([0.18884397, 0.5046215 , 0.42390743], dtype=float32)}\n",
      "reward:  {'agent-0': -0.7737117221342016, 'agent-1': -0.7737117221342016} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.9400354 , -0.34107694,  1.0141869 ], dtype=float32), 'agent-1': array([-0.9400354 ,  0.34107694,  1.0141869 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.7068667 , -0.04926908,  1.        ], dtype=float32), 'agent-1': array([ 1., -1.,  1.], dtype=float32)}\n",
      "reward:  {'agent-0': 2.0456951283626275, 'agent-1': 2.0456951283626275} \n",
      "\n",
      "obs:  {'agent-0': array([-0.15493147,  0.98792523,  0.30432734], dtype=float32), 'agent-1': array([ 0.15493147, -0.98792523,  0.30432734], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.59660566,  0.3502406 ], dtype=float32), 'agent-1': array([ 1.        , -0.74395025,  0.14209747], dtype=float32)}\n",
      "reward:  {'agent-0': 0.7725836305839787, 'agent-1': 0.7725836305839787} \n",
      "\n",
      "obs:  {'agent-0': array([0.5248649, 0.8511855, 0.2558543], dtype=float32), 'agent-1': array([-0.5248649, -0.8511855,  0.2558543], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.21995044,  0.9460323 ,  0.5507728 ], dtype=float32), 'agent-1': array([1.        , 0.01649272, 0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.24820563516241617, 'agent-1': -0.24820563516241617} \n",
      "\n",
      "obs:  {'agent-0': array([0.04142085, 0.9991418 , 0.5709146 ], dtype=float32), 'agent-1': array([-0.04142085, -0.9991418 ,  0.5709146 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.57443273, -0.53693223,  0.29309928], dtype=float32), 'agent-1': array([0.978611  , 0.04990053, 0.21934792], dtype=float32)}\n",
      "reward:  {'agent-0': 0.37793641723379734, 'agent-1': 0.37793641723379734} \n",
      "\n",
      "obs:  {'agent-0': array([-0.50451976,  0.86340016,  0.5282574 ], dtype=float32), 'agent-1': array([ 0.50451976, -0.86340016,  0.5282574 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.6271541 , -0.9105077 ,  0.53301847], dtype=float32), 'agent-1': array([-1.        ,  1.        ,  0.24580494], dtype=float32)}\n",
      "reward:  {'agent-0': 1.169180392521425, 'agent-1': 1.169180392521425} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.8692463 , -0.49437925,  0.2338041 ], dtype=float32), 'agent-1': array([-0.8692463 ,  0.49437925,  0.2338041 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.95651084,  0.8522768 ,  0.13040611], dtype=float32), 'agent-1': array([1.        , 0.48495245, 0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.0514902708644827, 'agent-1': 1.0514902708644827} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.9836592 , -0.18004052,  0.10070772], dtype=float32), 'agent-1': array([-0.9836592 ,  0.18004052,  0.10070772], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.4338634, -0.7215365,  0.       ], dtype=float32), 'agent-1': array([-0.8297862 , -0.9287264 ,  0.63555145], dtype=float32)}\n",
      "reward:  {'agent-0': -0.5971666858842644, 'agent-1': -0.5971666858842644} \n",
      "\n",
      "obs:  {'agent-0': array([0.7416903 , 0.6707425 , 0.61602855], dtype=float32), 'agent-1': array([-0.7416903 , -0.6707425 ,  0.61602855], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.60172313, -1.        ,  0.38505724], dtype=float32), 'agent-1': array([0.72443175, 0.5081358 , 0.31553847], dtype=float32)}\n",
      "reward:  {'agent-0': 1.5050812830233689, 'agent-1': 1.5050812830233689} \n",
      "\n",
      "obs:  {'agent-0': array([0.9888622 , 0.14883378, 0.15976861], dtype=float32), 'agent-1': array([-0.9888622 , -0.14883378,  0.15976861], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.31751823,  0.6360301 ,  0.82521224], dtype=float32), 'agent-1': array([1.        , 1.        , 0.03796384], dtype=float32)}\n",
      "reward:  {'agent-0': 0.1160779065818488, 'agent-1': 0.1160779065818488} \n",
      "\n",
      "obs:  {'agent-0': array([-0.24343283,  0.9699178 ,  0.42433968], dtype=float32), 'agent-1': array([ 0.24343283, -0.9699178 ,  0.42433968], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.18137789, -0.09540701,  0.71404654], dtype=float32), 'agent-1': array([0.45540214, 0.11788917, 0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.6394695645081132, 'agent-1': 0.6394695645081132} \n",
      "\n",
      "obs:  {'agent-0': array([0.00188581, 0.9999982 , 0.36780173], dtype=float32), 'agent-1': array([-0.00188581, -0.9999982 ,  0.36780173], dtype=float32)}\n",
      "action:  {'agent-0': array([0.28802228, 0.04574573, 0.        ], dtype=float32), 'agent-1': array([1.        , 0.34305322, 0.5954133 ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.16196175554625364, 'agent-1': 0.16196175554625364} \n",
      "\n",
      "obs:  {'agent-0': array([-0.9271446 ,  0.3747037 ,  0.49548674], dtype=float32), 'agent-1': array([ 0.9271446 , -0.3747037 ,  0.49548674], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.94378006,  0.10205379], dtype=float32), 'agent-1': array([0.5490706, 1.       , 0.       ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.6150272431826145, 'agent-1': 0.6150272431826145} \n",
      "\n",
      "obs:  {'agent-0': array([-0.959828  ,  0.280589  ,  0.41418356], dtype=float32), 'agent-1': array([ 0.959828  , -0.280589  ,  0.41418356], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.86479986,  0.        ], dtype=float32), 'agent-1': array([-1.        , -0.27343923,  0.59188896], dtype=float32)}\n",
      "reward:  {'agent-0': 0.8700071968592972, 'agent-1': 0.8700071968592972} \n",
      "\n",
      "obs:  {'agent-0': array([0.30901417, 0.95105743, 0.27881575], dtype=float32), 'agent-1': array([-0.30901417, -0.95105743,  0.27881575], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.22300309, -0.5750899 ,  0.        ], dtype=float32), 'agent-1': array([0.36709774, 1.        , 0.35696846], dtype=float32)}\n",
      "reward:  {'agent-0': 1.201249855576256, 'agent-1': 1.201249855576256} \n",
      "\n",
      "obs:  {'agent-0': array([-0.52653134, -0.8501557 ,  0.05841699], dtype=float32), 'agent-1': array([0.52653134, 0.8501557 , 0.05841699], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.7511538 ,  0.13133386], dtype=float32), 'agent-1': array([ 0.3628204 , -0.01633972,  0.2740155 ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.7695217909448041, 'agent-1': 0.7695217909448041} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.00165786, -0.9999986 ,  0.13568197], dtype=float32), 'agent-1': array([-0.00165786,  0.9999986 ,  0.13568197], dtype=float32)}\n",
      "action:  {'agent-0': array([1.        , 0.9325644 , 0.20990744], dtype=float32), 'agent-1': array([0.67005396, 0.56619453, 0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.7130861641372974, 'agent-1': 0.7130861641372974} \n",
      "\n",
      "obs:  {'agent-0': array([0.97238815, 0.23336945, 0.19566195], dtype=float32), 'agent-1': array([-0.97238815, -0.23336945,  0.19566195], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        , -0.62157404,  0.39244545], dtype=float32), 'agent-1': array([1.       , 0.0833931, 0.       ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.6844235967349301, 'agent-1': 0.6844235967349301} \n",
      "\n",
      "obs:  {'agent-0': array([-0.68572426, -0.72786146,  0.23574072], dtype=float32), 'agent-1': array([0.68572426, 0.72786146, 0.23574072], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.4576642 ,  0.53563595,  0.3219623 ], dtype=float32), 'agent-1': array([-0.85725427,  0.4959998 ,  0.39006168], dtype=float32)}\n",
      "reward:  {'agent-0': 0.8367115191631367, 'agent-1': 0.8367115191631367} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.02206458, -0.9997566 ,  0.19438785], dtype=float32), 'agent-1': array([-0.02206458,  0.9997566 ,  0.19438785], dtype=float32)}\n",
      "action:  {'agent-0': array([0.15077949, 0.23946297, 0.        ], dtype=float32), 'agent-1': array([ 0.33375478, -0.11798823,  0.05594534], dtype=float32)}\n",
      "reward:  {'agent-0': 0.7978057095783386, 'agent-1': 0.7978057095783386} \n",
      "\n",
      "obs:  {'agent-0': array([-0.06688567, -0.99776065,  0.18928127], dtype=float32), 'agent-1': array([0.06688567, 0.99776065, 0.18928127], dtype=float32)}\n",
      "action:  {'agent-0': array([1.        , 0.45799792, 0.        ], dtype=float32), 'agent-1': array([ 0.53707695, -0.38877326,  0.45943147], dtype=float32)}\n",
      "reward:  {'agent-0': 0.6837225800870711, 'agent-1': 0.6837225800870711} \n",
      "\n",
      "obs:  {'agent-0': array([-0.9937431 , -0.11168975,  0.23295851], dtype=float32), 'agent-1': array([0.9937431 , 0.11168975, 0.23295851], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.97224426, -0.09691441,  0.        ], dtype=float32), 'agent-1': array([-0.39445215,  0.22706163,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.7376708999013197, 'agent-1': 0.7376708999013197} \n",
      "\n",
      "obs:  {'agent-0': array([-0.9937431 , -0.11168975,  0.23295851], dtype=float32), 'agent-1': array([0.9937431 , 0.11168975, 0.23295851], dtype=float32)}\n",
      "action:  {'agent-0': array([1.       , 0.5991483, 0.4190288], dtype=float32), 'agent-1': array([0.08940339, 0.44781184, 0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.7173528197050919, 'agent-1': 0.7173528197050919} \n",
      "\n",
      "obs:  {'agent-0': array([0.5810934 , 0.81383693, 0.24097414], dtype=float32), 'agent-1': array([-0.5810934 , -0.81383693,  0.24097414], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.29456335, -0.54063   ,  0.46824288], dtype=float32), 'agent-1': array([0.3205849 , 0.67564106, 0.5352148 ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.43034938867953126, 'agent-1': 0.43034938867953126} \n",
      "\n",
      "obs:  {'agent-0': array([-0.35900953, -0.9333339 ,  0.35140967], dtype=float32), 'agent-1': array([0.35900953, 0.9333339 , 0.35140967], dtype=float32)}\n",
      "action:  {'agent-0': array([1.       , 1.       , 0.5729038], dtype=float32), 'agent-1': array([-1.        , -0.16150731,  0.70429313], dtype=float32)}\n",
      "reward:  {'agent-0': -0.7426204563060723, 'agent-1': -0.7426204563060723} \n",
      "\n",
      "obs:  {'agent-0': array([0.9676367, 0.2523473, 0.771815 ], dtype=float32), 'agent-1': array([-0.9676367, -0.2523473,  0.771815 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        , -0.05380332,  0.4163242 ], dtype=float32), 'agent-1': array([-1.        ,  0.24765074,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.6441374430270512, 'agent-1': 0.6441374430270512} \n",
      "\n",
      "obs:  {'agent-0': array([0.93409723, 0.35701877, 0.5651866 ], dtype=float32), 'agent-1': array([-0.93409723, -0.35701877,  0.5651866 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.33241463, 0.29332328, 0.6309056 ], dtype=float32), 'agent-1': array([-0.5090276 , -1.        ,  0.49698034], dtype=float32)}\n",
      "reward:  {'agent-0': -0.7512830733289665, 'agent-1': -0.7512830733289665} \n",
      "\n",
      "obs:  {'agent-0': array([0.77588207, 0.630878  , 0.92070466], dtype=float32), 'agent-1': array([-0.77588207, -0.630878  ,  0.92070466], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.98726225, -1.        ,  0.8094621 ], dtype=float32), 'agent-1': array([1.       , 1.       , 0.5485522], dtype=float32)}\n",
      "reward:  {'agent-0': 1.6289515276327315, 'agent-1': 1.6289515276327315} \n",
      "\n",
      "obs:  {'agent-0': array([-0.3974566 , -0.91762096,  0.3653747 ], dtype=float32), 'agent-1': array([0.3974566 , 0.91762096, 0.3653747 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.393588  ,  0.28803515,  0.17544505], dtype=float32), 'agent-1': array([-1.       , -0.1961419,  0.       ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.5804587541812001, 'agent-1': 0.5804587541812001} \n",
      "\n",
      "obs:  {'agent-0': array([-0.5678694, -0.8231186,  0.3578825], dtype=float32), 'agent-1': array([0.5678694, 0.8231186, 0.3578825], dtype=float32)}\n",
      "action:  {'agent-0': array([1.        , 0.23197329, 0.03996924], dtype=float32), 'agent-1': array([-1.        ,  1.        ,  0.40262297], dtype=float32)}\n",
      "reward:  {'agent-0': -0.11645463719905613, 'agent-1': -0.11645463719905613} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.25632983, -0.9665894 ,  0.57288516], dtype=float32), 'agent-1': array([-0.25632983,  0.9665894 ,  0.57288516], dtype=float32)}\n",
      "action:  {'agent-0': array([1.       , 1.       , 0.5324538], dtype=float32), 'agent-1': array([-0.15261912, -1.        ,  0.68241626], dtype=float32)}\n",
      "reward:  {'agent-0': -0.14011574473056043, 'agent-1': -0.14011574473056043} \n",
      "\n",
      "obs:  {'agent-0': array([0.87258583, 0.48846084, 0.6712828 ], dtype=float32), 'agent-1': array([-0.87258583, -0.48846084,  0.6712828 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.25250584, -0.7927365 ,  0.20002165], dtype=float32), 'agent-1': array([-1.        , -0.38545388,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.9578821443430323, 'agent-1': -0.9578821443430323} \n",
      "\n",
      "obs:  {'agent-0': array([0.9319493, 0.3625886, 1.0697422], dtype=float32), 'agent-1': array([-0.9319493, -0.3625886,  1.0697422], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.       , -0.7074309,  0.7540469], dtype=float32), 'agent-1': array([1.        , 0.9016082 , 0.11827293], dtype=float32)}\n",
      "reward:  {'agent-0': 1.0873828754551726, 'agent-1': 1.0873828754551726} \n",
      "\n",
      "obs:  {'agent-0': array([0.99824196, 0.05926999, 0.6489981 ], dtype=float32), 'agent-1': array([-0.99824196, -0.05926999,  0.6489981 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        ,  0.4244467 ,  0.00431052], dtype=float32), 'agent-1': array([ 0.14145744, -0.9686087 ,  0.6047377 ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.12934044561576574, 'agent-1': -0.12934044561576574} \n",
      "\n",
      "obs:  {'agent-0': array([0.7882938 , 0.61529905, 0.71440125], dtype=float32), 'agent-1': array([-0.7882938 , -0.61529905,  0.71440125], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.53716147, -0.18375182,  0.68827575], dtype=float32), 'agent-1': array([1.        , 0.22701418, 1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.8053810953039795, 'agent-1': 0.8053810953039795} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8848771 ,  0.4658246 ,  0.48167956], dtype=float32), 'agent-1': array([ 0.8848771 , -0.4658246 ,  0.48167956], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.6290475, -1.       ,  0.5473794], dtype=float32), 'agent-1': array([-0.1504302 , -0.51534975,  0.21914434], dtype=float32)}\n",
      "reward:  {'agent-0': 1.1699628474407486, 'agent-1': 1.1699628474407486} \n",
      "\n",
      "obs:  {'agent-0': array([-0.75868857, -0.6514535 ,  0.20246242], dtype=float32), 'agent-1': array([0.75868857, 0.6514535 , 0.20246242], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1., -1.,  0.], dtype=float32), 'agent-1': array([-0.8696111 ,  0.73993826,  0.41244352], dtype=float32)}\n",
      "reward:  {'agent-0': 0.2461735367092377, 'agent-1': 0.2461735367092377} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.3851917 , -0.9228366 ,  0.39818552], dtype=float32), 'agent-1': array([-0.3851917 ,  0.9228366 ,  0.39818552], dtype=float32)}\n",
      "action:  {'agent-0': array([0.9625945 , 1.        , 0.08616877], dtype=float32), 'agent-1': array([ 0.71461666, -1.        ,  0.37601596], dtype=float32)}\n",
      "reward:  {'agent-0': 1.4668697450507318, 'agent-1': 1.4668697450507318} \n",
      "\n",
      "obs:  {'agent-0': array([0.23760222, 0.97136253, 0.01106383], dtype=float32), 'agent-1': array([-0.23760222, -0.97136253,  0.01106383], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.87287587, -0.17758405,  0.60975593], dtype=float32), 'agent-1': array([1.        , 1.        , 0.75349265], dtype=float32)}\n",
      "reward:  {'agent-0': -1.5285076058387506, 'agent-1': -1.5285076058387506} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8333743 , -0.552709  ,  0.93201953], dtype=float32), 'agent-1': array([0.8333743 , 0.552709  , 0.93201953], dtype=float32)}\n",
      "action:  {'agent-0': array([1.        , 0.62617636, 0.60110706], dtype=float32), 'agent-1': array([ 0.5584619, -0.5198935,  0.507414 ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.5634412109138394, 'agent-1': 0.5634412109138394} \n",
      "\n",
      "obs:  {'agent-0': array([-0.97698504, -0.2133079 ,  0.6871773 ], dtype=float32), 'agent-1': array([0.97698504, 0.2133079 , 0.6871773 ], dtype=float32)}\n",
      "action:  {'agent-0': array([1.       , 0.9802613, 1.       ], dtype=float32), 'agent-1': array([-1.        ,  0.17217219,  0.532557  ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.22046153585737815, 'agent-1': 0.22046153585737815} \n",
      "\n",
      "obs:  {'agent-0': array([0.64176416, 0.766902  , 0.6333001 ], dtype=float32), 'agent-1': array([-0.64176416, -0.766902  ,  0.6333001 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        , -0.14346325,  0.3628085 ], dtype=float32), 'agent-1': array([0.58363426, 0.879091  , 0.37296852], dtype=float32)}\n",
      "reward:  {'agent-0': 1.287468673064995, 'agent-1': 1.287468673064995} \n",
      "\n",
      "obs:  {'agent-0': array([-0.04454952,  0.99900717,  0.26095885], dtype=float32), 'agent-1': array([ 0.04454952, -0.99900717,  0.26095885], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.7015314 , -0.51993906,  0.4760847 ], dtype=float32), 'agent-1': array([-1.        ,  0.20555663,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.6489099134060996, 'agent-1': 0.6489099134060996} \n",
      "\n",
      "obs:  {'agent-0': array([0.9879023 , 0.15507756, 0.2811348 ], dtype=float32), 'agent-1': array([-0.9879023 , -0.15507756,  0.2811348 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        ,  0.34921587,  0.        ], dtype=float32), 'agent-1': array([1.        , 0.38723493, 0.8099668 ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.21340511068210416, 'agent-1': 0.21340511068210416} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8805796 , -0.47389823,  0.44187   ], dtype=float32), 'agent-1': array([0.8805796 , 0.47389823, 0.44187   ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.09036398, 0.7365407 , 0.3342297 ], dtype=float32), 'agent-1': array([ 0.3217175, -1.       ,  1.       ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.6996336092847995, 'agent-1': -0.6996336092847995} \n",
      "\n",
      "obs:  {'agent-0': array([-0.62201065,  0.78300875,  0.81325954], dtype=float32), 'agent-1': array([ 0.62201065, -0.78300875,  0.81325954], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.9459536 , -0.84745383,  0.85860586], dtype=float32), 'agent-1': array([-0.26615357, -0.75040025,  0.42622164], dtype=float32)}\n",
      "reward:  {'agent-0': 1.0691613334236625, 'agent-1': 1.0691613334236625} \n",
      "\n",
      "obs:  {'agent-0': array([0.2442734 , 0.9697064 , 0.46564597], dtype=float32), 'agent-1': array([-0.2442734 , -0.9697064 ,  0.46564597], dtype=float32)}\n",
      "action:  {'agent-0': array([1.        , 0.57292795, 0.        ], dtype=float32), 'agent-1': array([-0.0820238 ,  1.        ,  0.49861783], dtype=float32)}\n",
      "reward:  {'agent-0': 1.1912766833888266, 'agent-1': 1.1912766833888266} \n",
      "\n",
      "obs:  {'agent-0': array([0.9247325, 0.3806177, 0.1830572], dtype=float32), 'agent-1': array([-0.9247325, -0.3806177,  0.1830572], dtype=float32)}\n",
      "action:  {'agent-0': array([-1., -1.,  0.], dtype=float32), 'agent-1': array([1.        , 1.        , 0.15115774], dtype=float32)}\n",
      "reward:  {'agent-0': 1.0362339682437216, 'agent-1': 1.0362339682437216} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.42035323, -0.90736055,  0.07911111], dtype=float32), 'agent-1': array([-0.42035323,  0.90736055,  0.07911111], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.74976814,  0.8810854 ,  0.18243966], dtype=float32), 'agent-1': array([0.69655025, 0.60897756, 0.45643693], dtype=float32)}\n",
      "reward:  {'agent-0': 0.15858048143005388, 'agent-1': 0.15858048143005388} \n",
      "\n",
      "obs:  {'agent-0': array([-0.9095881 , -0.4155112 ,  0.37971783], dtype=float32), 'agent-1': array([0.9095881 , 0.4155112 , 0.37971783], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.42254984,  0.4523331 ,  0.45468152], dtype=float32), 'agent-1': array([-1.       ,  1.       ,  0.7822619], dtype=float32)}\n",
      "reward:  {'agent-0': -0.1123089648616834, 'agent-1': -0.1123089648616834} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.21601455, -0.9763901 ,  0.5805889 ], dtype=float32), 'agent-1': array([-0.21601455,  0.9763901 ,  0.5805889 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.78068614, 0.08622599, 0.19397205], dtype=float32), 'agent-1': array([ 0.8834839, -1.       ,  0.4475537], dtype=float32)}\n",
      "reward:  {'agent-0': 1.1609155294338562, 'agent-1': 1.1609155294338562} \n",
      "\n",
      "obs:  {'agent-0': array([-0.23620293, -0.97170377,  0.27238122], dtype=float32), 'agent-1': array([0.23620293, 0.97170377, 0.27238122], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.5849586,  1.       ,  0.3720439], dtype=float32), 'agent-1': array([ 0.66002226, -0.8257486 ,  0.6675791 ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.6045793830103354, 'agent-1': -0.6045793830103354} \n",
      "\n",
      "obs:  {'agent-0': array([-0.7636358,  0.6456472,  0.6723491], dtype=float32), 'agent-1': array([ 0.7636358, -0.6456472,  0.6723491], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.66622466, -1.        ,  0.8173648 ], dtype=float32), 'agent-1': array([-1.        ,  0.19802666,  0.9053669 ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.8996011156166495, 'agent-1': 0.8996011156166495} \n",
      "\n",
      "obs:  {'agent-0': array([-0.7012272 , -0.7129379 ,  0.42501682], dtype=float32), 'agent-1': array([0.7012272 , 0.7129379 , 0.42501682], dtype=float32)}\n",
      "action:  {'agent-0': array([0.22080457, 1.        , 0.        ], dtype=float32), 'agent-1': array([-1., -1.,  0.], dtype=float32)}\n",
      "reward:  {'agent-0': 0.47038384015754353, 'agent-1': 0.47038384015754353} \n",
      "\n",
      "obs:  {'agent-0': array([-0.7012272 , -0.7129379 ,  0.42501682], dtype=float32), 'agent-1': array([0.7012272 , 0.7129379 , 0.42501682], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        ,  0.69085586,  0.14843869], dtype=float32), 'agent-1': array([ 0.09716368, -1.        ,  0.61988896], dtype=float32)}\n",
      "reward:  {'agent-0': 0.17997342127540272, 'agent-1': 0.17997342127540272} \n",
      "\n",
      "obs:  {'agent-0': array([-0.85956174,  0.5110319 ,  0.51570654], dtype=float32), 'agent-1': array([ 0.85956174, -0.5110319 ,  0.51570654], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.5011568 , -0.53033584,  0.3059718 ], dtype=float32), 'agent-1': array([1.        , 0.26093042, 0.8926419 ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.6454909370417609, 'agent-1': -0.6454909370417609} \n",
      "\n",
      "obs:  {'agent-0': array([-0.9992732 , -0.03811975,  0.8417018 ], dtype=float32), 'agent-1': array([0.9992732 , 0.03811975, 0.8417018 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.9824873, -0.303226 ,  1.       ], dtype=float32), 'agent-1': array([-0.92776334, -0.6439092 ,  0.17041388], dtype=float32)}\n",
      "reward:  {'agent-0': 1.7156428394935372, 'agent-1': 1.7156428394935372} \n",
      "\n",
      "obs:  {'agent-0': array([-0.59127045, -0.8064733 ,  0.2641586 ], dtype=float32), 'agent-1': array([0.59127045, 0.8064733 , 0.2641586 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.1341641 ,  1.        ,  0.05425286], dtype=float32), 'agent-1': array([-1.       ,  0.7401774,  0.5203888], dtype=float32)}\n",
      "reward:  {'agent-0': -0.02752054376952806, 'agent-1': -0.02752054376952806} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.50283426, -0.86438286,  0.50978166], dtype=float32), 'agent-1': array([-0.50283426,  0.86438286,  0.50978166], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.82764316,  1.        ,  0.        ], dtype=float32), 'agent-1': array([ 0.82544446, -0.19402665,  0.18451011], dtype=float32)}\n",
      "reward:  {'agent-0': 0.5271930117626193, 'agent-1': 0.5271930117626193} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.32001406, -0.9474128 ,  0.4503539 ], dtype=float32), 'agent-1': array([-0.32001406,  0.9474128 ,  0.4503539 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.1767683, 1.       , 0.3240258], dtype=float32), 'agent-1': array([ 0.6225579 , -1.        ,  0.63027114], dtype=float32)}\n",
      "reward:  {'agent-0': 0.6835700782906549, 'agent-1': 0.6835700782906549} \n",
      "\n",
      "obs:  {'agent-0': array([-0.3457759 ,  0.9383171 ,  0.36648074], dtype=float32), 'agent-1': array([ 0.3457759 , -0.9383171 ,  0.36648074], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -1.        ,  0.01211268], dtype=float32), 'agent-1': array([-0.07954198,  1.        ,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.588338386587927, 'agent-1': 0.588338386587927} \n",
      "\n",
      "obs:  {'agent-0': array([-0.32996103,  0.9439945 ,  0.35568303], dtype=float32), 'agent-1': array([ 0.32996103, -0.9439945 ,  0.35568303], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.6130139 ,  0.80214024], dtype=float32), 'agent-1': array([-0.22023284, -0.8419734 ,  0.03965795], dtype=float32)}\n",
      "reward:  {'agent-0': 0.08277156123143181, 'agent-1': 0.08277156123143181} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.9966347 , -0.08197079,  0.5141352 ], dtype=float32), 'agent-1': array([-0.9966347 ,  0.08197079,  0.5141352 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.04834747, 0.76778734, 0.8109953 ], dtype=float32), 'agent-1': array([0.5326631, 1.       , 1.       ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.7380869396867386, 'agent-1': 0.7380869396867386} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.37785083, -0.9258665 ,  0.38325524], dtype=float32), 'agent-1': array([-0.37785083,  0.9258665 ,  0.38325524], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.44173026,  1.        ,  0.46823326], dtype=float32), 'agent-1': array([ 0.89191294, -1.        ,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.9198410025824393, 'agent-1': -0.9198410025824393} \n",
      "\n",
      "obs:  {'agent-0': array([-0.66498953,  0.7468527 ,  0.8699927 ], dtype=float32), 'agent-1': array([ 0.66498953, -0.7468527 ,  0.8699927 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.85102683, -0.40606552,  0.8024415 ], dtype=float32), 'agent-1': array([-0.5648669 ,  0.5725889 ,  0.69089293], dtype=float32)}\n",
      "reward:  {'agent-0': 0.13197440178553843, 'agent-1': 0.13197440178553843} \n",
      "\n",
      "obs:  {'agent-0': array([-0.96811527,  0.2505051 ,  0.81311405], dtype=float32), 'agent-1': array([ 0.96811527, -0.2505051 ,  0.81311405], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.       , -1.       ,  0.6425078], dtype=float32), 'agent-1': array([-1.        ,  1.        ,  0.10828802], dtype=float32)}\n",
      "reward:  {'agent-0': 0.9807579818593375, 'agent-1': 0.9807579818593375} \n",
      "\n",
      "obs:  {'agent-0': array([-0.7284958, -0.6850503,  0.4929145], dtype=float32), 'agent-1': array([0.7284958, 0.6850503, 0.4929145], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.52592695, -0.65664023,  0.23509553], dtype=float32), 'agent-1': array([-0.19274801,  0.61872935,  0.37631646], dtype=float32)}\n",
      "reward:  {'agent-0': -0.09517425605534513, 'agent-1': -0.09517425605534513} \n",
      "\n",
      "obs:  {'agent-0': array([-0.3093461 , -0.9509495 ,  0.62386537], dtype=float32), 'agent-1': array([0.3093461 , 0.9509495 , 0.62386537], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.5363308 , -0.48373282,  0.3071725 ], dtype=float32), 'agent-1': array([0.23798418, 1.        , 0.9966903 ], dtype=float32)}\n",
      "reward:  {'agent-0': -1.2136302297038246, 'agent-1': -1.2136302297038246} \n",
      "\n",
      "obs:  {'agent-0': array([-0.3220932 , -0.94670796,  1.1248509 ], dtype=float32), 'agent-1': array([0.3220932 , 0.94670796, 1.1248509 ], dtype=float32)}\n",
      "action:  {'agent-0': array([1., 1., 0.], dtype=float32), 'agent-1': array([-0.65970826, -0.6694545 ,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.7802492402834831, 'agent-1': 0.7802492402834831} \n",
      "\n",
      "obs:  {'agent-0': array([-0.0078241 , -0.99996936,  0.83269536], dtype=float32), 'agent-1': array([0.0078241 , 0.99996936, 0.83269536], dtype=float32)}\n",
      "action:  {'agent-0': array([0.7376307, 1.       , 0.       ], dtype=float32), 'agent-1': array([ 1.        , -0.22941017,  0.5172348 ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.006270324680167194, 'agent-1': 0.006270324680167194} \n",
      "\n",
      "obs:  {'agent-0': array([-0.4078153, -0.9130644,  0.8299648], dtype=float32), 'agent-1': array([0.4078153, 0.9130644, 0.8299648], dtype=float32)}\n",
      "action:  {'agent-0': array([1.        , 0.82806134, 0.74724305], dtype=float32), 'agent-1': array([ 0.19164133, -0.43449914,  0.68890405], dtype=float32)}\n",
      "reward:  {'agent-0': 1.739223631062726, 'agent-1': 1.739223631062726} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.31702524, -0.9484171 ,  0.24451922], dtype=float32), 'agent-1': array([-0.31702524,  0.9484171 ,  0.24451922], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        , -0.18443596,  0.        ], dtype=float32), 'agent-1': array([ 0.438838  , -1.        ,  0.42873162], dtype=float32)}\n",
      "reward:  {'agent-0': 0.8890608538896831, 'agent-1': 0.8890608538896831} \n",
      "\n",
      "obs:  {'agent-0': array([-0.51721305,  0.8558567 ,  0.17728655], dtype=float32), 'agent-1': array([ 0.51721305, -0.8558567 ,  0.17728655], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.84990764, -0.44326764,  0.        ], dtype=float32), 'agent-1': array([-0.7527343 ,  1.        ,  0.01838595], dtype=float32)}\n",
      "reward:  {'agent-0': 0.8517825947838031, 'agent-1': 0.8517825947838031} \n",
      "\n",
      "obs:  {'agent-0': array([-0.5054828 ,  0.8628367 ,  0.15793946], dtype=float32), 'agent-1': array([ 0.5054828 , -0.8628367 ,  0.15793946], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.8079225 ,  0.15713471], dtype=float32), 'agent-1': array([0.42318058, 0.6193826 , 0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.0238713783101852, 'agent-1': 1.0238713783101852} \n",
      "\n",
      "obs:  {'agent-0': array([0.95974886, 0.28085968, 0.07102863], dtype=float32), 'agent-1': array([-0.95974886, -0.28085968,  0.07102863], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.21344471, -0.3456099 ,  0.30959675], dtype=float32), 'agent-1': array([1.        , 0.39431047, 0.6237554 ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.10554347739515624, 'agent-1': -0.10554347739515624} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8260567, -0.563587 ,  0.4634684], dtype=float32), 'agent-1': array([0.8260567, 0.563587 , 0.4634684], dtype=float32)}\n",
      "action:  {'agent-0': array([1.        , 0.18138409, 0.7769426 ], dtype=float32), 'agent-1': array([ 0.18686688, -0.6181265 ,  0.398963  ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.1448964876352052, 'agent-1': 1.1448964876352052} \n",
      "\n",
      "obs:  {'agent-0': array([0.96862656, 0.24852085, 0.20076756], dtype=float32), 'agent-1': array([-0.96862656, -0.24852085,  0.20076756], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        , -0.12362915,  0.51484454], dtype=float32), 'agent-1': array([-0.50604105, -0.2410391 ,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.6231463857267212, 'agent-1': 0.6231463857267212} \n",
      "\n",
      "obs:  {'agent-0': array([-0.9996075 , -0.02801601,  0.2620543 ], dtype=float32), 'agent-1': array([0.9996075 , 0.02801601, 0.2620543 ], dtype=float32)}\n",
      "action:  {'agent-0': array([1.        , 0.49519753, 0.        ], dtype=float32), 'agent-1': array([-0.32137555,  0.13192153,  0.2527354 ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.8551752201947925, 'agent-1': 0.8551752201947925} \n",
      "\n",
      "obs:  {'agent-0': array([-0.98220426, -0.18781593,  0.20066147], dtype=float32), 'agent-1': array([0.98220426, 0.18781593, 0.20066147], dtype=float32)}\n",
      "action:  {'agent-0': array([0.4730687 , 0.05379558, 0.58970606], dtype=float32), 'agent-1': array([-0.5216214 ,  0.30579865,  0.44911984], dtype=float32)}\n",
      "reward:  {'agent-0': 0.5627297250039106, 'agent-1': 0.5627297250039106} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.89459854, -0.4468707 ,  0.2849839 ], dtype=float32), 'agent-1': array([-0.89459854,  0.4468707 ,  0.2849839 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.       ,  0.5976212,  0.087758 ], dtype=float32), 'agent-1': array([ 1.        , -0.04672241,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.8738886428453061, 'agent-1': 0.8738886428453061} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.90918756, -0.4163868 ,  0.20532656], dtype=float32), 'agent-1': array([-0.90918756,  0.4163868 ,  0.20532656], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.8693373,  0.7752441,  0.       ], dtype=float32), 'agent-1': array([-0.02048051, -0.392375  ,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.7720740158862518, 'agent-1': 0.7720740158862518} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.90918756, -0.4163868 ,  0.20532656], dtype=float32), 'agent-1': array([-0.90918756,  0.4163868 ,  0.20532656], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        ,  1.        ,  0.08806384], dtype=float32), 'agent-1': array([ 1.        , -0.50101745,  0.5677253 ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.1729025102550878, 'agent-1': 0.1729025102550878} \n",
      "\n",
      "obs:  {'agent-0': array([-0.85033494,  0.52624184,  0.4236401 ], dtype=float32), 'agent-1': array([ 0.85033494, -0.52624184,  0.4236401 ], dtype=float32)}\n",
      "action:  {'agent-0': array([1.        , 0.09758401, 0.42323807], dtype=float32), 'agent-1': array([-1.       ,  1.       ,  0.4390121], dtype=float32)}\n",
      "reward:  {'agent-0': 0.6659663793684962, 'agent-1': 0.6659663793684962} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.9603406 , -0.27882943,  0.35821465], dtype=float32), 'agent-1': array([-0.9603406 ,  0.27882943,  0.35821465], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        , -0.59563637,  0.        ], dtype=float32), 'agent-1': array([ 1., -1.,  0.], dtype=float32)}\n",
      "reward:  {'agent-0': 0.5692273212195831, 'agent-1': 0.5692273212195831} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.9603406 , -0.27882943,  0.35821465], dtype=float32), 'agent-1': array([-0.9603406 ,  0.27882943,  0.35821465], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.2942829 ,  0.04888189,  0.2098186 ], dtype=float32), 'agent-1': array([ 1.        , -0.37341577,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.6933940765421758, 'agent-1': 0.6933940765421758} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.95457745, -0.297963  ,  0.31385356], dtype=float32), 'agent-1': array([-0.95457745,  0.297963  ,  0.31385356], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.,  1.,  0.], dtype=float32), 'agent-1': array([ 1.        , -0.33684886,  0.09496871], dtype=float32)}\n",
      "reward:  {'agent-0': 0.8316657117775055, 'agent-1': 0.8316657117775055} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.95702976, -0.28998974,  0.2378444 ], dtype=float32), 'agent-1': array([-0.95702976,  0.28998974,  0.2378444 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.       ,  1.       ,  0.5713769], dtype=float32), 'agent-1': array([0.8643069 , 0.23179531, 0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.09820808664650987, 'agent-1': 0.09820808664650987} \n",
      "\n",
      "obs:  {'agent-0': array([-0.5373018 ,  0.84339005,  0.46068022], dtype=float32), 'agent-1': array([ 0.5373018 , -0.84339005,  0.46068022], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.07646394, -0.82812876,  0.25292438], dtype=float32), 'agent-1': array([-0.3587379,  1.       ,  0.       ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.7660034024211284, 'agent-1': 0.7660034024211284} \n",
      "\n",
      "obs:  {'agent-0': array([-0.72041535,  0.6935429 ,  0.34328768], dtype=float32), 'agent-1': array([ 0.72041535, -0.6935429 ,  0.34328768], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.       , -0.1799922,  0.       ], dtype=float32), 'agent-1': array([-1.        ,  0.10958946,  0.32523468], dtype=float32)}\n",
      "reward:  {'agent-0': 0.9090932090847839, 'agent-1': 0.9090932090847839} \n",
      "\n",
      "obs:  {'agent-0': array([0.12056854, 0.992705  , 0.22333594], dtype=float32), 'agent-1': array([-0.12056854, -0.992705  ,  0.22333594], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.45553327, -0.37544364,  0.14881277], dtype=float32), 'agent-1': array([-0.3407421,  1.       ,  0.580837 ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.4100818849727084, 'agent-1': 0.4100818849727084} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.38158888, -0.92433214,  0.35071272], dtype=float32), 'agent-1': array([-0.38158888,  0.92433214,  0.35071272], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        , -0.41383505,  0.        ], dtype=float32), 'agent-1': array([ 0.9959159, -0.5327372,  0.       ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.5799206883086726, 'agent-1': 0.5799206883086726} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.38158888, -0.92433214,  0.35071272], dtype=float32), 'agent-1': array([-0.38158888,  0.92433214,  0.35071272], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.16060895,  0.25693226,  0.7414046 ], dtype=float32), 'agent-1': array([1.      , 1.      , 0.702656], dtype=float32)}\n",
      "reward:  {'agent-0': -0.6972032737330366, 'agent-1': -0.6972032737330366} \n",
      "\n",
      "obs:  {'agent-0': array([-0.5920029 , -0.8059358 ,  0.75013345], dtype=float32), 'agent-1': array([0.5920029 , 0.8059358 , 0.75013345], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.51108485,  1.        ,  0.3730176 ], dtype=float32), 'agent-1': array([-0.2714002 , -0.17943585,  0.6237829 ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.5187295528884872, 'agent-1': 0.5187295528884872} \n",
      "\n",
      "obs:  {'agent-0': array([-0.85425186, -0.5198594 ,  0.58738464], dtype=float32), 'agent-1': array([0.85425186, 0.5198594 , 0.58738464], dtype=float32)}\n",
      "action:  {'agent-0': array([1.        , 1.        , 0.48381725], dtype=float32), 'agent-1': array([-0.99236494, -1.        ,  0.26907673], dtype=float32)}\n",
      "reward:  {'agent-0': 1.1109201000306959, 'agent-1': 1.1109201000306959} \n",
      "\n",
      "obs:  {'agent-0': array([0.19773516, 0.9802555 , 0.29578283], dtype=float32), 'agent-1': array([-0.19773516, -0.9802555 ,  0.29578283], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.18353796, -0.46011615,  0.41348428], dtype=float32), 'agent-1': array([-1.       , -1.       ,  0.6861086], dtype=float32)}\n",
      "reward:  {'agent-0': -0.7302267598484025, 'agent-1': -0.7302267598484025} \n",
      "\n",
      "obs:  {'agent-0': array([0.6313026 , 0.7755366 , 0.72967434], dtype=float32), 'agent-1': array([-0.6313026 , -0.7755366 ,  0.72967434], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        , -0.11285686,  1.        ], dtype=float32), 'agent-1': array([ 1.        , -0.7792668 ,  0.31792346], dtype=float32)}\n",
      "reward:  {'agent-0': -0.08595170914741823, 'agent-1': -0.08595170914741823} \n",
      "\n",
      "obs:  {'agent-0': array([-0.5512519,  0.8343389,  0.7702733], dtype=float32), 'agent-1': array([ 0.5512519, -0.8343389,  0.7702733], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.6340109, -1.       ,  0.9980583], dtype=float32), 'agent-1': array([-0.42543685,  1.        ,  0.59096676], dtype=float32)}\n",
      "reward:  {'agent-0': 0.8257187541123714, 'agent-1': 0.8257187541123714} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.3664666 , -0.9304312 ,  0.51121694], dtype=float32), 'agent-1': array([-0.3664666 ,  0.9304312 ,  0.51121694], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        ,  1.        ,  0.62914217], dtype=float32), 'agent-1': array([ 0.24460316, -1.        ,  0.5624166 ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.12030908654333783, 'agent-1': 0.12030908654333783} \n",
      "\n",
      "obs:  {'agent-0': array([-0.6750583 ,  0.73776436,  0.57295775], dtype=float32), 'agent-1': array([ 0.6750583 , -0.73776436,  0.57295775], dtype=float32)}\n",
      "action:  {'agent-0': array([0.3247044, 0.0842433, 1.       ], dtype=float32), 'agent-1': array([-1.        ,  1.        ,  0.22540873], dtype=float32)}\n",
      "reward:  {'agent-0': 0.9126897911058878, 'agent-1': 0.9126897911058878} \n",
      "\n",
      "obs:  {'agent-0': array([0.06494357, 0.9978889 , 0.35795942], dtype=float32), 'agent-1': array([-0.06494357, -0.9978889 ,  0.35795942], dtype=float32)}\n",
      "action:  {'agent-0': array([0.07682979, 0.24006653, 0.7068264 ], dtype=float32), 'agent-1': array([ 1.        , -0.02690506,  0.5684688 ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.1367374088183766, 'agent-1': -0.1367374088183766} \n",
      "\n",
      "obs:  {'agent-0': array([-0.6205058 ,  0.78420186,  0.57861835], dtype=float32), 'agent-1': array([ 0.6205058 , -0.78420186,  0.57861835], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.9483742 ,  0.35743955], dtype=float32), 'agent-1': array([0.8651937 , 0.79567146, 0.8525525 ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.12738055658982506, 'agent-1': -0.12738055658982506} \n",
      "\n",
      "obs:  {'agent-0': array([-0.9067672 , -0.42163163,  0.6706339 ], dtype=float32), 'agent-1': array([0.9067672 , 0.42163163, 0.6706339 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.13436234,  0.47538188], dtype=float32), 'agent-1': array([-0.09924519, -1.        ,  0.45598397], dtype=float32)}\n",
      "reward:  {'agent-0': 1.2636250975898664, 'agent-1': 1.2636250975898664} \n",
      "\n",
      "obs:  {'agent-0': array([-0.99951714, -0.03107193,  0.29708204], dtype=float32), 'agent-1': array([0.99951714, 0.03107193, 0.29708204], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.5957327 , -0.32031405,  0.6278058 ], dtype=float32), 'agent-1': array([-0.22102177, -0.16243118,  0.39438352], dtype=float32)}\n",
      "reward:  {'agent-0': 0.9709050313670003, 'agent-1': 0.9709050313670003} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.61550266, -0.7881348 ,  0.17185897], dtype=float32), 'agent-1': array([-0.61550266,  0.7881348 ,  0.17185897], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.,  1.,  0.], dtype=float32), 'agent-1': array([ 0.98680425, -0.65912944,  0.6740323 ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.06165628150522845, 'agent-1': -0.06165628150522845} \n",
      "\n",
      "obs:  {'agent-0': array([-0.88014644,  0.47470224,  0.48525137], dtype=float32), 'agent-1': array([ 0.88014644, -0.47470224,  0.48525137], dtype=float32)}\n",
      "action:  {'agent-0': array([0.9136373 , 0.04999685, 0.9304179 ], dtype=float32), 'agent-1': array([-0.06381822, -0.43104464,  0.06112769], dtype=float32)}\n",
      "reward:  {'agent-0': 0.6675302736714622, 'agent-1': 0.6675302736714622} \n",
      "\n",
      "obs:  {'agent-0': array([0.6357866, 0.7718649, 0.391046 ], dtype=float32), 'agent-1': array([-0.6357866, -0.7718649,  0.391046 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        , -0.09644508,  0.9759676 ], dtype=float32), 'agent-1': array([0.0170083 , 0.67143464, 0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.026674180836235406, 'agent-1': 0.026674180836235406} \n",
      "\n",
      "obs:  {'agent-0': array([-0.9253377 ,  0.37914398,  0.5457638 ], dtype=float32), 'agent-1': array([ 0.9253377 , -0.37914398,  0.5457638 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.40437996,  0.5763637 ], dtype=float32), 'agent-1': array([ 0.08121014, -0.9485029 ,  0.97199607], dtype=float32)}\n",
      "reward:  {'agent-0': -0.23353526301605587, 'agent-1': -0.23353526301605587} \n",
      "\n",
      "obs:  {'agent-0': array([-0.1779051 ,  0.98404765,  0.6829608 ], dtype=float32), 'agent-1': array([ 0.1779051 , -0.98404765,  0.6829608 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.6183419, -1.       ,  0.8566425], dtype=float32), 'agent-1': array([-0.00274372, -0.18131232,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.0596108982869543, 'agent-1': 1.0596108982869543} \n",
      "\n",
      "obs:  {'agent-0': array([0.77846944, 0.6276825 , 0.37847745], dtype=float32), 'agent-1': array([-0.77846944, -0.6276825 ,  0.37847745], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.77734804, -0.77814555,  0.75645095], dtype=float32), 'agent-1': array([-0.22738826,  0.9434408 ,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.7043862032057934, 'agent-1': 0.7043862032057934} \n",
      "\n",
      "obs:  {'agent-0': array([-0.6084189 , -0.79361606,  0.32051477], dtype=float32), 'agent-1': array([0.6084189 , 0.79361606, 0.32051477], dtype=float32)}\n",
      "action:  {'agent-0': array([0.7923397 , 0.63238955, 0.        ], dtype=float32), 'agent-1': array([-1., -1.,  0.], dtype=float32)}\n",
      "reward:  {'agent-0': 0.6221631471775435, 'agent-1': 0.6221631471775435} \n",
      "\n",
      "obs:  {'agent-0': array([-0.6084189 , -0.79361606,  0.32051477], dtype=float32), 'agent-1': array([0.6084189 , 0.79361606, 0.32051477], dtype=float32)}\n",
      "action:  {'agent-0': array([1.       , 1.       , 0.5367274], dtype=float32), 'agent-1': array([-0.49018824, -0.41420484,  0.9552251 ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.6225874231762425, 'agent-1': -0.6225874231762425} \n",
      "\n",
      "obs:  {'agent-0': array([0.7747558, 0.6322606, 0.6933593], dtype=float32), 'agent-1': array([-0.7747558, -0.6322606,  0.6933593], dtype=float32)}\n",
      "action:  {'agent-0': array([0.09586811, 0.1153512 , 0.19205078], dtype=float32), 'agent-1': array([1., 1., 1.], dtype=float32)}\n",
      "reward:  {'agent-0': 1.1957173902139202, 'agent-1': 1.1957173902139202} \n",
      "\n",
      "obs:  {'agent-0': array([-0.5132403 , -0.85824496,  0.33815184], dtype=float32), 'agent-1': array([0.5132403 , 0.85824496, 0.33815184], dtype=float32)}\n",
      "action:  {'agent-0': array([1.        , 0.9471266 , 0.32256892], dtype=float32), 'agent-1': array([-1.       , -0.5478749,  0.1208199], dtype=float32)}\n",
      "reward:  {'agent-0': 0.9256528815762572, 'agent-1': 0.9256528815762572} \n",
      "\n",
      "obs:  {'agent-0': array([0.9938517 , 0.11071935, 0.21378008], dtype=float32), 'agent-1': array([-0.9938517 , -0.11071935,  0.21378008], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        ,  0.33694506,  0.24222174], dtype=float32), 'agent-1': array([0.72140753, 0.9160385 , 0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.0220759134926065, 'agent-1': 1.0220759134926065} \n",
      "\n",
      "obs:  {'agent-0': array([-0.04935313,  0.9987814 ,  0.10268039], dtype=float32), 'agent-1': array([ 0.04935313, -0.9987814 ,  0.10268039], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.4181285 , -0.04644388,  0.6855587 ], dtype=float32), 'agent-1': array([-1.       ,  1.       ,  0.5268408], dtype=float32)}\n",
      "reward:  {'agent-0': -0.7425098943231861, 'agent-1': -0.7425098943231861} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.87337613, -0.4870463 ,  0.655094  ], dtype=float32), 'agent-1': array([-0.87337613,  0.4870463 ,  0.655094  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.294003 , -0.6291114,  0.4686431], dtype=float32), 'agent-1': array([ 0.82671726, -0.24375105,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.8938784699150193, 'agent-1': 0.8938784699150193} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.23117225, -0.97291285,  0.41589224], dtype=float32), 'agent-1': array([-0.23117225,  0.97291285,  0.41589224], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.16628194,  0.        ], dtype=float32), 'agent-1': array([ 0.602808 , -1.       ,  0.3132025], dtype=float32)}\n",
      "reward:  {'agent-0': 1.1137636880179418, 'agent-1': 1.1137636880179418} \n",
      "\n",
      "obs:  {'agent-0': array([-0.34620577, -0.93815863,  0.18313742], dtype=float32), 'agent-1': array([0.34620577, 0.93815863, 0.18313742], dtype=float32)}\n",
      "action:  {'agent-0': array([1., 1., 0.], dtype=float32), 'agent-1': array([-1.        ,  0.35574508,  0.7394418 ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.41476823695292275, 'agent-1': -0.41476823695292275} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.82916605, -0.55900234,  0.59215146], dtype=float32), 'agent-1': array([-0.82916605,  0.55900234,  0.59215146], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.19225103,  0.6351805 ,  0.08441764], dtype=float32), 'agent-1': array([ 1.        , -0.6699741 ,  0.51270986], dtype=float32)}\n",
      "reward:  {'agent-0': 1.505700087983774, 'agent-1': 1.505700087983774} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.93272394, -0.36059126,  0.1407066 ], dtype=float32), 'agent-1': array([-0.93272394,  0.36059126,  0.1407066 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.1744067 ,  0.5591662 ,  0.03963524], dtype=float32), 'agent-1': array([ 1.        , -0.45071185,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.7724439421537314, 'agent-1': -0.7724439421537314} \n",
      "\n",
      "obs:  {'agent-0': array([-0.9004174,  0.435027 ,  0.6738448], dtype=float32), 'agent-1': array([ 0.9004174, -0.435027 ,  0.6738448], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.6847739 , -0.51256514,  0.3638259 ], dtype=float32), 'agent-1': array([-1.        ,  0.7207444 ,  0.28751156], dtype=float32)}\n",
      "reward:  {'agent-0': 1.3012369225330833, 'agent-1': 1.3012369225330833} \n",
      "\n",
      "obs:  {'agent-0': array([-0.99720234,  0.07474963,  0.2853776 ], dtype=float32), 'agent-1': array([ 0.99720234, -0.07474963,  0.2853776 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.18725866,  0.        ], dtype=float32), 'agent-1': array([-1.        ,  0.79339504,  0.10678464], dtype=float32)}\n",
      "reward:  {'agent-0': 0.869242578654845, 'agent-1': 0.869242578654845} \n",
      "\n",
      "obs:  {'agent-0': array([-0.9654888, -0.2604445,  0.2074294], dtype=float32), 'agent-1': array([0.9654888, 0.2604445, 0.2074294], dtype=float32)}\n",
      "action:  {'agent-0': array([1.        , 0.45211458, 0.        ], dtype=float32), 'agent-1': array([-0.3078692 ,  0.5439491 ,  0.23791042], dtype=float32)}\n",
      "reward:  {'agent-0': 0.7480855716946301, 'agent-1': 0.7480855716946301} \n",
      "\n",
      "obs:  {'agent-0': array([-0.6189988 , -0.78539187,  0.21608883], dtype=float32), 'agent-1': array([0.6189988 , 0.78539187, 0.21608883], dtype=float32)}\n",
      "action:  {'agent-0': array([0.5525569, 1.       , 0.       ], dtype=float32), 'agent-1': array([ 0.38056242, -0.5872743 ,  0.72246516], dtype=float32)}\n",
      "reward:  {'agent-0': 0.27138742667849786, 'agent-1': 0.27138742667849786} \n",
      "\n",
      "obs:  {'agent-0': array([-0.874907  ,  0.48429096,  0.3953559 ], dtype=float32), 'agent-1': array([ 0.874907  , -0.48429096,  0.3953559 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.16999364, -0.84318435,  0.        ], dtype=float32), 'agent-1': array([-0.8537798,  1.       ,  0.       ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.5150873983401828, 'agent-1': 0.5150873983401828} \n",
      "\n",
      "obs:  {'agent-0': array([-0.874907  ,  0.48429096,  0.3953559 ], dtype=float32), 'agent-1': array([ 0.874907  , -0.48429096,  0.3953559 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.0338614 , -0.53057104,  0.00252938], dtype=float32), 'agent-1': array([0.2454052, 1.       , 0.       ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.5162343320766052, 'agent-1': 0.5162343320766052} \n",
      "\n",
      "obs:  {'agent-0': array([-0.87611973,  0.48209354,  0.39496964], dtype=float32), 'agent-1': array([ 0.87611973, -0.48209354,  0.39496964], dtype=float32)}\n",
      "action:  {'agent-0': array([0.9345691 , 0.22776127, 1.        ], dtype=float32), 'agent-1': array([-1.,  1.,  0.], dtype=float32)}\n",
      "reward:  {'agent-0': 0.1087030359491899, 'agent-1': 0.1087030359491899} \n",
      "\n",
      "obs:  {'agent-0': array([0.7418096 , 0.67061055, 0.5234366 ], dtype=float32), 'agent-1': array([-0.7418096 , -0.67061055,  0.5234366 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.51971793, -1.        ,  0.2984184 ], dtype=float32), 'agent-1': array([0.9509163 , 0.22144139, 0.7952164 ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.8852849963763663, 'agent-1': 0.8852849963763663} \n",
      "\n",
      "obs:  {'agent-0': array([-0.9994543 , -0.03303088,  0.33737648], dtype=float32), 'agent-1': array([0.9994543 , 0.03303088, 0.33737648], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.06735885, -0.66233677,  0.9921829 ], dtype=float32), 'agent-1': array([-1.        , -0.83641434,  0.286197  ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.465747247508271, 'agent-1': 0.465747247508271} \n",
      "\n",
      "obs:  {'agent-0': array([-0.38841072, -0.9214864 ,  0.38373718], dtype=float32), 'agent-1': array([0.38841072, 0.9214864 , 0.38373718], dtype=float32)}\n",
      "action:  {'agent-0': array([0.31064403, 1.        , 0.2809005 ], dtype=float32), 'agent-1': array([ 0.84224725, -0.09675652,  0.20512113], dtype=float32)}\n",
      "reward:  {'agent-0': 0.8732421995497008, 'agent-1': 0.8732421995497008} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8988315 , -0.43829432,  0.26025337], dtype=float32), 'agent-1': array([0.8988315 , 0.43829432, 0.26025337], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.       ,  0.8195119,  0.1884794], dtype=float32), 'agent-1': array([-0.5001183 , -0.4009356 ,  0.46194485], dtype=float32)}\n",
      "reward:  {'agent-0': 0.6830796387587411, 'agent-1': 0.6830796387587411} \n",
      "\n",
      "obs:  {'agent-0': array([-0.73150593,  0.6818351 ,  0.26780295], dtype=float32), 'agent-1': array([ 0.73150593, -0.6818351 ,  0.26780295], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.08511513, -1.        ,  0.5296631 ], dtype=float32), 'agent-1': array([-0.06010383,  1.        ,  0.52172554], dtype=float32)}\n",
      "reward:  {'agent-0': -0.44309642811271566, 'agent-1': -0.44309642811271566} \n",
      "\n",
      "obs:  {'agent-0': array([-0.2723853 , -0.96218824,  0.62865824], dtype=float32), 'agent-1': array([0.2723853 , 0.96218824, 0.62865824], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.9276297 ,  0.43344104,  1.        ], dtype=float32), 'agent-1': array([ 0.04462159, -0.15502048,  0.1166915 ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.3594826574109832, 'agent-1': -0.3594826574109832} \n",
      "\n",
      "obs:  {'agent-0': array([-0.9486653 , -0.31628162,  0.80405134], dtype=float32), 'agent-1': array([0.9486653 , 0.31628162, 0.80405134], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.34712088, -1.        ,  0.44456375], dtype=float32), 'agent-1': array([-0.22137272,  0.8733666 ,  0.5962893 ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.5717229725819937, 'agent-1': -0.5717229725819937} \n",
      "\n",
      "obs:  {'agent-0': array([-0.6607514, -0.7506048,  1.0318664], dtype=float32), 'agent-1': array([0.6607514, 0.7506048, 1.0318664], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.       ,  0.5337374,  1.       ], dtype=float32), 'agent-1': array([-0.68954086, -1.        ,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.2918376601474264, 'agent-1': 0.2918376601474264} \n",
      "\n",
      "obs:  {'agent-0': array([-0.9930751 ,  0.11748137,  0.92205846], dtype=float32), 'agent-1': array([ 0.9930751 , -0.11748137,  0.92205846], dtype=float32)}\n",
      "action:  {'agent-0': array([0.40873098, 1.        , 0.5662116 ], dtype=float32), 'agent-1': array([-1.        ,  0.09948552,  0.4705827 ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.45173143586957254, 'agent-1': 0.45173143586957254} \n",
      "\n",
      "obs:  {'agent-0': array([-0.75462526,  0.656156  ,  0.72403014], dtype=float32), 'agent-1': array([ 0.75462526, -0.656156  ,  0.72403014], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.40778297, -0.32749796,  0.45468208], dtype=float32), 'agent-1': array([-1.,  1.,  0.], dtype=float32)}\n",
      "reward:  {'agent-0': -0.06672169436886577, 'agent-1': -0.06672169436886577} \n",
      "\n",
      "obs:  {'agent-0': array([-0.87420696,  0.48555344,  0.7558643 ], dtype=float32), 'agent-1': array([ 0.87420696, -0.48555344,  0.7558643 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.8591627, -0.2718945,  0.757763 ], dtype=float32), 'agent-1': array([-0.15583742,  0.57085896,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.5443554554145804, 'agent-1': -0.5443554554145804} \n",
      "\n",
      "obs:  {'agent-0': array([-0.9788561 ,  0.20455009,  0.98350316], dtype=float32), 'agent-1': array([ 0.9788561 , -0.20455009,  0.98350316], dtype=float32)}\n",
      "action:  {'agent-0': array([0.48474836, 0.02792978, 0.70130926], dtype=float32), 'agent-1': array([-1.       ,  0.6838691,  1.       ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.7958562638232216, 'agent-1': 1.7958562638232216} \n",
      "\n",
      "obs:  {'agent-0': array([-0.6798957 , -0.73330885,  0.3639312 ], dtype=float32), 'agent-1': array([0.6798957 , 0.73330885, 0.3639312 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.24307597,  1.        ], dtype=float32), 'agent-1': array([-0.88350725,  0.51882386,  0.1079894 ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.5713630323396032, 'agent-1': -0.5713630323396032} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.7887973, -0.6146534,  0.698303 ], dtype=float32), 'agent-1': array([-0.7887973,  0.6146534,  0.698303 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.02207226,  0.5055281 ,  1.        ], dtype=float32), 'agent-1': array([ 0.5795995 , -1.        ,  0.98028845], dtype=float32)}\n",
      "reward:  {'agent-0': 0.23200070849972754, 'agent-1': 0.23200070849972754} \n",
      "\n",
      "obs:  {'agent-0': array([0.23247007, 0.97260356, 0.63613695], dtype=float32), 'agent-1': array([-0.23247007, -0.97260356,  0.63613695], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        , -0.5180644 ,  0.30302367], dtype=float32), 'agent-1': array([ 1.        , -0.20982635,  0.17854425], dtype=float32)}\n",
      "reward:  {'agent-0': 0.30045720822407895, 'agent-1': 0.30045720822407895} \n",
      "\n",
      "obs:  {'agent-0': array([-0.34601974,  0.93822724,  0.58464605], dtype=float32), 'agent-1': array([ 0.34601974, -0.93822724,  0.58464605], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.9594921 ,  0.78046083], dtype=float32), 'agent-1': array([ 0.08747303, -0.59863055,  0.28793564], dtype=float32)}\n",
      "reward:  {'agent-0': 0.7759431886728493, 'agent-1': 0.7759431886728493} \n",
      "\n",
      "obs:  {'agent-0': array([0.94345146, 0.33151063, 0.41158387], dtype=float32), 'agent-1': array([-0.94345146, -0.33151063,  0.41158387], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.6300101 , -0.41311783,  0.33642316], dtype=float32), 'agent-1': array([1.        , 1.        , 0.65689224], dtype=float32)}\n",
      "reward:  {'agent-0': 0.253066931812704, 'agent-1': 0.253066931812704} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.05647334, -0.9984041 ,  0.48739508], dtype=float32), 'agent-1': array([-0.05647334,  0.9984041 ,  0.48739508], dtype=float32)}\n",
      "action:  {'agent-0': array([1., 1., 0.], dtype=float32), 'agent-1': array([ 0.6062031 , -0.63985217,  0.6746712 ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.7850074313625628, 'agent-1': 0.7850074313625628} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8860996 , -0.46349493,  0.35173455], dtype=float32), 'agent-1': array([0.8860996 , 0.46349493, 0.35173455], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.7366761 ,  1.        ,  0.16883677], dtype=float32), 'agent-1': array([-0.95101523, -1.        ,  0.34940118], dtype=float32)}\n",
      "reward:  {'agent-0': 0.6958155988274971, 'agent-1': 0.6958155988274971} \n",
      "\n",
      "obs:  {'agent-0': array([-0.4564067 ,  0.8897713 ,  0.30958378], dtype=float32), 'agent-1': array([ 0.4564067 , -0.8897713 ,  0.30958378], dtype=float32)}\n",
      "action:  {'agent-0': array([0.84145606, 0.26751256, 0.09137952], dtype=float32), 'agent-1': array([1.        , 0.09417129, 0.21213481], dtype=float32)}\n",
      "reward:  {'agent-0': 0.4736852705571474, 'agent-1': 0.4736852705571474} \n",
      "\n",
      "obs:  {'agent-0': array([-0.6767049 ,  0.7362544 ,  0.36782295], dtype=float32), 'agent-1': array([ 0.6767049 , -0.7362544 ,  0.36782295], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.8791971 , -0.8951068 ,  0.24628788], dtype=float32), 'agent-1': array([-1.        ,  0.7541306 ,  0.12205136], dtype=float32)}\n",
      "reward:  {'agent-0': 1.363498959729946, 'agent-1': 1.363498959729946} \n",
      "\n",
      "obs:  {'agent-0': array([0.93068296, 0.36582682, 0.03974331], dtype=float32), 'agent-1': array([-0.93068296, -0.36582682,  0.03974331], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.12234831, -0.5156743 ,  0.47110742], dtype=float32), 'agent-1': array([ 1.       , -0.5927601,  1.       ], dtype=float32)}\n",
      "reward:  {'agent-0': -1.042591002657395, 'agent-1': -1.042591002657395} \n",
      "\n",
      "obs:  {'agent-0': array([-0.94162434,  0.33666548,  0.7338738 ], dtype=float32), 'agent-1': array([ 0.94162434, -0.33666548,  0.7338738 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.06726307,  0.45686564], dtype=float32), 'agent-1': array([-0.82204854,  0.23951101,  0.59434617], dtype=float32)}\n",
      "reward:  {'agent-0': 1.6720690462300343, 'agent-1': 1.6720690462300343} \n",
      "\n",
      "obs:  {'agent-0': array([-0.3622725 ,  0.9320722 ,  0.18692163], dtype=float32), 'agent-1': array([ 0.3622725 , -0.9320722 ,  0.18692163], dtype=float32)}\n",
      "action:  {'agent-0': array([0.34413803, 0.54608345, 0.4959923 ], dtype=float32), 'agent-1': array([0.7702291 , 0.15403605, 0.6320394 ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.13627043554106977, 'agent-1': 0.13627043554106977} \n",
      "\n",
      "obs:  {'agent-0': array([-0.73056763,  0.68284035,  0.42829007], dtype=float32), 'agent-1': array([ 0.73056763, -0.68284035,  0.42829007], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.98960626, -1.        ,  0.25609764], dtype=float32), 'agent-1': array([-0.43618405, -0.14681518,  0.3477024 ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.213281258716712, 'agent-1': 1.213281258716712} \n",
      "\n",
      "obs:  {'agent-0': array([0.0903305 , 0.99591184, 0.1490017 ], dtype=float32), 'agent-1': array([-0.0903305 , -0.99591184,  0.1490017 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.5791428 , -0.17403805,  0.30807978], dtype=float32), 'agent-1': array([0.1938008, 1.       , 0.       ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.7200146385767564, 'agent-1': 0.7200146385767564} \n",
      "\n",
      "obs:  {'agent-0': array([0.87566787, 0.48291388, 0.19912145], dtype=float32), 'agent-1': array([-0.87566787, -0.48291388,  0.19912145], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.3351493 , -1.        ,  0.17153418], dtype=float32), 'agent-1': array([ 1.        , -0.4650569 ,  0.83166504], dtype=float32)}\n",
      "reward:  {'agent-0': -0.10826339504136046, 'agent-1': -0.10826339504136046} \n",
      "\n",
      "obs:  {'agent-0': array([-0.87496954,  0.48417798,  0.5094027 ], dtype=float32), 'agent-1': array([ 0.87496954, -0.48417798,  0.5094027 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -1.        ,  0.06378794], dtype=float32), 'agent-1': array([-1.,  1.,  0.], dtype=float32)}\n",
      "reward:  {'agent-0': 0.5080223321927729, 'agent-1': 0.5080223321927729} \n",
      "\n",
      "obs:  {'agent-0': array([-0.89503235,  0.44600126,  0.45624518], dtype=float32), 'agent-1': array([ 0.89503235, -0.44600126,  0.45624518], dtype=float32)}\n",
      "action:  {'agent-0': array([0.7426915 , 0.34709358, 1.        ], dtype=float32), 'agent-1': array([0.77878547, 0.08288503, 0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.287107054388298, 'agent-1': 0.287107054388298} \n",
      "\n",
      "obs:  {'agent-0': array([0.3489307 , 0.9371485 , 0.49805346], dtype=float32), 'agent-1': array([-0.3489307 , -0.9371485 ,  0.49805346], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        ,  1.        ,  0.50674266], dtype=float32), 'agent-1': array([-0.6042798 ,  1.        ,  0.95959455], dtype=float32)}\n",
      "reward:  {'agent-0': 0.9757356174196292, 'agent-1': 0.9757356174196292} \n",
      "\n",
      "obs:  {'agent-0': array([0.890923  , 0.45415434, 0.2888487 ], dtype=float32), 'agent-1': array([-0.890923  , -0.45415434,  0.2888487 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.35995817, -1.        ,  0.49250928], dtype=float32), 'agent-1': array([0.04644084, 0.22392392, 0.29759425], dtype=float32)}\n",
      "reward:  {'agent-0': 0.10367027426731318, 'agent-1': 0.10367027426731318} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.75018823, -0.6612243 ,  0.47971243], dtype=float32), 'agent-1': array([-0.75018823,  0.6612243 ,  0.47971243], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        , -1.        ,  0.78966755], dtype=float32), 'agent-1': array([ 1., -1.,  0.], dtype=float32)}\n",
      "reward:  {'agent-0': -0.6252083311154815, 'agent-1': -0.6252083311154815} \n",
      "\n",
      "obs:  {'agent-0': array([-0.2642163, -0.9644635,  0.806841 ], dtype=float32), 'agent-1': array([0.2642163, 0.9644635, 0.806841 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.05465412, -0.31620932,  1.        ], dtype=float32), 'agent-1': array([1.       , 0.8761351, 0.7676542], dtype=float32)}\n",
      "reward:  {'agent-0': -1.1798791665861357, 'agent-1': -1.1798791665861357} \n",
      "\n",
      "obs:  {'agent-0': array([-0.42997712, -0.9028398 ,  1.2298445 ], dtype=float32), 'agent-1': array([0.42997712, 0.9028398 , 1.2298445 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        ,  0.9530438 ,  0.98282415], dtype=float32), 'agent-1': array([-0.3706357 , -0.03440428,  0.43500385], dtype=float32)}\n",
      "reward:  {'agent-0': 0.18662051823280468, 'agent-1': 0.18662051823280468} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8336511, -0.5522914,  1.1737435], dtype=float32), 'agent-1': array([0.8336511, 0.5522914, 1.1737435], dtype=float32)}\n",
      "action:  {'agent-0': array([1.        , 1.        , 0.58798385], dtype=float32), 'agent-1': array([-0.30859154, -0.3842185 ,  0.30155376], dtype=float32)}\n",
      "reward:  {'agent-0': 0.939232561734312, 'agent-1': 0.939232561734312} \n",
      "\n",
      "obs:  {'agent-0': array([-0.91238767, -0.40932718,  0.83066493], dtype=float32), 'agent-1': array([0.91238767, 0.40932718, 0.83066493], dtype=float32)}\n",
      "action:  {'agent-0': array([1.       , 0.6928127, 1.       ], dtype=float32), 'agent-1': array([-1.       , -0.5097049,  0.9809163], dtype=float32)}\n",
      "reward:  {'agent-0': 0.2563365000167612, 'agent-1': 0.2563365000167612} \n",
      "\n",
      "obs:  {'agent-0': array([0.7698706 , 0.63820004, 0.712218  ], dtype=float32), 'agent-1': array([-0.7698706 , -0.63820004,  0.712218  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.566134  , -0.44344306,  0.79405963], dtype=float32), 'agent-1': array([1.        , 0.7301631 , 0.32814637], dtype=float32)}\n",
      "reward:  {'agent-0': 0.19120486922176627, 'agent-1': 0.19120486922176627} \n",
      "\n",
      "obs:  {'agent-0': array([0.99703664, 0.07692838, 0.6542252 ], dtype=float32), 'agent-1': array([-0.99703664, -0.07692838,  0.6542252 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        , -0.25466317,  0.20091975], dtype=float32), 'agent-1': array([1.        , 0.19404757, 0.08603147], dtype=float32)}\n",
      "reward:  {'agent-0': 0.6557092064949235, 'agent-1': 0.6557092064949235} \n",
      "\n",
      "obs:  {'agent-0': array([0.9999873 , 0.00503801, 0.49101332], dtype=float32), 'agent-1': array([-0.9999873 , -0.00503801,  0.49101332], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        ,  0.48483014,  0.        ], dtype=float32), 'agent-1': array([-0.58615386, -0.66526747,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.3660289087247267, 'agent-1': 0.3660289087247267} \n",
      "\n",
      "obs:  {'agent-0': array([0.9999873 , 0.00503801, 0.49101332], dtype=float32), 'agent-1': array([-0.9999873 , -0.00503801,  0.49101332], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.6241661 , -1.        ,  0.31261462], dtype=float32), 'agent-1': array([1.        , 0.49511075, 0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.5600598328440644, 'agent-1': 0.5600598328440644} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.8172735 , -0.57624996,  0.4298036 ], dtype=float32), 'agent-1': array([-0.8172735 ,  0.57624996,  0.4298036 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        ,  0.72934747,  0.        ], dtype=float32), 'agent-1': array([ 1.        , -1.        ,  0.04331905], dtype=float32)}\n",
      "reward:  {'agent-0': 0.5835478793011692, 'agent-1': 0.5835478793011692} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.8296986 , -0.55821157,  0.3898125 ], dtype=float32), 'agent-1': array([-0.8296986 ,  0.55821157,  0.3898125 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.7634945 , -0.51832455,  0.        ], dtype=float32), 'agent-1': array([ 0.81495523, -0.24099505,  0.681404  ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.0976801327481192, 'agent-1': 1.0976801327481192} \n",
      "\n",
      "obs:  {'agent-0': array([-0.84318304, -0.5376266 ,  0.17354304], dtype=float32), 'agent-1': array([0.84318304, 0.5376266 , 0.17354304], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.95775604, -0.33877105,  0.44364625], dtype=float32), 'agent-1': array([ 0.6278943, -1.       ,  0.1561949], dtype=float32)}\n",
      "reward:  {'agent-0': 0.8042068698671081, 'agent-1': 0.8042068698671081} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.867042  , -0.4982351 ,  0.17617981], dtype=float32), 'agent-1': array([-0.867042  ,  0.4982351 ,  0.17617981], dtype=float32)}\n",
      "action:  {'agent-0': array([0.15242219, 0.6818501 , 0.        ], dtype=float32), 'agent-1': array([0.42550766, 0.0054307 , 0.64272773], dtype=float32)}\n",
      "reward:  {'agent-0': 0.9012645013230098, 'agent-1': 0.9012645013230098} \n",
      "\n",
      "obs:  {'agent-0': array([-0.7306256 , -0.6827783 ,  0.13601057], dtype=float32), 'agent-1': array([0.7306256 , 0.6827783 , 0.13601057], dtype=float32)}\n",
      "action:  {'agent-0': array([1.        , 0.00996041, 0.21668452], dtype=float32), 'agent-1': array([0.32323897, 0.19396198, 1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.4239898434436303, 'agent-1': 0.4239898434436303} \n",
      "\n",
      "obs:  {'agent-0': array([-0.5902757 , -0.8072017 ,  0.30811104], dtype=float32), 'agent-1': array([0.5902757 , 0.8072017 , 0.30811104], dtype=float32)}\n",
      "action:  {'agent-0': array([1.       , 1.       , 0.6647047], dtype=float32), 'agent-1': array([-1.        , -0.03778976,  0.3407624 ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.40236032285830636, 'agent-1': -0.40236032285830636} \n",
      "\n",
      "obs:  {'agent-0': array([0.8988876, 0.4381793, 0.6321258], dtype=float32), 'agent-1': array([-0.8988876, -0.4381793,  0.6321258], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.75397104, -1.        ,  0.41111258], dtype=float32), 'agent-1': array([1.        , 0.31200695, 0.08750015], dtype=float32)}\n",
      "reward:  {'agent-0': 1.0847653571715297, 'agent-1': 1.0847653571715297} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.99140936, -0.13079558,  0.33534333], dtype=float32), 'agent-1': array([-0.99140936,  0.13079558,  0.33534333], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.,  1.,  0.], dtype=float32), 'agent-1': array([ 0.64395094, -0.01066101,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.6015795783456601, 'agent-1': 0.6015795783456601} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.99140936, -0.13079558,  0.33534333], dtype=float32), 'agent-1': array([-0.99140936,  0.13079558,  0.33534333], dtype=float32)}\n",
      "action:  {'agent-0': array([0.7035397, 1.       , 0.       ], dtype=float32), 'agent-1': array([ 1.       , -0.9967774,  0.5316306], dtype=float32)}\n",
      "reward:  {'agent-0': 0.40450525553456956, 'agent-1': 0.40450525553456956} \n",
      "\n",
      "obs:  {'agent-0': array([-0.27493864,  0.9614618 ,  0.40343478], dtype=float32), 'agent-1': array([ 0.27493864, -0.9614618 ,  0.40343478], dtype=float32)}\n",
      "action:  {'agent-0': array([0.03431332, 0.3042183 , 0.5688838 ], dtype=float32), 'agent-1': array([ 1.        , -0.29259014,  0.34128696], dtype=float32)}\n",
      "reward:  {'agent-0': -0.262272126219351, 'agent-1': -0.262272126219351} \n",
      "\n",
      "obs:  {'agent-0': array([-0.52113664,  0.85347325,  0.6310669 ], dtype=float32), 'agent-1': array([ 0.52113664, -0.85347325,  0.6310669 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.10900879, -1.        ,  0.8539006 ], dtype=float32), 'agent-1': array([-1.        , -0.91894126,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.1204020382446804, 'agent-1': 1.1204020382446804} \n",
      "\n",
      "obs:  {'agent-0': array([-0.96235687, -0.27178898,  0.32179824], dtype=float32), 'agent-1': array([0.96235687, 0.27178898, 0.32179824], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.73858047, -1.        ,  0.        ], dtype=float32), 'agent-1': array([-0.9968736,  1.       ,  0.       ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.6203935918025312, 'agent-1': 0.6203935918025312} \n",
      "\n",
      "obs:  {'agent-0': array([-0.96235687, -0.27178898,  0.32179824], dtype=float32), 'agent-1': array([0.96235687, 0.27178898, 0.32179824], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.26825595, -0.0283429 ,  0.5652168 ], dtype=float32), 'agent-1': array([-0.15795714, -0.36017692,  0.13896203], dtype=float32)}\n",
      "reward:  {'agent-0': 0.9719479155164426, 'agent-1': 0.9719479155164426} \n",
      "\n",
      "obs:  {'agent-0': array([-0.9407091 , -0.33921435,  0.18550752], dtype=float32), 'agent-1': array([0.9407091 , 0.33921435, 0.18550752], dtype=float32)}\n",
      "action:  {'agent-0': array([1.        , 0.61228013, 0.        ], dtype=float32), 'agent-1': array([-1.       , -0.4295755,  0.6371311], dtype=float32)}\n",
      "reward:  {'agent-0': 0.22360065942658602, 'agent-1': 0.22360065942658602} \n",
      "\n",
      "obs:  {'agent-0': array([0.90874124, 0.41736   , 0.39885283], dtype=float32), 'agent-1': array([-0.90874124, -0.41736   ,  0.39885283], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        , -1.        ,  0.78371584], dtype=float32), 'agent-1': array([0.13284707, 1.        , 0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.14863051013130724, 'agent-1': 0.14863051013130724} \n",
      "\n",
      "obs:  {'agent-0': array([-0.5044098 , -0.8634644 ,  0.51326776], dtype=float32), 'agent-1': array([0.5044098 , 0.8634644 , 0.51326776], dtype=float32)}\n",
      "action:  {'agent-0': array([1.        , 1.        , 0.28272372], dtype=float32), 'agent-1': array([0.26855457, 0.75504375, 0.44569612], dtype=float32)}\n",
      "reward:  {'agent-0': 0.35717332340310237, 'agent-1': 0.35717332340310237} \n",
      "\n",
      "obs:  {'agent-0': array([-0.26690465, -0.96372294,  0.5048785 ], dtype=float32), 'agent-1': array([0.26690465, 0.96372294, 0.5048785 ], dtype=float32)}\n",
      "action:  {'agent-0': array([1., 1., 0.], dtype=float32), 'agent-1': array([ 0.02047765, -0.57178295,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.3432157158701371, 'agent-1': 0.3432157158701371} \n",
      "\n",
      "obs:  {'agent-0': array([-0.26690465, -0.96372294,  0.5048785 ], dtype=float32), 'agent-1': array([0.26690465, 0.96372294, 0.5048785 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.08789867,  0.04958367,  1.        ], dtype=float32), 'agent-1': array([-0.7306112 ,  0.26826453,  0.55342644], dtype=float32)}\n",
      "reward:  {'agent-0': 0.1661340148569309, 'agent-1': 0.1661340148569309} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.18936986, -0.9819058 ,  0.55694085], dtype=float32), 'agent-1': array([-0.18936986,  0.9819058 ,  0.55694085], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        , -1.        ,  0.53251016], dtype=float32), 'agent-1': array([-0.26052928, -1.        ,  0.77942216], dtype=float32)}\n",
      "reward:  {'agent-0': 0.7049149413500919, 'agent-1': 0.7049149413500919} \n",
      "\n",
      "obs:  {'agent-0': array([-0.3619837 , -0.93218446,  0.41884527], dtype=float32), 'agent-1': array([0.3619837 , 0.93218446, 0.41884527], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.17197627,  1.        ,  0.        ], dtype=float32), 'agent-1': array([ 0.6041517, -0.3639356,  1.       ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.08307629309848497, 'agent-1': -0.08307629309848497} \n",
      "\n",
      "obs:  {'agent-0': array([-0.98854446, -0.15092984,  0.58869773], dtype=float32), 'agent-1': array([0.98854446, 0.15092984, 0.58869773], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -1.        ,  0.49758905], dtype=float32), 'agent-1': array([-1.        , -0.11335295,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.43110898336252224, 'agent-1': 0.43110898336252224} \n",
      "\n",
      "obs:  {'agent-0': array([-0.43029785, -0.90268695,  0.5219233 ], dtype=float32), 'agent-1': array([0.43029785, 0.90268695, 0.5219233 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.       ,  0.7047994,  0.5592296], dtype=float32), 'agent-1': array([-0.8232704 , -0.36802137,  0.5678604 ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.9114578191844116, 'agent-1': 0.9114578191844116} \n",
      "\n",
      "obs:  {'agent-0': array([-0.99920225, -0.03993518,  0.32707393], dtype=float32), 'agent-1': array([0.99920225, 0.03993518, 0.32707393], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.3411442, -0.3002897,  0.       ], dtype=float32), 'agent-1': array([-0.62740314, -1.        ,  0.10690337], dtype=float32)}\n",
      "reward:  {'agent-0': 0.7221965030590082, 'agent-1': 0.7221965030590082} \n",
      "\n",
      "obs:  {'agent-0': array([-0.9613969 ,  0.27516553,  0.28694713], dtype=float32), 'agent-1': array([ 0.9613969 , -0.27516553,  0.28694713], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.68591774, -1.        ,  0.        ], dtype=float32), 'agent-1': array([-1.       , -0.1011892,  0.543262 ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.7975681842772626, 'agent-1': 0.7975681842772626} \n",
      "\n",
      "obs:  {'agent-0': array([0.83673966, 0.5476009 , 0.23696187], dtype=float32), 'agent-1': array([-0.83673966, -0.5476009 ,  0.23696187], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.09007949,  0.3409254 ], dtype=float32), 'agent-1': array([1., 1., 0.], dtype=float32)}\n",
      "reward:  {'agent-0': 0.11459674668786513, 'agent-1': 0.11459674668786513} \n",
      "\n",
      "obs:  {'agent-0': array([0.9796417 , 0.20075391, 0.4551425 ], dtype=float32), 'agent-1': array([-0.9796417 , -0.20075391,  0.4551425 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.02963686, 0.95839596, 0.5210896 ], dtype=float32), 'agent-1': array([1.       , 0.4675095, 0.       ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.11463962102273284, 'agent-1': -0.11463962102273284} \n",
      "\n",
      "obs:  {'agent-0': array([0.6860959, 0.7275111, 0.6127604], dtype=float32), 'agent-1': array([-0.6860959, -0.7275111,  0.6127604], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.       , -0.7546952,  0.       ], dtype=float32), 'agent-1': array([ 0.6711266 , -0.17689669,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.25105104354507635, 'agent-1': 0.25105104354507635} \n",
      "\n",
      "obs:  {'agent-0': array([-0.11416926,  0.9934613 ,  0.58624876], dtype=float32), 'agent-1': array([ 0.11416926, -0.9934613 ,  0.58624876], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.9700248 , -1.        ,  0.43379217], dtype=float32), 'agent-1': array([0.02943909, 0.95645106, 0.28070033], dtype=float32)}\n",
      "reward:  {'agent-0': 0.7417135494846313, 'agent-1': 0.7417135494846313} \n",
      "\n",
      "obs:  {'agent-0': array([-0.9854331 ,  0.17006347,  0.42380273], dtype=float32), 'agent-1': array([ 0.9854331 , -0.17006347,  0.42380273], dtype=float32)}\n",
      "action:  {'agent-0': array([1.        , 0.35329223, 0.        ], dtype=float32), 'agent-1': array([-0.08716774,  0.2552575 ,  0.47271588], dtype=float32)}\n",
      "reward:  {'agent-0': 0.5680333189374724, 'agent-1': 0.5680333189374724} \n",
      "\n",
      "obs:  {'agent-0': array([-0.997923  , -0.06441737,  0.3919498 ], dtype=float32), 'agent-1': array([0.997923  , 0.06441737, 0.3919498 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.4748094,  0.5754652,  0.       ], dtype=float32), 'agent-1': array([-0.70393443,  1.        ,  0.37182483], dtype=float32)}\n",
      "reward:  {'agent-0': 0.5647885885772507, 'agent-1': 0.5647885885772507} \n",
      "\n",
      "obs:  {'agent-0': array([-0.47455457, -0.8802261 ,  0.37674832], dtype=float32), 'agent-1': array([0.47455457, 0.8802261 , 0.37674832], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.8769824 ,  0.5769813 ,  0.24345258], dtype=float32), 'agent-1': array([-0.6036426, -0.149921 ,  0.7523602], dtype=float32)}\n",
      "reward:  {'agent-0': 1.1549087175530626, 'agent-1': 1.1549087175530626} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.15548146, -0.9878388 ,  0.1409042 ], dtype=float32), 'agent-1': array([-0.15548146,  0.9878388 ,  0.1409042 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.74300504, 1.        , 0.0673514 ], dtype=float32), 'agent-1': array([ 0.19140005, -0.15809011,  0.18378192], dtype=float32)}\n",
      "reward:  {'agent-0': 1.0203133133888134, 'agent-1': 1.0203133133888134} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.5861501 , -0.8102025 ,  0.06344466], dtype=float32), 'agent-1': array([-0.5861501 ,  0.8102025 ,  0.06344466], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.4614442 ,  1.        ,  0.16960609], dtype=float32), 'agent-1': array([ 1., -1.,  0.], dtype=float32)}\n",
      "reward:  {'agent-0': 0.819162272293853, 'agent-1': 0.819162272293853} \n",
      "\n",
      "obs:  {'agent-0': array([-0.3237062 ,  0.94615763,  0.11615426], dtype=float32), 'agent-1': array([ 0.3237062 , -0.94615763,  0.11615426], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.12154984, -1.        ,  0.1859973 ], dtype=float32), 'agent-1': array([-0.17405647,  0.7838247 ,  0.30411288], dtype=float32)}\n",
      "reward:  {'agent-0': 0.5033874362996126, 'agent-1': 0.5033874362996126} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.11510552, -0.99335325,  0.2699438 ], dtype=float32), 'agent-1': array([-0.11510552,  0.99335325,  0.2699438 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.8110409 , -0.08594453,  0.13730636], dtype=float32), 'agent-1': array([0.6203433 , 0.12103581, 0.25642318], dtype=float32)}\n",
      "reward:  {'agent-0': 0.608141231285003, 'agent-1': 0.608141231285003} \n",
      "\n",
      "obs:  {'agent-0': array([-0.03431191, -0.99941117,  0.3007524 ], dtype=float32), 'agent-1': array([0.03431191, 0.99941117, 0.3007524 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.16649508, 0.8072021 , 0.        ], dtype=float32), 'agent-1': array([ 0.12897456, -0.57420826,  0.32443088], dtype=float32)}\n",
      "reward:  {'agent-0': 1.0049081892745537, 'agent-1': 1.0049081892745537} \n",
      "\n",
      "obs:  {'agent-0': array([-0.31148967, -0.95024955,  0.15955034], dtype=float32), 'agent-1': array([0.31148967, 0.95024955, 0.15955034], dtype=float32)}\n",
      "action:  {'agent-0': array([0.8228332 , 1.        , 0.17527455], dtype=float32), 'agent-1': array([ 0.37687635, -0.84849906,  0.09056571], dtype=float32)}\n",
      "reward:  {'agent-0': 0.964580541694789, 'agent-1': 0.964580541694789} \n",
      "\n",
      "obs:  {'agent-0': array([0.5394086 , 0.8420441 , 0.09912235], dtype=float32), 'agent-1': array([-0.5394086 , -0.8420441 ,  0.09912235], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.26129222,  0.9627924 ], dtype=float32), 'agent-1': array([1.        , 1.        , 0.79430175], dtype=float32)}\n",
      "reward:  {'agent-0': -0.8640482989153991, 'agent-1': -0.8640482989153991} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.22832243, -0.97358555,  0.6851779 ], dtype=float32), 'agent-1': array([-0.22832243,  0.97358555,  0.6851779 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.62957656,  0.33706844,  0.6002575 ], dtype=float32), 'agent-1': array([-0.84202117, -0.75671124,  0.83943796], dtype=float32)}\n",
      "reward:  {'agent-0': 0.8509335674335793, 'agent-1': 0.8509335674335793} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.9770888 , -0.2128321 ,  0.44890487], dtype=float32), 'agent-1': array([-0.9770888 ,  0.2128321 ,  0.44890487], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        ,  0.77711654,  0.10931504], dtype=float32), 'agent-1': array([ 0.81155753, -1.        ,  0.27045316], dtype=float32)}\n",
      "reward:  {'agent-0': 0.9164340204403689, 'agent-1': 0.9164340204403689} \n",
      "\n",
      "obs:  {'agent-0': array([0.69155395, 0.72232485, 0.28147346], dtype=float32), 'agent-1': array([-0.69155395, -0.72232485,  0.28147346], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.6857286 , -0.05434585,  0.69200826], dtype=float32), 'agent-1': array([-0.06790501,  0.4057032 ,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.6886852547894362, 'agent-1': 0.6886852547894362} \n",
      "\n",
      "obs:  {'agent-0': array([-0.78479165,  0.61975974,  0.27626547], dtype=float32), 'agent-1': array([ 0.78479165, -0.61975974,  0.27626547], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -1.        ,  0.61447823], dtype=float32), 'agent-1': array([-0.6997949,  0.5914273,  0.3066573], dtype=float32)}\n",
      "reward:  {'agent-0': -0.3479605711505527, 'agent-1': -0.3479605711505527} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.69543993, -0.71858424,  0.60599715], dtype=float32), 'agent-1': array([-0.69543993,  0.71858424,  0.60599715], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        ,  0.73725104,  0.32544568], dtype=float32), 'agent-1': array([ 0.87958777, -1.        ,  0.79964995], dtype=float32)}\n",
      "reward:  {'agent-0': 0.5738073462160589, 'agent-1': 0.5738073462160589} \n",
      "\n",
      "obs:  {'agent-0': array([-0.7138276 ,  0.70032144,  0.48835662], dtype=float32), 'agent-1': array([ 0.7138276 , -0.70032144,  0.48835662], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.21803284, -0.72964865,  1.        ], dtype=float32), 'agent-1': array([-0.5954143,  0.5655992,  0.       ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.8896280038661722, 'agent-1': 0.8896280038661722} \n",
      "\n",
      "obs:  {'agent-0': array([-0.6254492, -0.7802649,  0.3148136], dtype=float32), 'agent-1': array([0.6254492, 0.7802649, 0.3148136], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        ,  0.7327533 ,  0.59648526], dtype=float32), 'agent-1': array([-1.        , -0.06044596,  0.35726053], dtype=float32)}\n",
      "reward:  {'agent-0': 0.3692140020494168, 'agent-1': 0.3692140020494168} \n",
      "\n",
      "obs:  {'agent-0': array([-0.94054383,  0.33967242,  0.4057284 ], dtype=float32), 'agent-1': array([ 0.94054383, -0.33967242,  0.4057284 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.36371756, -0.90241766,  0.15535995], dtype=float32), 'agent-1': array([-1.        ,  1.        ,  0.40054902], dtype=float32)}\n",
      "reward:  {'agent-0': 0.7168021559488256, 'agent-1': 0.7168021559488256} \n",
      "\n",
      "obs:  {'agent-0': array([-0.32313088, -0.94635427,  0.33059528], dtype=float32), 'agent-1': array([0.32313088, 0.94635427, 0.33059528], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.8272543,  1.       ,  0.       ], dtype=float32), 'agent-1': array([-0.2381599 ,  0.5567918 ,  0.49691817], dtype=float32)}\n",
      "reward:  {'agent-0': 0.09677481304238644, 'agent-1': 0.09677481304238644} \n",
      "\n",
      "obs:  {'agent-0': array([-0.01274964, -0.9999187 ,  0.49926555], dtype=float32), 'agent-1': array([0.01274964, 0.9999187 , 0.49926555], dtype=float32)}\n",
      "action:  {'agent-0': array([1., 1., 1.], dtype=float32), 'agent-1': array([1.        , 0.42918658, 0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.40503033215279227, 'agent-1': -0.40503033215279227} \n",
      "\n",
      "obs:  {'agent-0': array([0.9422382 , 0.33494353, 0.7190786 ], dtype=float32), 'agent-1': array([-0.9422382 , -0.33494353,  0.7190786 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.79463434, -0.8477343 ,  0.27888742], dtype=float32), 'agent-1': array([-0.5079144,  1.       ,  0.7591337], dtype=float32)}\n",
      "reward:  {'agent-0': -0.27000319896959946, 'agent-1': -0.27000319896959946} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.8738504 , -0.48619488,  0.8426633 ], dtype=float32), 'agent-1': array([-0.8738504 ,  0.48619488,  0.8426633 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        , -0.22845608,  0.75947255], dtype=float32), 'agent-1': array([ 1.       , -1.       ,  0.3247208], dtype=float32)}\n",
      "reward:  {'agent-0': 1.3286008644003116, 'agent-1': 1.3286008644003116} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.14389661, -0.98959273,  0.40344423], dtype=float32), 'agent-1': array([-0.14389661,  0.98959273,  0.40344423], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        ,  0.5293369 ,  0.44366342], dtype=float32), 'agent-1': array([ 0.05425286, -0.7455959 ,  0.26782584], dtype=float32)}\n",
      "reward:  {'agent-0': 0.7151755568853033, 'agent-1': 0.7151755568853033} \n",
      "\n",
      "obs:  {'agent-0': array([-0.9892123 , -0.14648917,  0.32994965], dtype=float32), 'agent-1': array([0.9892123 , 0.14648917, 0.32994965], dtype=float32)}\n",
      "action:  {'agent-0': array([0.13590932, 0.20785475, 0.        ], dtype=float32), 'agent-1': array([-1.        ,  0.3033961 ,  0.30657655], dtype=float32)}\n",
      "reward:  {'agent-0': 1.050311711150863, 'agent-1': 1.050311711150863} \n",
      "\n",
      "obs:  {'agent-0': array([-0.4703925 , -0.8824573 ,  0.15725431], dtype=float32), 'agent-1': array([0.4703925 , 0.8824573 , 0.15725431], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.0274868,  0.5473405,  0.       ], dtype=float32), 'agent-1': array([-0.30434668,  0.348521  ,  0.27921942], dtype=float32)}\n",
      "reward:  {'agent-0': 0.6750166174781564, 'agent-1': 0.6750166174781564} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.01968543, -0.9998062 ,  0.2212524 ], dtype=float32), 'agent-1': array([-0.01968543,  0.9998062 ,  0.2212524 ], dtype=float32)}\n",
      "action:  {'agent-0': array([1.       , 1.       , 0.6483417], dtype=float32), 'agent-1': array([0.44284117, 0.3010894 , 0.24759054], dtype=float32)}\n",
      "reward:  {'agent-0': -0.020243921149308952, 'agent-1': -0.020243921149308952} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "from gymnasium.wrappers.time_limit import TimeLimit\n",
    "\n",
    "trainin_steps = 30\n",
    "\n",
    "algo = (\n",
    "    PPOConfig()\n",
    "    .training(gamma = 0.99, \n",
    "              lr = 0.001,\n",
    "              train_batch_size = 4096, \n",
    "              sgd_minibatch_size = 256, \n",
    "              num_sgd_iter = 30,\n",
    "              #entropy_coeff=0.005,\n",
    "              )\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"KeepTheDistance?dst=0&agent=2&spawn_area=100\")\n",
    "    .build()\n",
    ")\n",
    "clear_output()\n",
    "\n",
    "out = \"\"\n",
    "for i in range(trainin_steps):\n",
    "    result = algo.train()\n",
    "    clear_output()\n",
    "    out += ppo_result_format(result) + \"\\n\"\n",
    "    print(out)\n",
    "    simulate_episode(RenderableKeepTheDistance(env_config), algo, 300, sleep_between_frames=0.03, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63cb03c360c74fe5bb87b698c62e80a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CanvasWithBorders(height=300, width=300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env_config_2 = EnvironmentConfiguration(n_agents=4, target_distance=0, max_steps=500, speed=1, spawn_area=100)\n",
    "simulate_episode(RenderableKeepTheDistance(env_config_2), algo, 100, sleep_between_frames=0.03, print_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An Algorithm checkpoint has been created inside directory: 'TrainingResult(checkpoint=Checkpoint(filesystem=local, path=/mnt/c/Users/nicol/Desktop/Universit/tesi/experiments/RL_experiments/algos/KeepTheDistance?dst=0&agent=2&spawn_area=100), metrics={'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'grad_gnorm': 5.060915484527746, 'cur_kl_coeff': 0.4500000000000001, 'cur_lr': 0.0010000000000000005, 'total_loss': 7.350311240057151, 'policy_loss': -0.008947773230223296, 'vf_loss': 7.352725898722808, 'vf_explained_var': 0.325551925599575, 'kl': 0.01451804825777196, 'entropy': 3.6460072847704095, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 256.0, 'num_grad_updates_lifetime': 28320.5, 'diff_num_grad_updates_vs_sampler_policy': 479.5}}, 'num_env_steps_sampled': 122880, 'num_env_steps_trained': 122880, 'num_agent_steps_sampled': 245760, 'num_agent_steps_trained': 245760}, 'sampler_results': {'episode_reward_max': 394.43853111585, 'episode_reward_min': 220.2853102145482, 'episode_reward_mean': 295.590107097121, 'episode_len_mean': 300.0, 'episode_media': {}, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [243.18113049447194, 220.2853102145482, 238.36522379893165, 246.55503806014724, 265.97825189132425, 323.6826420157082, 309.5249726850915, 312.7719096310698, 241.33069204301694, 279.4672000787647, 253.3923875689679, 272.4276925007591, 309.3255478594667, 258.43883189275203, 251.24791579648965, 253.96128993780172, 258.87899955352486, 257.7570284964926, 254.75802604157354, 297.61422962531555, 301.46098635754606, 336.88051200592673, 382.4508202158337, 276.66718789043, 300.70432343099975, 244.18001531074356, 305.5723338808965, 247.86133866443123, 286.881498516591, 239.65371165169188, 341.7648313647623, 290.46664403931555, 337.7307775549208, 358.3277110627002, 335.1900730474518, 312.14991016882476, 331.2751499754433, 260.03752404162583, 279.2108459225111, 269.05159958270144, 324.16234911369224, 265.6705044470601, 260.99874918688903, 334.67952709463043, 284.1003328364529, 261.7847682523385, 268.02903719855715, 293.9044202617196, 287.4562336376295, 265.9433480345298, 323.92961742491156, 252.63803345199943, 276.85564316375866, 285.4260363844733, 278.9233504881574, 320.2103491176079, 241.96438358908273, 381.8292422829967, 322.2272608483771, 232.7598023489894, 267.79763944224567, 326.67379803867925, 277.7567325516662, 325.14565845112844, 314.72680922163994, 288.969181407568, 255.7911845301059, 268.05229794633635, 365.0545334800075, 288.31329345874923, 282.7484821932227, 331.7394729916256, 307.19212425269325, 383.34220165709877, 301.5776982350379, 292.17986039037044, 309.05604746735276, 350.6870683858674, 264.8264106138076, 293.3084974714222, 271.7294809165441, 353.0769469995895, 394.43853111585, 318.4101939084111, 271.31033265214836, 329.26097264384856, 305.88660756357604, 327.86281106119435, 260.9570380745676, 339.77570663639887, 311.673397850659, 378.75086842796765, 323.6109294312135, 265.56155070957726, 394.0860729410903, 268.0756464329769, 271.07840160354436, 325.7716648821084, 336.97790045128016, 265.8215591875137], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.46407706739665217, 'mean_inference_ms': 1.120941574826772, 'mean_action_processing_ms': 0.29985688865814664, 'mean_env_wait_ms': 0.5318157332559353, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.007748842239379883, 'StateBufferConnector_ms': 0.006106138229370117, 'ViewRequirementAgentConnector_ms': 0.19031763076782227}, 'num_episodes': 14, 'episode_return_max': 394.43853111585, 'episode_return_min': 220.2853102145482, 'episode_return_mean': 295.590107097121}, 'env_runner_results': {'episode_reward_max': 394.43853111585, 'episode_reward_min': 220.2853102145482, 'episode_reward_mean': 295.590107097121, 'episode_len_mean': 300.0, 'episode_media': {}, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [243.18113049447194, 220.2853102145482, 238.36522379893165, 246.55503806014724, 265.97825189132425, 323.6826420157082, 309.5249726850915, 312.7719096310698, 241.33069204301694, 279.4672000787647, 253.3923875689679, 272.4276925007591, 309.3255478594667, 258.43883189275203, 251.24791579648965, 253.96128993780172, 258.87899955352486, 257.7570284964926, 254.75802604157354, 297.61422962531555, 301.46098635754606, 336.88051200592673, 382.4508202158337, 276.66718789043, 300.70432343099975, 244.18001531074356, 305.5723338808965, 247.86133866443123, 286.881498516591, 239.65371165169188, 341.7648313647623, 290.46664403931555, 337.7307775549208, 358.3277110627002, 335.1900730474518, 312.14991016882476, 331.2751499754433, 260.03752404162583, 279.2108459225111, 269.05159958270144, 324.16234911369224, 265.6705044470601, 260.99874918688903, 334.67952709463043, 284.1003328364529, 261.7847682523385, 268.02903719855715, 293.9044202617196, 287.4562336376295, 265.9433480345298, 323.92961742491156, 252.63803345199943, 276.85564316375866, 285.4260363844733, 278.9233504881574, 320.2103491176079, 241.96438358908273, 381.8292422829967, 322.2272608483771, 232.7598023489894, 267.79763944224567, 326.67379803867925, 277.7567325516662, 325.14565845112844, 314.72680922163994, 288.969181407568, 255.7911845301059, 268.05229794633635, 365.0545334800075, 288.31329345874923, 282.7484821932227, 331.7394729916256, 307.19212425269325, 383.34220165709877, 301.5776982350379, 292.17986039037044, 309.05604746735276, 350.6870683858674, 264.8264106138076, 293.3084974714222, 271.7294809165441, 353.0769469995895, 394.43853111585, 318.4101939084111, 271.31033265214836, 329.26097264384856, 305.88660756357604, 327.86281106119435, 260.9570380745676, 339.77570663639887, 311.673397850659, 378.75086842796765, 323.6109294312135, 265.56155070957726, 394.0860729410903, 268.0756464329769, 271.07840160354436, 325.7716648821084, 336.97790045128016, 265.8215591875137], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.46407706739665217, 'mean_inference_ms': 1.120941574826772, 'mean_action_processing_ms': 0.29985688865814664, 'mean_env_wait_ms': 0.5318157332559353, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.007748842239379883, 'StateBufferConnector_ms': 0.006106138229370117, 'ViewRequirementAgentConnector_ms': 0.19031763076782227}, 'num_episodes': 14, 'episode_return_max': 394.43853111585, 'episode_return_min': 220.2853102145482, 'episode_return_mean': 295.590107097121}, 'episode_reward_max': 394.43853111585, 'episode_reward_min': 220.2853102145482, 'episode_reward_mean': 295.590107097121, 'episode_len_mean': 300.0, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [243.18113049447194, 220.2853102145482, 238.36522379893165, 246.55503806014724, 265.97825189132425, 323.6826420157082, 309.5249726850915, 312.7719096310698, 241.33069204301694, 279.4672000787647, 253.3923875689679, 272.4276925007591, 309.3255478594667, 258.43883189275203, 251.24791579648965, 253.96128993780172, 258.87899955352486, 257.7570284964926, 254.75802604157354, 297.61422962531555, 301.46098635754606, 336.88051200592673, 382.4508202158337, 276.66718789043, 300.70432343099975, 244.18001531074356, 305.5723338808965, 247.86133866443123, 286.881498516591, 239.65371165169188, 341.7648313647623, 290.46664403931555, 337.7307775549208, 358.3277110627002, 335.1900730474518, 312.14991016882476, 331.2751499754433, 260.03752404162583, 279.2108459225111, 269.05159958270144, 324.16234911369224, 265.6705044470601, 260.99874918688903, 334.67952709463043, 284.1003328364529, 261.7847682523385, 268.02903719855715, 293.9044202617196, 287.4562336376295, 265.9433480345298, 323.92961742491156, 252.63803345199943, 276.85564316375866, 285.4260363844733, 278.9233504881574, 320.2103491176079, 241.96438358908273, 381.8292422829967, 322.2272608483771, 232.7598023489894, 267.79763944224567, 326.67379803867925, 277.7567325516662, 325.14565845112844, 314.72680922163994, 288.969181407568, 255.7911845301059, 268.05229794633635, 365.0545334800075, 288.31329345874923, 282.7484821932227, 331.7394729916256, 307.19212425269325, 383.34220165709877, 301.5776982350379, 292.17986039037044, 309.05604746735276, 350.6870683858674, 264.8264106138076, 293.3084974714222, 271.7294809165441, 353.0769469995895, 394.43853111585, 318.4101939084111, 271.31033265214836, 329.26097264384856, 305.88660756357604, 327.86281106119435, 260.9570380745676, 339.77570663639887, 311.673397850659, 378.75086842796765, 323.6109294312135, 265.56155070957726, 394.0860729410903, 268.0756464329769, 271.07840160354436, 325.7716648821084, 336.97790045128016, 265.8215591875137], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.46407706739665217, 'mean_inference_ms': 1.120941574826772, 'mean_action_processing_ms': 0.29985688865814664, 'mean_env_wait_ms': 0.5318157332559353, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.007748842239379883, 'StateBufferConnector_ms': 0.006106138229370117, 'ViewRequirementAgentConnector_ms': 0.19031763076782227}, 'num_episodes': 14, 'episode_return_max': 394.43853111585, 'episode_return_min': 220.2853102145482, 'episode_return_mean': 295.590107097121, 'num_healthy_workers': 1, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 245760, 'num_agent_steps_trained': 245760, 'num_env_steps_sampled': 122880, 'num_env_steps_trained': 122880, 'num_env_steps_sampled_this_iter': 4096, 'num_env_steps_trained_this_iter': 4096, 'num_env_steps_sampled_throughput_per_sec': 187.17926032773792, 'num_env_steps_trained_throughput_per_sec': 187.17926032773792, 'timesteps_total': 122880, 'num_env_steps_sampled_lifetime': 122880, 'num_agent_steps_sampled_lifetime': 245760, 'num_steps_trained_this_iter': 4096, 'agent_timesteps_total': 245760, 'timers': {'training_iteration_time_ms': 19309.428, 'restore_workers_time_ms': 0.022, 'training_step_time_ms': 19309.365, 'sample_time_ms': 10564.325, 'load_time_ms': 0.775, 'load_throughput': 5286438.914, 'learn_time_ms': 8738.104, 'learn_throughput': 468.752, 'synch_weights_time_ms': 5.437}, 'counters': {'num_env_steps_sampled': 122880, 'num_env_steps_trained': 122880, 'num_agent_steps_sampled': 245760, 'num_agent_steps_trained': 245760}, 'done': False, 'episodes_total': 409, 'training_iteration': 30, 'trial_id': 'default', 'date': '2024-05-28_15-28-40', 'timestamp': 1716902920, 'time_this_iter_s': 21.889414310455322, 'time_total_s': 549.4043662548065, 'pid': 666, 'hostname': 'LAPTOP-9AD2MD1C', 'node_ip': '172.23.87.11', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'enable_rl_module_and_learner': False, 'enable_env_runner_and_connector_v2': False, 'env': 'KeepTheDistance?dst=0&agent=2&spawn_area=100', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'env_task_fn': None, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_env_runner': 1, 'validate_env_runners_after_construction': True, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'enable_connectors': True, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.001, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 4096, 'train_batch_size_per_learner': None, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x7f0b669942c0>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'always_attach_evaluation_results': True, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_run_training_always_in_thread': False, '_evaluation_parallel_to_training_wo_thread': False, 'ignore_env_runner_failures': False, 'recreate_failed_env_runners': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30, 'env_runner_restore_timeout_s': 1800, '_model_config_dict': {}, '_rl_module_spec': None, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'disable_env_checking': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'sgd_minibatch_size': 256, 'mini_batch_size_per_learner': None, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 1}, 'time_since_restore': 549.4043662548065, 'iterations_since_restore': 30, 'perf': {'cpu_util_percent': 46.20652173913044, 'ram_util_percent': 85.0195652173913}})'.\n"
     ]
    }
   ],
   "source": [
    "save_algo(algo, \"KeepTheDistance?dst=0&agent=2&spawn_area=100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KeepTheDistance?dst=0&visible_nbrs=2&spawn_area=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "env_config = EnvironmentConfiguration(n_agents=3, visible_nbrs=2, target_distance=0, max_steps=300, speed=1, spawn_area=100)\n",
    "register_env(\"KeepTheDistance?dst=0&visible_nbrs=2&spawn_area=100\", lambda _: KeepTheDistance(env_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-28 16:49:18,114\tWARNING deprecation.py:50 -- DeprecationWarning: `WorkerSet(num_workers=... OR local_worker=...)` has been deprecated. Use `EnvRunnerGroup(num_env_runners=... AND local_env_runner=...)` instead. This will raise an error in the future!\n",
      "2024-05-28 16:49:20,496\tERROR actor_manager.py:519 -- Ray error, taking actor 1 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=15562, ip=172.23.87.11, actor_id=b5dbde7ce66578d46964347001000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fb41e9daf90>)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/gymnasium/envs/registration.py\", line 740, in make\n",
      "    env_spec = _find_spec(id)\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/gymnasium/envs/registration.py\", line 519, in _find_spec\n",
      "    ns, name, version = parse_env_id(env_name)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/gymnasium/envs/registration.py\", line 286, in parse_env_id\n",
      "    raise error.Error(\n",
      "gymnasium.error.Error: Malformed environment ID: KeepTheDistance?dst=0&visible_nbrs=2&spawn_area=100. (Currently all IDs must be of the form [namespace/](env-name)-v(version). (namespace is optional))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=15562, ip=172.23.87.11, actor_id=b5dbde7ce66578d46964347001000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fb41e9daf90>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 407, in __init__\n",
      "    self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/env/utils/__init__.py\", line 115, in _gym_env_creator\n",
      "    raise EnvError(ERR_MSG_INVALID_ENV_DESCRIPTOR.format(env_descriptor))\n",
      "ray.rllib.utils.error.EnvError: The env string you provided ('KeepTheDistance?dst=0&visible_nbrs=2&spawn_area=100') is:\n",
      "a) Not a supported/installed environment.\n",
      "b) Not a tune-registered environment creator.\n",
      "c) Not a valid env class string.\n",
      "\n",
      "Try one of the following:\n",
      "a) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.\n",
      "   For PyBullet support: `pip install pybullet`.\n",
      "b) To register your custom env, do `from ray import tune;\n",
      "   tune.register('[name]', lambda cfg: [return env obj from here using cfg])`.\n",
      "   Then in your config, do `config['env'] = [name]`.\n",
      "c) Make sure you provide a fully qualified classpath, e.g.:\n",
      "   `ray.rllib.examples.envs.classes.repeat_after_me_env.RepeatAfterMeEnv`\n"
     ]
    },
    {
     "ename": "EnvError",
     "evalue": "The env string you provided ('KeepTheDistance?dst=0&visible_nbrs=2&spawn_area=100') is:\na) Not a supported/installed environment.\nb) Not a tune-registered environment creator.\nc) Not a valid env class string.\n\nTry one of the following:\na) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.\n   For PyBullet support: `pip install pybullet`.\nb) To register your custom env, do `from ray import tune;\n   tune.register('[name]', lambda cfg: [return env obj from here using cfg])`.\n   Then in your config, do `config['env'] = [name]`.\nc) Make sure you provide a fully qualified classpath, e.g.:\n   `ray.rllib.examples.envs.classes.repeat_after_me_env.RepeatAfterMeEnv`\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mActorDiedError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py:169\u001b[0m, in \u001b[0;36mEnvRunnerGroup.__init__\u001b[0;34m(self, env_creator, validate_env, default_policy_class, config, num_env_runners, local_env_runner, logdir, _setup, num_workers, local_worker)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_env_runners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_env_runners\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_env_runner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_env_runner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# EnvRunnerGroup creation possibly fails, if some (remote) workers cannot\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# be initialized properly (due to some errors in the EnvRunners's\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# constructor).\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py:239\u001b[0m, in \u001b[0;36mEnvRunnerGroup._setup\u001b[0;34m(self, validate_env, config, num_env_runners, local_env_runner)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# Create a number of @ray.remote workers.\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_workers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_env_runners\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_env_runners_after_construction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# If num_workers > 0 and we don't have an env on the local worker,\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m# get the observation- and action spaces for each policy from\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# the first remote worker (which does have an env).\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py:754\u001b[0m, in \u001b[0;36mEnvRunnerGroup.add_workers\u001b[0;34m(self, num_workers, validate)\u001b[0m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39mok:\n\u001b[0;32m--> 754\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m result\u001b[38;5;241m.\u001b[39mget()\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/utils/actor_manager.py:497\u001b[0m, in \u001b[0;36mFaultTolerantActorManager._fetch_result\u001b[0;34m(self, remote_actor_ids, remote_calls, tags, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m     remote_results\u001b[38;5;241m.\u001b[39madd_result(actor_id, ResultOrError(result\u001b[38;5;241m=\u001b[39mresult), tag)\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py:21\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m auto_init_ray()\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py:2623\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2622\u001b[0m \u001b[38;5;66;03m# TODO(ujvl): Consider how to allow user to retrieve the ready objects.\u001b[39;00m\n\u001b[0;32m-> 2623\u001b[0m values, debugger_breakpoint \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2624\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(values):\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py:863\u001b[0m, in \u001b[0;36mWorker.get_objects\u001b[0;34m(self, object_refs, timeout)\u001b[0m\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 863\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[1;32m    864\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m values, debugger_breakpoint\n",
      "\u001b[0;31mActorDiedError\u001b[0m: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=15562, ip=172.23.87.11, actor_id=b5dbde7ce66578d46964347001000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fb41e9daf90>)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/gymnasium/envs/registration.py\", line 740, in make\n    env_spec = _find_spec(id)\n               ^^^^^^^^^^^^^^\n  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/gymnasium/envs/registration.py\", line 519, in _find_spec\n    ns, name, version = parse_env_id(env_name)\n                        ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/gymnasium/envs/registration.py\", line 286, in parse_env_id\n    raise error.Error(\ngymnasium.error.Error: Malformed environment ID: KeepTheDistance?dst=0&visible_nbrs=2&spawn_area=100. (Currently all IDs must be of the form [namespace/](env-name)-v(version). (namespace is optional))\n\nDuring handling of the above exception, another exception occurred:\n\n\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=15562, ip=172.23.87.11, actor_id=b5dbde7ce66578d46964347001000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fb41e9daf90>)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 407, in __init__\n    self.env = env_creator(copy.deepcopy(self.env_context))\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/env/utils/__init__.py\", line 115, in _gym_env_creator\n    raise EnvError(ERR_MSG_INVALID_ENV_DESCRIPTOR.format(env_descriptor))\nray.rllib.utils.error.EnvError: The env string you provided ('KeepTheDistance?dst=0&visible_nbrs=2&spawn_area=100') is:\na) Not a supported/installed environment.\nb) Not a tune-registered environment creator.\nc) Not a valid env class string.\n\nTry one of the following:\na) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.\n   For PyBullet support: `pip install pybullet`.\nb) To register your custom env, do `from ray import tune;\n   tune.register('[name]', lambda cfg: [return env obj from here using cfg])`.\n   Then in your config, do `config['env'] = [name]`.\nc) Make sure you provide a fully qualified classpath, e.g.:\n   `ray.rllib.examples.envs.classes.repeat_after_me_env.RepeatAfterMeEnv`",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mEnvError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[130], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m algo \u001b[38;5;241m=\u001b[39m \u001b[43mload_algo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mKeepTheDistance?dst=0&visible_nbrs=2&spawn_area=100\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[118], line 35\u001b[0m, in \u001b[0;36mload_algo\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(subfolder_path):\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe specified subfolder \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubfolder_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m does not exist.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mAlgorithm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubfolder_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:363\u001b[0m, in \u001b[0;36mAlgorithm.from_checkpoint\u001b[0;34m(checkpoint, policy_ids, policy_mapping_fn, policies_to_train)\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    346\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to restore a multi-agent algorithm from a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    347\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`msgpack` formatted checkpoint, which do NOT store the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    353\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou would like to train all policies anyways.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    354\u001b[0m             )\n\u001b[1;32m    356\u001b[0m state \u001b[38;5;241m=\u001b[39m Algorithm\u001b[38;5;241m.\u001b[39m_checkpoint_info_to_algorithm_state(\n\u001b[1;32m    357\u001b[0m     checkpoint_info\u001b[38;5;241m=\u001b[39mcheckpoint_info,\n\u001b[1;32m    358\u001b[0m     policy_ids\u001b[38;5;241m=\u001b[39mpolicy_ids,\n\u001b[1;32m    359\u001b[0m     policy_mapping_fn\u001b[38;5;241m=\u001b[39mpolicy_mapping_fn,\n\u001b[1;32m    360\u001b[0m     policies_to_train\u001b[38;5;241m=\u001b[39mpolicies_to_train,\n\u001b[1;32m    361\u001b[0m )\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mAlgorithm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:391\u001b[0m, in \u001b[0;36mAlgorithm.from_state\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config:\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo `config` found in given Algorithm state!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 391\u001b[0m new_algo \u001b[38;5;241m=\u001b[39m \u001b[43malgorithm_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;66;03m# Set the new algo's state.\u001b[39;00m\n\u001b[1;32m    393\u001b[0m new_algo\u001b[38;5;241m.\u001b[39m__setstate__(state)\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:554\u001b[0m, in \u001b[0;36mAlgorithm.__init__\u001b[0;34m(self, config, env, logger_creator, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;66;03m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;66;03m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;66;03m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;66;03m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;66;03m# TODO: Don't dump sampler results into top-level.\u001b[39;00m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    551\u001b[0m     },\n\u001b[1;32m    552\u001b[0m }\n\u001b[0;32m--> 554\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/trainable/trainable.py:158\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, storage)\u001b[0m\n\u001b[1;32m    154\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStorageContext on the TRAINABLE:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mstorage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open_logfiles(stdout_file, stderr_file)\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m setup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m setup_time \u001b[38;5;241m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:640\u001b[0m, in \u001b[0;36mAlgorithm.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moff_policy_estimation_methods \u001b[38;5;241m=\u001b[39m ope_dict\n\u001b[1;32m    639\u001b[0m \u001b[38;5;66;03m# Create a set of env runner actors via a EnvRunnerGroup.\u001b[39;00m\n\u001b[0;32m--> 640\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers \u001b[38;5;241m=\u001b[39m \u001b[43mEnvRunnerGroup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_policy_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_policy_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_env_runners\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;66;03m# Ensure remote workers are initially in sync with the local worker.\u001b[39;00m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers\u001b[38;5;241m.\u001b[39msync_weights(inference_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py:191\u001b[0m, in \u001b[0;36mEnvRunnerGroup.__init__\u001b[0;34m(self, env_creator, validate_env, default_policy_class, config, num_env_runners, local_env_runner, logdir, _setup, num_workers, local_worker)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RayActorError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;66;03m# In case of an actor (remote worker) init failure, the remote worker\u001b[39;00m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;66;03m# may still exist and will be accessible, however, e.g. calling\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;66;03m# its `sample.remote()` would result in strange \"property not found\"\u001b[39;00m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;66;03m# errors.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mactor_init_failed:\n\u001b[1;32m    184\u001b[0m         \u001b[38;5;66;03m# Raise the original error here that the EnvRunners raised\u001b[39;00m\n\u001b[1;32m    185\u001b[0m         \u001b[38;5;66;03m# during its construction process. This is to enforce transparency\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;66;03m# - e.args[0].args[2]: The original Exception (e.g. a ValueError due\u001b[39;00m\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;66;03m# to a config mismatch) thrown inside the actor.\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# In any other case, raise the RayActorError as-is.\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[0;31mEnvError\u001b[0m: The env string you provided ('KeepTheDistance?dst=0&visible_nbrs=2&spawn_area=100') is:\na) Not a supported/installed environment.\nb) Not a tune-registered environment creator.\nc) Not a valid env class string.\n\nTry one of the following:\na) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.\n   For PyBullet support: `pip install pybullet`.\nb) To register your custom env, do `from ray import tune;\n   tune.register('[name]', lambda cfg: [return env obj from here using cfg])`.\n   Then in your config, do `config['env'] = [name]`.\nc) Make sure you provide a fully qualified classpath, e.g.:\n   `ray.rllib.examples.envs.classes.repeat_after_me_env.RepeatAfterMeEnv`\n"
     ]
    }
   ],
   "source": [
    "algo = load_algo(\"KeepTheDistance?dst=0&visible_nbrs=2&spawn_area=100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88299ad3bf224b79aa3995f1f072e0d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CanvasWithBorders(height=300, width=300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration [1] => episode_reward_mean: -9.076550823166727, episode_len_mean: 300.0, agent_steps_trained: 12276, env_steps_trained: 4092, entropy: 4.224508220689339, learning_rate: 0.0010000000000000002\n",
      "iteration [2] => episode_reward_mean: 69.10403511569474, episode_len_mean: 300.0, agent_steps_trained: 24552, env_steps_trained: 8184, entropy: 4.239348649644015, learning_rate: 0.0010000000000000002\n",
      "iteration [3] => episode_reward_mean: 132.94989005653392, episode_len_mean: 300.0, agent_steps_trained: 36828, env_steps_trained: 12276, entropy: 4.217899766135634, learning_rate: 0.0010000000000000002\n",
      "iteration [4] => episode_reward_mean: 190.06564283236446, episode_len_mean: 300.0, agent_steps_trained: 49104, env_steps_trained: 16368, entropy: 4.273230808324981, learning_rate: 0.0010000000000000002\n",
      "iteration [5] => episode_reward_mean: 234.7931710832915, episode_len_mean: 300.0, agent_steps_trained: 61380, env_steps_trained: 20460, entropy: 4.26117914777053, learning_rate: 0.0010000000000000002\n",
      "iteration [6] => episode_reward_mean: 272.0990945973401, episode_len_mean: 300.0, agent_steps_trained: 73656, env_steps_trained: 24552, entropy: 4.195444786674098, learning_rate: 0.0010000000000000002\n",
      "iteration [7] => episode_reward_mean: 311.0583395013724, episode_len_mean: 300.0, agent_steps_trained: 85932, env_steps_trained: 28644, entropy: 4.1384101019407575, learning_rate: 0.0010000000000000002\n",
      "iteration [8] => episode_reward_mean: 384.14471846854946, episode_len_mean: 300.0, agent_steps_trained: 98208, env_steps_trained: 32736, entropy: 4.069837636696665, learning_rate: 0.0010000000000000002\n",
      "iteration [9] => episode_reward_mean: 453.8653543685983, episode_len_mean: 300.0, agent_steps_trained: 110484, env_steps_trained: 36828, entropy: 4.017635357421741, learning_rate: 0.0010000000000000002\n",
      "iteration [10] => episode_reward_mean: 523.8376849772401, episode_len_mean: 300.0, agent_steps_trained: 122760, env_steps_trained: 40920, entropy: 3.9984360255693137, learning_rate: 0.0010000000000000002\n",
      "iteration [11] => episode_reward_mean: 584.0995118479473, episode_len_mean: 300.0, agent_steps_trained: 135036, env_steps_trained: 45012, entropy: 3.905055452815273, learning_rate: 0.0010000000000000002\n",
      "iteration [12] => episode_reward_mean: 644.7044652432309, episode_len_mean: 300.0, agent_steps_trained: 147312, env_steps_trained: 49104, entropy: 3.8292210965407523, learning_rate: 0.0010000000000000002\n",
      "iteration [13] => episode_reward_mean: 703.3936314627672, episode_len_mean: 300.0, agent_steps_trained: 159588, env_steps_trained: 53196, entropy: 3.823683662330895, learning_rate: 0.0010000000000000002\n",
      "iteration [14] => episode_reward_mean: 748.9858661059598, episode_len_mean: 300.0, agent_steps_trained: 171864, env_steps_trained: 57288, entropy: 3.7876506351169787, learning_rate: 0.0010000000000000002\n",
      "iteration [15] => episode_reward_mean: 800.0903960878179, episode_len_mean: 300.0, agent_steps_trained: 184140, env_steps_trained: 61380, entropy: 3.734442796539842, learning_rate: 0.0010000000000000002\n",
      "iteration [16] => episode_reward_mean: 849.5560239443756, episode_len_mean: 300.0, agent_steps_trained: 196416, env_steps_trained: 65472, entropy: 3.7083602380752563, learning_rate: 0.0010000000000000002\n",
      "iteration [17] => episode_reward_mean: 891.2039437266408, episode_len_mean: 300.0, agent_steps_trained: 208692, env_steps_trained: 69564, entropy: 3.6489802442517196, learning_rate: 0.0010000000000000002\n",
      "iteration [18] => episode_reward_mean: 936.7435365473165, episode_len_mean: 300.0, agent_steps_trained: 220968, env_steps_trained: 73656, entropy: 3.5915524504477516, learning_rate: 0.0010000000000000002\n",
      "iteration [19] => episode_reward_mean: 975.7648664382938, episode_len_mean: 300.0, agent_steps_trained: 233244, env_steps_trained: 77748, entropy: 3.5519524389400816, learning_rate: 0.0010000000000000002\n",
      "iteration [20] => episode_reward_mean: 1015.3156579382559, episode_len_mean: 300.0, agent_steps_trained: 245520, env_steps_trained: 81840, entropy: 3.5002174873519363, learning_rate: 0.0010000000000000002\n",
      "iteration [21] => episode_reward_mean: 1061.0219783236907, episode_len_mean: 300.0, agent_steps_trained: 257796, env_steps_trained: 85932, entropy: 3.463580599500422, learning_rate: 0.0010000000000000002\n",
      "iteration [22] => episode_reward_mean: 1107.8741236363337, episode_len_mean: 300.0, agent_steps_trained: 270072, env_steps_trained: 90024, entropy: 3.414194972975212, learning_rate: 0.0010000000000000002\n",
      "iteration [23] => episode_reward_mean: 1150.6196513995862, episode_len_mean: 300.0, agent_steps_trained: 282348, env_steps_trained: 94116, entropy: 3.3864696424049243, learning_rate: 0.0010000000000000002\n",
      "iteration [24] => episode_reward_mean: 1193.3478206582513, episode_len_mean: 300.0, agent_steps_trained: 294624, env_steps_trained: 98208, entropy: 3.3546639004088283, learning_rate: 0.0010000000000000002\n",
      "iteration [25] => episode_reward_mean: 1232.9218706195584, episode_len_mean: 300.0, agent_steps_trained: 306900, env_steps_trained: 102300, entropy: 3.288957563952396, learning_rate: 0.0010000000000000002\n",
      "iteration [26] => episode_reward_mean: 1265.8748371707386, episode_len_mean: 300.0, agent_steps_trained: 319176, env_steps_trained: 106392, entropy: 3.264656237384729, learning_rate: 0.0010000000000000002\n",
      "iteration [27] => episode_reward_mean: 1304.381817401921, episode_len_mean: 300.0, agent_steps_trained: 331452, env_steps_trained: 110484, entropy: 3.27218609090437, learning_rate: 0.0010000000000000002\n",
      "iteration [28] => episode_reward_mean: 1328.8604772691738, episode_len_mean: 300.0, agent_steps_trained: 343728, env_steps_trained: 114576, entropy: 3.2866732506166425, learning_rate: 0.0010000000000000002\n",
      "iteration [29] => episode_reward_mean: 1348.9248339035996, episode_len_mean: 300.0, agent_steps_trained: 356004, env_steps_trained: 118668, entropy: 3.2746203745457163, learning_rate: 0.0010000000000000002\n",
      "iteration [30] => episode_reward_mean: 1368.981982680619, episode_len_mean: 300.0, agent_steps_trained: 368280, env_steps_trained: 122760, entropy: 3.2479646437628227, learning_rate: 0.0010000000000000002\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "from gymnasium.wrappers.time_limit import TimeLimit\n",
    "\n",
    "trainin_steps = 30\n",
    "\n",
    "algo = (\n",
    "    PPOConfig()\n",
    "    .training(gamma = 0.95, \n",
    "              lr = 0.001,\n",
    "              train_batch_size = 4092, \n",
    "              sgd_minibatch_size = 128, \n",
    "              num_sgd_iter = 30,\n",
    "              #entropy_coeff=0.005,\n",
    "              )\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"KeepTheDistance?dst=0&visible_nbrs=2&spawn_area=100\")\n",
    "    .build()\n",
    ")\n",
    "clear_output()\n",
    "\n",
    "env_show = RenderableKeepTheDistance(env_config)\n",
    "for i in range(trainin_steps):\n",
    "    result = algo.train()\n",
    "    simulate_episode(env_show, algo, 150, sleep_between_frames=0.03, print_info=False)\n",
    "    print(ppo_result_format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c772ee3ee5074ffe934e88684e6f5636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CanvasWithBorders(height=300, width=300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env_config_2 = EnvironmentConfiguration(n_agents=10, visible_nbrs=2, target_distance=0, max_steps=500, speed=1, spawn_area=500)\n",
    "simulate_episode(RenderableKeepTheDistance(env_config_2), algo, 300, sleep_between_frames=0.01, print_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An Algorithm checkpoint has been created inside directory: 'TrainingResult(checkpoint=Checkpoint(filesystem=local, path=/mnt/c/Users/nicol/Desktop/Universit/tesi/experiments/RL_experiments/algos/KeepTheDistance?dst=0&visible_nbrs=2&spawn_area=100), metrics={'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'grad_gnorm': 9.59812976611288, 'cur_kl_coeff': 0.6749999999999998, 'cur_lr': 0.0010000000000000002, 'total_loss': 3.6643962602343474, 'policy_loss': -0.00980367020883581, 'vf_loss': 3.665363485813141, 'vf_explained_var': 0.218065206565355, 'kl': 0.013091016343160103, 'entropy': 3.2479646437628227, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 128.0, 'num_grad_updates_lifetime': 84075.5, 'diff_num_grad_updates_vs_sampler_policy': 1424.5}}, 'num_env_steps_sampled': 122760, 'num_env_steps_trained': 122760, 'num_agent_steps_sampled': 368280, 'num_agent_steps_trained': 368280}, 'sampler_results': {'episode_reward_max': 1539.0278822334228, 'episode_reward_min': 1203.2076663275193, 'episode_reward_mean': 1368.981982680619, 'episode_len_mean': 300.0, 'episode_media': {}, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [1244.0032770452449, 1368.3276384787182, 1203.2076663275193, 1229.3656452125017, 1259.0891313530533, 1257.5284204766538, 1275.0169251235259, 1309.7901027524724, 1323.041871439887, 1293.7054266998134, 1213.3148826637148, 1300.169790309533, 1300.4307312016592, 1350.566968691381, 1269.1435239212994, 1415.0805128743687, 1334.5219219970927, 1313.2453783448686, 1411.1062822679394, 1309.2495680417826, 1328.878707228706, 1334.411003058787, 1385.8799115870465, 1347.5584135078063, 1320.6640984013263, 1356.6375957446144, 1328.678076290337, 1328.0711115009096, 1331.9390462589035, 1266.072042515907, 1272.288965085734, 1322.93416887266, 1315.5774156878754, 1272.4007634895745, 1403.437143965339, 1464.4936449606637, 1394.1147846084737, 1349.4046673940002, 1299.0169116879547, 1338.998286457107, 1400.8322099486732, 1432.1565063995304, 1350.4639201293621, 1444.7089828921128, 1279.2777288456864, 1433.7102049091363, 1415.004934673243, 1399.9569484280005, 1348.2232310344068, 1389.631227256376, 1453.7950282945274, 1455.6533314844085, 1328.7582501840761, 1435.9453439684924, 1439.4094130305518, 1404.6926307089198, 1452.2907031207267, 1427.5519316822263, 1397.3645829925729, 1360.524203733244, 1398.6718852125796, 1390.4390151879102, 1379.527961027734, 1408.3531172623248, 1482.2917465524326, 1397.451152589588, 1406.6224195551144, 1339.3251409687716, 1259.9226466151536, 1375.9081536240594, 1440.922309525106, 1416.129009194547, 1328.9255634597962, 1402.2066267953785, 1300.7601722097752, 1434.1973928883785, 1468.1697939553244, 1356.4588336559068, 1444.546703874799, 1369.3527668602264, 1405.371531352963, 1318.7830260726337, 1326.8726943207514, 1438.5662144743715, 1433.1235956004136, 1398.7782325975484, 1378.3366150661448, 1395.6252902940846, 1383.5768802224197, 1511.7044189600745, 1423.4735497810857, 1500.4995979420696, 1539.0278822334228, 1403.7304430529025, 1436.320385818375, 1476.346869774137, 1434.3517618293668, 1355.5282765805807, 1384.904225259928, 1361.7806285707072], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6707851104708787, 'mean_inference_ms': 1.219831121462141, 'mean_action_processing_ms': 0.4403560969854789, 'mean_env_wait_ms': 0.8519521919911223, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.007554292678833008, 'StateBufferConnector_ms': 0.005788087844848633, 'ViewRequirementAgentConnector_ms': 0.20926380157470703}, 'num_episodes': 14, 'episode_return_max': 1539.0278822334228, 'episode_return_min': 1203.2076663275193, 'episode_return_mean': 1368.981982680619}, 'env_runner_results': {'episode_reward_max': 1539.0278822334228, 'episode_reward_min': 1203.2076663275193, 'episode_reward_mean': 1368.981982680619, 'episode_len_mean': 300.0, 'episode_media': {}, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [1244.0032770452449, 1368.3276384787182, 1203.2076663275193, 1229.3656452125017, 1259.0891313530533, 1257.5284204766538, 1275.0169251235259, 1309.7901027524724, 1323.041871439887, 1293.7054266998134, 1213.3148826637148, 1300.169790309533, 1300.4307312016592, 1350.566968691381, 1269.1435239212994, 1415.0805128743687, 1334.5219219970927, 1313.2453783448686, 1411.1062822679394, 1309.2495680417826, 1328.878707228706, 1334.411003058787, 1385.8799115870465, 1347.5584135078063, 1320.6640984013263, 1356.6375957446144, 1328.678076290337, 1328.0711115009096, 1331.9390462589035, 1266.072042515907, 1272.288965085734, 1322.93416887266, 1315.5774156878754, 1272.4007634895745, 1403.437143965339, 1464.4936449606637, 1394.1147846084737, 1349.4046673940002, 1299.0169116879547, 1338.998286457107, 1400.8322099486732, 1432.1565063995304, 1350.4639201293621, 1444.7089828921128, 1279.2777288456864, 1433.7102049091363, 1415.004934673243, 1399.9569484280005, 1348.2232310344068, 1389.631227256376, 1453.7950282945274, 1455.6533314844085, 1328.7582501840761, 1435.9453439684924, 1439.4094130305518, 1404.6926307089198, 1452.2907031207267, 1427.5519316822263, 1397.3645829925729, 1360.524203733244, 1398.6718852125796, 1390.4390151879102, 1379.527961027734, 1408.3531172623248, 1482.2917465524326, 1397.451152589588, 1406.6224195551144, 1339.3251409687716, 1259.9226466151536, 1375.9081536240594, 1440.922309525106, 1416.129009194547, 1328.9255634597962, 1402.2066267953785, 1300.7601722097752, 1434.1973928883785, 1468.1697939553244, 1356.4588336559068, 1444.546703874799, 1369.3527668602264, 1405.371531352963, 1318.7830260726337, 1326.8726943207514, 1438.5662144743715, 1433.1235956004136, 1398.7782325975484, 1378.3366150661448, 1395.6252902940846, 1383.5768802224197, 1511.7044189600745, 1423.4735497810857, 1500.4995979420696, 1539.0278822334228, 1403.7304430529025, 1436.320385818375, 1476.346869774137, 1434.3517618293668, 1355.5282765805807, 1384.904225259928, 1361.7806285707072], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6707851104708787, 'mean_inference_ms': 1.219831121462141, 'mean_action_processing_ms': 0.4403560969854789, 'mean_env_wait_ms': 0.8519521919911223, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.007554292678833008, 'StateBufferConnector_ms': 0.005788087844848633, 'ViewRequirementAgentConnector_ms': 0.20926380157470703}, 'num_episodes': 14, 'episode_return_max': 1539.0278822334228, 'episode_return_min': 1203.2076663275193, 'episode_return_mean': 1368.981982680619}, 'episode_reward_max': 1539.0278822334228, 'episode_reward_min': 1203.2076663275193, 'episode_reward_mean': 1368.981982680619, 'episode_len_mean': 300.0, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [1244.0032770452449, 1368.3276384787182, 1203.2076663275193, 1229.3656452125017, 1259.0891313530533, 1257.5284204766538, 1275.0169251235259, 1309.7901027524724, 1323.041871439887, 1293.7054266998134, 1213.3148826637148, 1300.169790309533, 1300.4307312016592, 1350.566968691381, 1269.1435239212994, 1415.0805128743687, 1334.5219219970927, 1313.2453783448686, 1411.1062822679394, 1309.2495680417826, 1328.878707228706, 1334.411003058787, 1385.8799115870465, 1347.5584135078063, 1320.6640984013263, 1356.6375957446144, 1328.678076290337, 1328.0711115009096, 1331.9390462589035, 1266.072042515907, 1272.288965085734, 1322.93416887266, 1315.5774156878754, 1272.4007634895745, 1403.437143965339, 1464.4936449606637, 1394.1147846084737, 1349.4046673940002, 1299.0169116879547, 1338.998286457107, 1400.8322099486732, 1432.1565063995304, 1350.4639201293621, 1444.7089828921128, 1279.2777288456864, 1433.7102049091363, 1415.004934673243, 1399.9569484280005, 1348.2232310344068, 1389.631227256376, 1453.7950282945274, 1455.6533314844085, 1328.7582501840761, 1435.9453439684924, 1439.4094130305518, 1404.6926307089198, 1452.2907031207267, 1427.5519316822263, 1397.3645829925729, 1360.524203733244, 1398.6718852125796, 1390.4390151879102, 1379.527961027734, 1408.3531172623248, 1482.2917465524326, 1397.451152589588, 1406.6224195551144, 1339.3251409687716, 1259.9226466151536, 1375.9081536240594, 1440.922309525106, 1416.129009194547, 1328.9255634597962, 1402.2066267953785, 1300.7601722097752, 1434.1973928883785, 1468.1697939553244, 1356.4588336559068, 1444.546703874799, 1369.3527668602264, 1405.371531352963, 1318.7830260726337, 1326.8726943207514, 1438.5662144743715, 1433.1235956004136, 1398.7782325975484, 1378.3366150661448, 1395.6252902940846, 1383.5768802224197, 1511.7044189600745, 1423.4735497810857, 1500.4995979420696, 1539.0278822334228, 1403.7304430529025, 1436.320385818375, 1476.346869774137, 1434.3517618293668, 1355.5282765805807, 1384.904225259928, 1361.7806285707072], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6707851104708787, 'mean_inference_ms': 1.219831121462141, 'mean_action_processing_ms': 0.4403560969854789, 'mean_env_wait_ms': 0.8519521919911223, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.007554292678833008, 'StateBufferConnector_ms': 0.005788087844848633, 'ViewRequirementAgentConnector_ms': 0.20926380157470703}, 'num_episodes': 14, 'episode_return_max': 1539.0278822334228, 'episode_return_min': 1203.2076663275193, 'episode_return_mean': 1368.981982680619, 'num_healthy_workers': 1, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 368280, 'num_agent_steps_trained': 368280, 'num_env_steps_sampled': 122760, 'num_env_steps_trained': 122760, 'num_env_steps_sampled_this_iter': 4092, 'num_env_steps_trained_this_iter': 4092, 'num_env_steps_sampled_throughput_per_sec': 160.68598803503886, 'num_env_steps_trained_throughput_per_sec': 160.68598803503886, 'timesteps_total': 122760, 'num_env_steps_sampled_lifetime': 122760, 'num_agent_steps_sampled_lifetime': 368280, 'num_steps_trained_this_iter': 4092, 'agent_timesteps_total': 368280, 'timers': {'training_iteration_time_ms': 30313.034, 'restore_workers_time_ms': 0.02, 'training_step_time_ms': 30312.985, 'sample_time_ms': 11341.393, 'load_time_ms': 0.794, 'load_throughput': 5153307.902, 'learn_time_ms': 18964.15, 'learn_throughput': 215.776, 'synch_weights_time_ms': 5.215}, 'counters': {'num_env_steps_sampled': 122760, 'num_env_steps_trained': 122760, 'num_agent_steps_sampled': 368280, 'num_agent_steps_trained': 368280}, 'done': False, 'episodes_total': 409, 'training_iteration': 30, 'trial_id': 'default', 'date': '2024-05-28_15-56-27', 'timestamp': 1716904587, 'time_this_iter_s': 25.475690364837646, 'time_total_s': 1000.2000412940979, 'pid': 666, 'hostname': 'LAPTOP-9AD2MD1C', 'node_ip': '172.23.87.11', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'enable_rl_module_and_learner': False, 'enable_env_runner_and_connector_v2': False, 'env': 'KeepTheDistance?dst=0&visible_nbrs=2&spawn_area=100', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'env_task_fn': None, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_env_runner': 1, 'validate_env_runners_after_construction': True, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'enable_connectors': True, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.95, 'lr': 0.001, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 4092, 'train_batch_size_per_learner': None, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x7f0b669942c0>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'always_attach_evaluation_results': True, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_run_training_always_in_thread': False, '_evaluation_parallel_to_training_wo_thread': False, 'ignore_env_runner_failures': False, 'recreate_failed_env_runners': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30, 'env_runner_restore_timeout_s': 1800, '_model_config_dict': {}, '_rl_module_spec': None, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'disable_env_checking': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'mini_batch_size_per_learner': None, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 1}, 'time_since_restore': 1000.2000412940979, 'iterations_since_restore': 30, 'perf': {'cpu_util_percent': 36.65581395348838, 'ram_util_percent': 79.30930232558141}})'.\n"
     ]
    }
   ],
   "source": [
    "save_algo(algo, \"KeepTheDistance?dst=0&visible_nbrs=2&spawn_area=100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KeepTheDistance?dst=0&visible_nbrs=3&spawn_area=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "\n",
    "env_config = EnvironmentConfiguration(n_agents=4, visible_nbrs=3, target_distance=0, max_steps=300, speed=1, spawn_area=100)\n",
    "register_env(\"KeepTheDistance?dst=0&visible_nbrs=3&spawn_area=100\", lambda _: KeepTheDistance(env_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 14:02:44,485\tWARNING deprecation.py:50 -- DeprecationWarning: `WorkerSet(num_workers=... OR local_worker=...)` has been deprecated. Use `EnvRunnerGroup(num_env_runners=... AND local_env_runner=...)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m algo \u001b[38;5;241m=\u001b[39m \u001b[43mload_algo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mKeepTheDistance?dst=0&visible_nbrs=3&spawn_area=100\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/nicol/Desktop/Universit/tesi/experiments/RL_experiments/utils/algo_utils.py:17\u001b[0m, in \u001b[0;36mload_algo\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(subfolder_path):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe specified subfolder \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubfolder_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m does not exist.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mAlgorithm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubfolder_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:363\u001b[0m, in \u001b[0;36mAlgorithm.from_checkpoint\u001b[0;34m(checkpoint, policy_ids, policy_mapping_fn, policies_to_train)\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    346\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to restore a multi-agent algorithm from a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    347\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`msgpack` formatted checkpoint, which do NOT store the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    353\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou would like to train all policies anyways.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    354\u001b[0m             )\n\u001b[1;32m    356\u001b[0m state \u001b[38;5;241m=\u001b[39m Algorithm\u001b[38;5;241m.\u001b[39m_checkpoint_info_to_algorithm_state(\n\u001b[1;32m    357\u001b[0m     checkpoint_info\u001b[38;5;241m=\u001b[39mcheckpoint_info,\n\u001b[1;32m    358\u001b[0m     policy_ids\u001b[38;5;241m=\u001b[39mpolicy_ids,\n\u001b[1;32m    359\u001b[0m     policy_mapping_fn\u001b[38;5;241m=\u001b[39mpolicy_mapping_fn,\n\u001b[1;32m    360\u001b[0m     policies_to_train\u001b[38;5;241m=\u001b[39mpolicies_to_train,\n\u001b[1;32m    361\u001b[0m )\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mAlgorithm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:391\u001b[0m, in \u001b[0;36mAlgorithm.from_state\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config:\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo `config` found in given Algorithm state!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 391\u001b[0m new_algo \u001b[38;5;241m=\u001b[39m \u001b[43malgorithm_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;66;03m# Set the new algo's state.\u001b[39;00m\n\u001b[1;32m    393\u001b[0m new_algo\u001b[38;5;241m.\u001b[39m__setstate__(state)\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:554\u001b[0m, in \u001b[0;36mAlgorithm.__init__\u001b[0;34m(self, config, env, logger_creator, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;66;03m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;66;03m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;66;03m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;66;03m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;66;03m# TODO: Don't dump sampler results into top-level.\u001b[39;00m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    551\u001b[0m     },\n\u001b[1;32m    552\u001b[0m }\n\u001b[0;32m--> 554\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/tune/trainable/trainable.py:158\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, storage)\u001b[0m\n\u001b[1;32m    154\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStorageContext on the TRAINABLE:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mstorage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open_logfiles(stdout_file, stderr_file)\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m setup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m setup_time \u001b[38;5;241m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:640\u001b[0m, in \u001b[0;36mAlgorithm.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moff_policy_estimation_methods \u001b[38;5;241m=\u001b[39m ope_dict\n\u001b[1;32m    639\u001b[0m \u001b[38;5;66;03m# Create a set of env runner actors via a EnvRunnerGroup.\u001b[39;00m\n\u001b[0;32m--> 640\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers \u001b[38;5;241m=\u001b[39m \u001b[43mEnvRunnerGroup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_policy_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_policy_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_env_runners\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;66;03m# Ensure remote workers are initially in sync with the local worker.\u001b[39;00m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers\u001b[38;5;241m.\u001b[39msync_weights(inference_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py:169\u001b[0m, in \u001b[0;36mEnvRunnerGroup.__init__\u001b[0;34m(self, env_creator, validate_env, default_policy_class, config, num_env_runners, local_env_runner, logdir, _setup, num_workers, local_worker)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _setup:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_env_runners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_env_runners\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_env_runner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_env_runner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;66;03m# EnvRunnerGroup creation possibly fails, if some (remote) workers cannot\u001b[39;00m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;66;03m# be initialized properly (due to some errors in the EnvRunners's\u001b[39;00m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;66;03m# constructor).\u001b[39;00m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m RayActorError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;66;03m# In case of an actor (remote worker) init failure, the remote worker\u001b[39;00m\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;66;03m# may still exist and will be accessible, however, e.g. calling\u001b[39;00m\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;66;03m# its `sample.remote()` would result in strange \"property not found\"\u001b[39;00m\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;66;03m# errors.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py:239\u001b[0m, in \u001b[0;36mEnvRunnerGroup._setup\u001b[0;34m(self, validate_env, config, num_env_runners, local_env_runner)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ds_shards \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# Create a number of @ray.remote workers.\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_workers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_env_runners\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_env_runners_after_construction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# If num_workers > 0 and we don't have an env on the local worker,\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m# get the observation- and action spaces for each policy from\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# the first remote worker (which does have an env).\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    248\u001b[0m     local_env_runner\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker_manager\u001b[38;5;241m.\u001b[39mnum_actors() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39mobservation_space \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39maction_space)\n\u001b[1;32m    253\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py:748\u001b[0m, in \u001b[0;36mEnvRunnerGroup.add_workers\u001b[0;34m(self, num_workers, validate)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;66;03m# Validate here, whether all remote workers have been constructed properly\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;66;03m# and are \"up and running\". Establish initial states.\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate:\n\u001b[0;32m--> 748\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_worker_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeach_actor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_healthy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    751\u001b[0m         \u001b[38;5;66;03m# Simiply raise the error, which will get handled by the try-except\u001b[39;00m\n\u001b[1;32m    752\u001b[0m         \u001b[38;5;66;03m# clause around the _setup().\u001b[39;00m\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39mok:\n\u001b[1;32m    754\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m result\u001b[38;5;241m.\u001b[39mget()\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/rllib/utils/actor_manager.py:622\u001b[0m, in \u001b[0;36mFaultTolerantActorManager.foreach_actor\u001b[0;34m(self, func, healthy_only, remote_actor_ids, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[1;32m    616\u001b[0m remote_calls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_actors(\n\u001b[1;32m    617\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m    618\u001b[0m     remote_actor_ids\u001b[38;5;241m=\u001b[39mremote_actor_ids,\n\u001b[1;32m    619\u001b[0m )\n\u001b[1;32m    621\u001b[0m \u001b[38;5;66;03m# Collect remote request results (if available given timeout and/or errors).\u001b[39;00m\n\u001b[0;32m--> 622\u001b[0m _, remote_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_actor_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremote_actor_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremote_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmark_healthy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmark_healthy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m remote_results\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/rllib/utils/actor_manager.py:476\u001b[0m, in \u001b[0;36mFaultTolerantActorManager._fetch_result\u001b[0;34m(self, remote_actor_ids, remote_calls, tags, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m remote_calls:\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [], RemoteCallResults()\n\u001b[0;32m--> 476\u001b[0m ready, _ \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Make sure remote results are fetched locally in parallel.\u001b[39;49;00m\n\u001b[1;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfetch_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# Remote data should already be fetched to local object store at this point.\u001b[39;00m\n\u001b[1;32m    485\u001b[0m remote_results \u001b[38;5;241m=\u001b[39m RemoteCallResults()\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py:21\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauto_init_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     20\u001b[0m     auto_init_ray()\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/_private/worker.py:2854\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(ray_waitables, num_returns, timeout, fetch_local)\u001b[0m\n\u001b[1;32m   2852\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeout \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m6\u001b[39m\n\u001b[1;32m   2853\u001b[0m timeout_milliseconds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m-> 2854\u001b[0m ready_ids, remaining_ids \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mray_waitables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_milliseconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_task_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfetch_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2861\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ready_ids, remaining_ids\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:3812\u001b[0m, in \u001b[0;36mray._raylet.CoreWorker.wait\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:571\u001b[0m, in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "algo = load_algo(\"KeepTheDistance?dst=0&visible_nbrs=3&spawn_area=100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "383048de8d554a06b64a52775edc6b62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CanvasWithBorders(height=300, width=300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration [1] => episode_reward_mean: -9.511731059783386, episode_len_mean: 300.0, agent_steps_trained: 16368, env_steps_trained: 4092, entropy: 4.227877699296306, learning_rate: 0.0010000000000000005\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m env_show \u001b[38;5;241m=\u001b[39m RenderableKeepTheDistance(env_config)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(trainin_steps):\n\u001b[0;32m---> 26\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43malgo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     simulate_episode(env_show, algo, \u001b[38;5;241m150\u001b[39m, sleep_between_frames\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.03\u001b[39m, print_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(ppo_result_format(result))\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/trainable/trainable.py:328\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    326\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    330\u001b[0m     skipped \u001b[38;5;241m=\u001b[39m skip_exceptions(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:873\u001b[0m, in \u001b[0;36mAlgorithm.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    863\u001b[0m     (\n\u001b[1;32m    864\u001b[0m         train_results,\n\u001b[1;32m    865\u001b[0m         eval_results,\n\u001b[1;32m    866\u001b[0m         train_iter_ctx,\n\u001b[1;32m    867\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_one_training_iteration_and_evaluation_in_parallel()\n\u001b[1;32m    869\u001b[0m \u001b[38;5;66;03m# - No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;66;03m# - We have to evaluate in this training iteration, but no parallelism ->\u001b[39;00m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;66;03m#   evaluate after the training iteration is entirely done.\u001b[39;00m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 873\u001b[0m     train_results, train_iter_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_one_training_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# Sequential: Train (already done above), then evaluate.\u001b[39;00m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluate_this_iter \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mevaluation_parallel_to_training:\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:3156\u001b[0m, in \u001b[0;36mAlgorithm._run_one_training_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3154\u001b[0m             \u001b[38;5;66;03m# Try to train one step.\u001b[39;00m\n\u001b[1;32m   3155\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timers[TRAINING_STEP_TIMER]:\n\u001b[0;32m-> 3156\u001b[0m                 results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results, train_iter_ctx\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/ppo/ppo.py:428\u001b[0m, in \u001b[0;36mPPO.training_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_training_step_new_api_stack()\n\u001b[1;32m    425\u001b[0m \u001b[38;5;66;03m# Old and hybrid API stacks (Policy, RolloutWorker, Connector, maybe RLModule,\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;66;03m# maybe Learner).\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 428\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_training_step_old_and_hybrid_api_stacks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/ppo/ppo.py:562\u001b[0m, in \u001b[0;36mPPO._training_step_old_and_hybrid_api_stacks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    557\u001b[0m     train_batch \u001b[38;5;241m=\u001b[39m synchronous_parallel_sample(\n\u001b[1;32m    558\u001b[0m         worker_set\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers,\n\u001b[1;32m    559\u001b[0m         max_agent_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtotal_train_batch_size,\n\u001b[1;32m    560\u001b[0m     )\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 562\u001b[0m     train_batch \u001b[38;5;241m=\u001b[39m \u001b[43msynchronous_parallel_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworker_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_env_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtotal_train_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m train_batch \u001b[38;5;241m=\u001b[39m train_batch\u001b[38;5;241m.\u001b[39mas_multi_agent()\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_counters[NUM_AGENT_STEPS_SAMPLED] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_batch\u001b[38;5;241m.\u001b[39magent_steps()\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/execution/rollout_ops.py:97\u001b[0m, in \u001b[0;36msynchronous_parallel_sample\u001b[0;34m(worker_set, max_agent_steps, max_env_steps, concat, sample_timeout_s, _uses_new_env_runners, _return_metrics)\u001b[0m\n\u001b[1;32m     94\u001b[0m         stats_dicts \u001b[38;5;241m=\u001b[39m [worker_set\u001b[38;5;241m.\u001b[39mlocal_worker()\u001b[38;5;241m.\u001b[39mget_metrics()]\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Loop over remote workers' `sample()` method in parallel.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     sampled_data \u001b[38;5;241m=\u001b[39m \u001b[43mworker_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeach_worker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_return_metrics\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_timeout_s\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# Nothing was returned (maybe all workers are stalling) or no healthy\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# remote workers left: Break.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# There is no point staying in this loop, since we will not be able to\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;66;03m# get any new samples if we don't have any healthy remote workers left.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sampled_data \u001b[38;5;129;01mor\u001b[39;00m worker_set\u001b[38;5;241m.\u001b[39mnum_healthy_remote_workers() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py:840\u001b[0m, in \u001b[0;36mEnvRunnerGroup.foreach_worker\u001b[0;34m(self, func, local_worker, healthy_only, remote_worker_ids, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker_manager\u001b[38;5;241m.\u001b[39mactor_ids():\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m local_result\n\u001b[0;32m--> 840\u001b[0m remote_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_worker_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeach_actor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhealthy_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhealthy_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_actor_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremote_worker_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmark_healthy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmark_healthy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    849\u001b[0m _handle_remote_call_result_errors(\n\u001b[1;32m    850\u001b[0m     remote_results, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ignore_env_runner_failures\n\u001b[1;32m    851\u001b[0m )\n\u001b[1;32m    853\u001b[0m \u001b[38;5;66;03m# With application errors handled, return good results.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/utils/actor_manager.py:622\u001b[0m, in \u001b[0;36mFaultTolerantActorManager.foreach_actor\u001b[0;34m(self, func, healthy_only, remote_actor_ids, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[1;32m    616\u001b[0m remote_calls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_actors(\n\u001b[1;32m    617\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m    618\u001b[0m     remote_actor_ids\u001b[38;5;241m=\u001b[39mremote_actor_ids,\n\u001b[1;32m    619\u001b[0m )\n\u001b[1;32m    621\u001b[0m \u001b[38;5;66;03m# Collect remote request results (if available given timeout and/or errors).\u001b[39;00m\n\u001b[0;32m--> 622\u001b[0m _, remote_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_actor_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremote_actor_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremote_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmark_healthy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmark_healthy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m remote_results\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/utils/actor_manager.py:476\u001b[0m, in \u001b[0;36mFaultTolerantActorManager._fetch_result\u001b[0;34m(self, remote_actor_ids, remote_calls, tags, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m remote_calls:\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [], RemoteCallResults()\n\u001b[0;32m--> 476\u001b[0m ready, _ \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Make sure remote results are fetched locally in parallel.\u001b[39;49;00m\n\u001b[1;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfetch_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# Remote data should already be fetched to local object store at this point.\u001b[39;00m\n\u001b[1;32m    485\u001b[0m remote_results \u001b[38;5;241m=\u001b[39m RemoteCallResults()\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py:21\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauto_init_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     20\u001b[0m     auto_init_ray()\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py:2854\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(ray_waitables, num_returns, timeout, fetch_local)\u001b[0m\n\u001b[1;32m   2852\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeout \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m6\u001b[39m\n\u001b[1;32m   2853\u001b[0m timeout_milliseconds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m-> 2854\u001b[0m ready_ids, remaining_ids \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mray_waitables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_milliseconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_task_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfetch_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2861\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ready_ids, remaining_ids\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:3812\u001b[0m, in \u001b[0;36mray._raylet.CoreWorker.wait\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:571\u001b[0m, in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "from gymnasium.wrappers.time_limit import TimeLimit\n",
    "\n",
    "trainin_steps = 30\n",
    "\n",
    "algo = (\n",
    "    PPOConfig()\n",
    "    .training(gamma = 0.95, \n",
    "              lr = 0.001,\n",
    "              train_batch_size = 4092, \n",
    "              sgd_minibatch_size = 128, \n",
    "              num_sgd_iter = 30,\n",
    "              #entropy_coeff=0.005,\n",
    "              )\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"KeepTheDistance?dst=0&visible_nbrs=3&spawn_area=100\")\n",
    "    .build()\n",
    ")\n",
    "clear_output()\n",
    "\n",
    "env_show = RenderableKeepTheDistance(env_config)\n",
    "for i in range(trainin_steps):\n",
    "    result = algo.train()\n",
    "    simulate_episode(env_show, algo, 150, sleep_between_frames=0.03, print_info=False)\n",
    "    print(ppo_result_format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d0544de63b345eb8d7ceb48075240e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CanvasWithBorders(height=300, width=300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env_config_2 = EnvironmentConfiguration(n_agents=10, visible_nbrs=3, target_distance=0, max_steps=500, speed=1, spawn_area=400)\n",
    "simulate_episode(RenderableKeepTheDistance(env_config_2), algo, 300, sleep_between_frames=0.01, print_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An Algorithm checkpoint has been created inside directory: 'TrainingResult(checkpoint=Checkpoint(filesystem=local, path=/mnt/c/Users/nicol/Desktop/Universit/tesi/experiments/RL_experiments/algos/KeepTheDistance?dst=0&visible_nbrs=3&spawn_area=100), metrics={'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'grad_gnorm': 8.819473860895853, 'cur_kl_coeff': 0.6750000000000002, 'cur_lr': 0.0010000000000000005, 'total_loss': 4.865067737543677, 'policy_loss': -0.008388914499832733, 'vf_loss': 4.862613392408126, 'vf_explained_var': 0.12559887126987687, 'kl': 0.01606408571965559, 'entropy': 3.1699658083477673, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 128.0, 'num_grad_updates_lifetime': 112395.5, 'diff_num_grad_updates_vs_sampler_policy': 1904.5}}, 'num_env_steps_sampled': 122760, 'num_env_steps_trained': 122760, 'num_agent_steps_sampled': 491040, 'num_agent_steps_trained': 491040}, 'sampler_results': {'episode_reward_max': 2909.6687525246543, 'episode_reward_min': 2269.9620431607536, 'episode_reward_mean': 2622.307084105791, 'episode_len_mean': 300.0, 'episode_media': {}, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [2460.4148909542173, 2472.352994864214, 2269.9620431607536, 2583.754206533484, 2532.534688208508, 2564.8033427488035, 2372.29204281701, 2559.6130632218315, 2440.6772592351326, 2519.528499638881, 2543.251320094094, 2457.6088088637375, 2452.858066223916, 2544.8880440345456, 2496.677925724944, 2535.855163791634, 2465.8233302087347, 2460.423545518293, 2576.244476192975, 2489.1172564177236, 2473.1919960046785, 2702.201531335308, 2494.754280860624, 2680.8367183060896, 2586.8645384418037, 2565.0683309643587, 2507.7700560616067, 2715.4455453882574, 2624.231689605109, 2554.1947926743687, 2634.96780451485, 2615.9041642613156, 2477.461149607917, 2627.632626714633, 2559.553862004532, 2459.1381782992535, 2565.0074814013824, 2409.7113144129994, 2720.4077748603963, 2375.8866803625956, 2525.9700335058774, 2516.350787036143, 2447.1598275840242, 2830.0277385521367, 2591.177156610882, 2565.7078886082845, 2521.5126025751665, 2715.2164991401264, 2619.34630578734, 2564.166935751629, 2397.96590265702, 2764.149770681248, 2730.442516016416, 2826.890355665586, 2516.4050628823557, 2521.0763980444376, 2568.296683613576, 2761.1246569433783, 2398.506681504311, 2752.90653110132, 2658.7133935727106, 2680.8035452283307, 2665.9766830145118, 2688.0296282372788, 2664.9487644608967, 2687.3144351252904, 2652.3489832992145, 2670.8800650390726, 2728.408588924212, 2759.74333418668, 2624.3360876731035, 2546.3356418312096, 2838.95524837164, 2692.748340620383, 2689.945214004612, 2666.0053214343225, 2842.2278012356396, 2687.1920892868275, 2732.824734008082, 2850.3782979100024, 2627.42313803283, 2807.733239363243, 2777.6350169438438, 2487.906560098489, 2871.9838199932433, 2746.7610336944963, 2819.8288994464333, 2909.6687525246543, 2709.0120512796957, 2682.8893214331997, 2681.668190740857, 2702.0682513715046, 2706.85669862791, 2706.399071422442, 2769.5661487556918, 2789.0347576967242, 2817.190666461392, 2699.345469761446, 2888.009840453078, 2718.2994662171545], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.7194199783081834, 'mean_inference_ms': 1.121632855234117, 'mean_action_processing_ms': 0.42801676183401016, 'mean_env_wait_ms': 0.9858080342857882, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.007732391357421875, 'StateBufferConnector_ms': 0.006838083267211914, 'ViewRequirementAgentConnector_ms': 0.24573421478271484}, 'num_episodes': 14, 'episode_return_max': 2909.6687525246543, 'episode_return_min': 2269.9620431607536, 'episode_return_mean': 2622.307084105791}, 'env_runner_results': {'episode_reward_max': 2909.6687525246543, 'episode_reward_min': 2269.9620431607536, 'episode_reward_mean': 2622.307084105791, 'episode_len_mean': 300.0, 'episode_media': {}, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [2460.4148909542173, 2472.352994864214, 2269.9620431607536, 2583.754206533484, 2532.534688208508, 2564.8033427488035, 2372.29204281701, 2559.6130632218315, 2440.6772592351326, 2519.528499638881, 2543.251320094094, 2457.6088088637375, 2452.858066223916, 2544.8880440345456, 2496.677925724944, 2535.855163791634, 2465.8233302087347, 2460.423545518293, 2576.244476192975, 2489.1172564177236, 2473.1919960046785, 2702.201531335308, 2494.754280860624, 2680.8367183060896, 2586.8645384418037, 2565.0683309643587, 2507.7700560616067, 2715.4455453882574, 2624.231689605109, 2554.1947926743687, 2634.96780451485, 2615.9041642613156, 2477.461149607917, 2627.632626714633, 2559.553862004532, 2459.1381782992535, 2565.0074814013824, 2409.7113144129994, 2720.4077748603963, 2375.8866803625956, 2525.9700335058774, 2516.350787036143, 2447.1598275840242, 2830.0277385521367, 2591.177156610882, 2565.7078886082845, 2521.5126025751665, 2715.2164991401264, 2619.34630578734, 2564.166935751629, 2397.96590265702, 2764.149770681248, 2730.442516016416, 2826.890355665586, 2516.4050628823557, 2521.0763980444376, 2568.296683613576, 2761.1246569433783, 2398.506681504311, 2752.90653110132, 2658.7133935727106, 2680.8035452283307, 2665.9766830145118, 2688.0296282372788, 2664.9487644608967, 2687.3144351252904, 2652.3489832992145, 2670.8800650390726, 2728.408588924212, 2759.74333418668, 2624.3360876731035, 2546.3356418312096, 2838.95524837164, 2692.748340620383, 2689.945214004612, 2666.0053214343225, 2842.2278012356396, 2687.1920892868275, 2732.824734008082, 2850.3782979100024, 2627.42313803283, 2807.733239363243, 2777.6350169438438, 2487.906560098489, 2871.9838199932433, 2746.7610336944963, 2819.8288994464333, 2909.6687525246543, 2709.0120512796957, 2682.8893214331997, 2681.668190740857, 2702.0682513715046, 2706.85669862791, 2706.399071422442, 2769.5661487556918, 2789.0347576967242, 2817.190666461392, 2699.345469761446, 2888.009840453078, 2718.2994662171545], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.7194199783081834, 'mean_inference_ms': 1.121632855234117, 'mean_action_processing_ms': 0.42801676183401016, 'mean_env_wait_ms': 0.9858080342857882, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.007732391357421875, 'StateBufferConnector_ms': 0.006838083267211914, 'ViewRequirementAgentConnector_ms': 0.24573421478271484}, 'num_episodes': 14, 'episode_return_max': 2909.6687525246543, 'episode_return_min': 2269.9620431607536, 'episode_return_mean': 2622.307084105791}, 'episode_reward_max': 2909.6687525246543, 'episode_reward_min': 2269.9620431607536, 'episode_reward_mean': 2622.307084105791, 'episode_len_mean': 300.0, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [2460.4148909542173, 2472.352994864214, 2269.9620431607536, 2583.754206533484, 2532.534688208508, 2564.8033427488035, 2372.29204281701, 2559.6130632218315, 2440.6772592351326, 2519.528499638881, 2543.251320094094, 2457.6088088637375, 2452.858066223916, 2544.8880440345456, 2496.677925724944, 2535.855163791634, 2465.8233302087347, 2460.423545518293, 2576.244476192975, 2489.1172564177236, 2473.1919960046785, 2702.201531335308, 2494.754280860624, 2680.8367183060896, 2586.8645384418037, 2565.0683309643587, 2507.7700560616067, 2715.4455453882574, 2624.231689605109, 2554.1947926743687, 2634.96780451485, 2615.9041642613156, 2477.461149607917, 2627.632626714633, 2559.553862004532, 2459.1381782992535, 2565.0074814013824, 2409.7113144129994, 2720.4077748603963, 2375.8866803625956, 2525.9700335058774, 2516.350787036143, 2447.1598275840242, 2830.0277385521367, 2591.177156610882, 2565.7078886082845, 2521.5126025751665, 2715.2164991401264, 2619.34630578734, 2564.166935751629, 2397.96590265702, 2764.149770681248, 2730.442516016416, 2826.890355665586, 2516.4050628823557, 2521.0763980444376, 2568.296683613576, 2761.1246569433783, 2398.506681504311, 2752.90653110132, 2658.7133935727106, 2680.8035452283307, 2665.9766830145118, 2688.0296282372788, 2664.9487644608967, 2687.3144351252904, 2652.3489832992145, 2670.8800650390726, 2728.408588924212, 2759.74333418668, 2624.3360876731035, 2546.3356418312096, 2838.95524837164, 2692.748340620383, 2689.945214004612, 2666.0053214343225, 2842.2278012356396, 2687.1920892868275, 2732.824734008082, 2850.3782979100024, 2627.42313803283, 2807.733239363243, 2777.6350169438438, 2487.906560098489, 2871.9838199932433, 2746.7610336944963, 2819.8288994464333, 2909.6687525246543, 2709.0120512796957, 2682.8893214331997, 2681.668190740857, 2702.0682513715046, 2706.85669862791, 2706.399071422442, 2769.5661487556918, 2789.0347576967242, 2817.190666461392, 2699.345469761446, 2888.009840453078, 2718.2994662171545], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.7194199783081834, 'mean_inference_ms': 1.121632855234117, 'mean_action_processing_ms': 0.42801676183401016, 'mean_env_wait_ms': 0.9858080342857882, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.007732391357421875, 'StateBufferConnector_ms': 0.006838083267211914, 'ViewRequirementAgentConnector_ms': 0.24573421478271484}, 'num_episodes': 14, 'episode_return_max': 2909.6687525246543, 'episode_return_min': 2269.9620431607536, 'episode_return_mean': 2622.307084105791, 'num_healthy_workers': 1, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 491040, 'num_agent_steps_trained': 491040, 'num_env_steps_sampled': 122760, 'num_env_steps_trained': 122760, 'num_env_steps_sampled_this_iter': 4092, 'num_env_steps_trained_this_iter': 4092, 'num_env_steps_sampled_throughput_per_sec': 124.63715566855504, 'num_env_steps_trained_throughput_per_sec': 124.63715566855504, 'timesteps_total': 122760, 'num_env_steps_sampled_lifetime': 122760, 'num_agent_steps_sampled_lifetime': 491040, 'num_steps_trained_this_iter': 4092, 'agent_timesteps_total': 491040, 'timers': {'training_iteration_time_ms': 36392.949, 'restore_workers_time_ms': 0.013, 'training_step_time_ms': 36392.91, 'sample_time_ms': 12647.467, 'load_time_ms': 0.806, 'load_throughput': 5074985.058, 'learn_time_ms': 23737.928, 'learn_throughput': 172.382, 'synch_weights_time_ms': 4.894}, 'counters': {'num_env_steps_sampled': 122760, 'num_env_steps_trained': 122760, 'num_agent_steps_sampled': 491040, 'num_agent_steps_trained': 491040}, 'done': False, 'episodes_total': 409, 'training_iteration': 30, 'trial_id': 'default', 'date': '2024-05-28_16-23-46', 'timestamp': 1716906226, 'time_this_iter_s': 32.83763933181763, 'time_total_s': 1286.2511758804321, 'pid': 666, 'hostname': 'LAPTOP-9AD2MD1C', 'node_ip': '172.23.87.11', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'enable_rl_module_and_learner': False, 'enable_env_runner_and_connector_v2': False, 'env': 'KeepTheDistance?dst=0&visible_nbrs=3&spawn_area=100', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'env_task_fn': None, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_env_runner': 1, 'validate_env_runners_after_construction': True, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'enable_connectors': True, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.95, 'lr': 0.001, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 4092, 'train_batch_size_per_learner': None, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x7f0b669942c0>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'always_attach_evaluation_results': True, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_run_training_always_in_thread': False, '_evaluation_parallel_to_training_wo_thread': False, 'ignore_env_runner_failures': False, 'recreate_failed_env_runners': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30, 'env_runner_restore_timeout_s': 1800, '_model_config_dict': {}, '_rl_module_spec': None, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'disable_env_checking': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'mini_batch_size_per_learner': None, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 1}, 'time_since_restore': 1286.2511758804321, 'iterations_since_restore': 30, 'perf': {'cpu_util_percent': 37.8962962962963, 'ram_util_percent': 69.2777777777778}})'.\n"
     ]
    }
   ],
   "source": [
    "save_algo(algo, \"KeepTheDistance?dst=0&visible_nbrs=3&spawn_area=100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KeepTheDistance?dst=5&visible_nbrs=3&spawn_area=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "base_algo = load_algo(\"KeepTheDistance?dst=0&visible_nbrs=3&spawn_area=100\")\n",
    "env_config = EnvironmentConfiguration(n_agents=4, visible_nbrs=3, target_distance=5, max_steps=300, speed=1, spawn_area=100)\n",
    "register_env(\"KeepTheDistance?dst=5&visible_nbrs=3&spawn_area=100\", lambda _: KeepTheDistance(env_config))\n",
    "\n",
    "algo = (\n",
    "    PPOConfig()\n",
    "    .training(gamma = 0.95, \n",
    "              lr = 0.001,\n",
    "              train_batch_size = 4092, \n",
    "              sgd_minibatch_size = 128, \n",
    "              num_sgd_iter = 30,\n",
    "              #entropy_coeff=0.005,\n",
    "              )\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"KeepTheDistance?dst=5&visible_nbrs=3&spawn_area=100\")\n",
    "    .build()\n",
    ")\n",
    "clear_output()\n",
    "algo.set_weights(base_algo.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 14:03:14,616\tWARNING deprecation.py:50 -- DeprecationWarning: `WorkerSet(num_workers=... OR local_worker=...)` has been deprecated. Use `EnvRunnerGroup(num_env_runners=... AND local_env_runner=...)` instead. This will raise an error in the future!\n",
      "2024-06-24 14:03:27,705\tINFO trainable.py:161 -- Trainable.setup took 13.089 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2024-06-24 14:03:27,709\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "algo = load_algo(\"KeepTheDistance?dst=5&visible_nbrs=3&spawn_area=100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 14:03:06,997\tWARNING deprecation.py:50 -- DeprecationWarning: `WorkerSet(num_workers=... OR local_worker=...)` has been deprecated. Use `EnvRunnerGroup(num_env_runners=... AND local_env_runner=...)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 20\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtime_limit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TimeLimit\n\u001b[1;32m      6\u001b[0m trainin_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m\n\u001b[1;32m      8\u001b[0m algo \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      9\u001b[0m     \u001b[43mPPOConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m              \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m              \u001b[49m\u001b[43mtrain_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4092\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m              \u001b[49m\u001b[43msgd_minibatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m              \u001b[49m\u001b[43mnum_sgd_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m              \u001b[49m\u001b[38;5;66;43;03m#entropy_coeff=0.005,\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m              \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_runners\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_env_runners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresources\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvironment\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mKeepTheDistance?dst=5&visible_nbrs=3&spawn_area=100\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m clear_output()\n\u001b[1;32m     24\u001b[0m env_show \u001b[38;5;241m=\u001b[39m RenderableKeepTheDistance(env_config)\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm_config.py:859\u001b[0m, in \u001b[0;36mAlgorithmConfig.build\u001b[0;34m(self, env, logger_creator, use_copy)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgo_class, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    857\u001b[0m     algo_class \u001b[38;5;241m=\u001b[39m get_trainable_cls(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgo_class)\n\u001b[0;32m--> 859\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgo_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_copy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:554\u001b[0m, in \u001b[0;36mAlgorithm.__init__\u001b[0;34m(self, config, env, logger_creator, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;66;03m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;66;03m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;66;03m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;66;03m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;66;03m# TODO: Don't dump sampler results into top-level.\u001b[39;00m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    551\u001b[0m     },\n\u001b[1;32m    552\u001b[0m }\n\u001b[0;32m--> 554\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/tune/trainable/trainable.py:158\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, storage)\u001b[0m\n\u001b[1;32m    154\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStorageContext on the TRAINABLE:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mstorage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open_logfiles(stdout_file, stderr_file)\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m setup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m setup_time \u001b[38;5;241m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:640\u001b[0m, in \u001b[0;36mAlgorithm.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moff_policy_estimation_methods \u001b[38;5;241m=\u001b[39m ope_dict\n\u001b[1;32m    639\u001b[0m \u001b[38;5;66;03m# Create a set of env runner actors via a EnvRunnerGroup.\u001b[39;00m\n\u001b[0;32m--> 640\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers \u001b[38;5;241m=\u001b[39m \u001b[43mEnvRunnerGroup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_policy_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_policy_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_env_runners\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;66;03m# Ensure remote workers are initially in sync with the local worker.\u001b[39;00m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers\u001b[38;5;241m.\u001b[39msync_weights(inference_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py:169\u001b[0m, in \u001b[0;36mEnvRunnerGroup.__init__\u001b[0;34m(self, env_creator, validate_env, default_policy_class, config, num_env_runners, local_env_runner, logdir, _setup, num_workers, local_worker)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _setup:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_env_runners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_env_runners\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_env_runner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_env_runner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;66;03m# EnvRunnerGroup creation possibly fails, if some (remote) workers cannot\u001b[39;00m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;66;03m# be initialized properly (due to some errors in the EnvRunners's\u001b[39;00m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;66;03m# constructor).\u001b[39;00m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m RayActorError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;66;03m# In case of an actor (remote worker) init failure, the remote worker\u001b[39;00m\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;66;03m# may still exist and will be accessible, however, e.g. calling\u001b[39;00m\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;66;03m# its `sample.remote()` would result in strange \"property not found\"\u001b[39;00m\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;66;03m# errors.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py:239\u001b[0m, in \u001b[0;36mEnvRunnerGroup._setup\u001b[0;34m(self, validate_env, config, num_env_runners, local_env_runner)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ds_shards \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# Create a number of @ray.remote workers.\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_workers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_env_runners\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_env_runners_after_construction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# If num_workers > 0 and we don't have an env on the local worker,\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m# get the observation- and action spaces for each policy from\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# the first remote worker (which does have an env).\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    248\u001b[0m     local_env_runner\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker_manager\u001b[38;5;241m.\u001b[39mnum_actors() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39mobservation_space \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39maction_space)\n\u001b[1;32m    253\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py:748\u001b[0m, in \u001b[0;36mEnvRunnerGroup.add_workers\u001b[0;34m(self, num_workers, validate)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;66;03m# Validate here, whether all remote workers have been constructed properly\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;66;03m# and are \"up and running\". Establish initial states.\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate:\n\u001b[0;32m--> 748\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_worker_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeach_actor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_healthy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    751\u001b[0m         \u001b[38;5;66;03m# Simiply raise the error, which will get handled by the try-except\u001b[39;00m\n\u001b[1;32m    752\u001b[0m         \u001b[38;5;66;03m# clause around the _setup().\u001b[39;00m\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39mok:\n\u001b[1;32m    754\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m result\u001b[38;5;241m.\u001b[39mget()\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/rllib/utils/actor_manager.py:622\u001b[0m, in \u001b[0;36mFaultTolerantActorManager.foreach_actor\u001b[0;34m(self, func, healthy_only, remote_actor_ids, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[1;32m    616\u001b[0m remote_calls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_actors(\n\u001b[1;32m    617\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m    618\u001b[0m     remote_actor_ids\u001b[38;5;241m=\u001b[39mremote_actor_ids,\n\u001b[1;32m    619\u001b[0m )\n\u001b[1;32m    621\u001b[0m \u001b[38;5;66;03m# Collect remote request results (if available given timeout and/or errors).\u001b[39;00m\n\u001b[0;32m--> 622\u001b[0m _, remote_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_actor_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremote_actor_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremote_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmark_healthy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmark_healthy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m remote_results\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/rllib/utils/actor_manager.py:476\u001b[0m, in \u001b[0;36mFaultTolerantActorManager._fetch_result\u001b[0;34m(self, remote_actor_ids, remote_calls, tags, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m remote_calls:\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [], RemoteCallResults()\n\u001b[0;32m--> 476\u001b[0m ready, _ \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Make sure remote results are fetched locally in parallel.\u001b[39;49;00m\n\u001b[1;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfetch_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# Remote data should already be fetched to local object store at this point.\u001b[39;00m\n\u001b[1;32m    485\u001b[0m remote_results \u001b[38;5;241m=\u001b[39m RemoteCallResults()\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py:21\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauto_init_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     20\u001b[0m     auto_init_ray()\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/_private/worker.py:2854\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(ray_waitables, num_returns, timeout, fetch_local)\u001b[0m\n\u001b[1;32m   2852\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeout \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m6\u001b[39m\n\u001b[1;32m   2853\u001b[0m timeout_milliseconds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m-> 2854\u001b[0m ready_ids, remaining_ids \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mray_waitables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_milliseconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_task_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfetch_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2861\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ready_ids, remaining_ids\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:3812\u001b[0m, in \u001b[0;36mray._raylet.CoreWorker.wait\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:571\u001b[0m, in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "from gymnasium.wrappers.time_limit import TimeLimit\n",
    "\n",
    "trainin_steps = 30\n",
    "\n",
    "algo = (\n",
    "    PPOConfig()\n",
    "    .training(gamma = 0.95, \n",
    "              lr = 0.001,\n",
    "              train_batch_size = 4092, \n",
    "              sgd_minibatch_size = 128, \n",
    "              num_sgd_iter = 30,\n",
    "              #entropy_coeff=0.005,\n",
    "              )\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"KeepTheDistance?dst=5&visible_nbrs=3&spawn_area=100\")\n",
    "    .build()\n",
    ")\n",
    "clear_output()\n",
    "\n",
    "env_show = RenderableKeepTheDistance(env_config)\n",
    "for i in range(trainin_steps):\n",
    "    result = algo.train()\n",
    "    simulate_episode(env_show, algo, 150, sleep_between_frames=0.03, print_info=False)\n",
    "    print(ppo_result_format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55721af53d9d4bbb9386fc1a40915b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CanvasWithBorders(height=300, width=300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env_config_2 = EnvironmentConfiguration(n_agents=20, visible_nbrs=3, target_distance=5, max_steps=500, speed=1, spawn_area=500)\n",
    "simulate_episode(RenderableKeepTheDistance(env_config_2), algo, 500, sleep_between_frames=0.03, print_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An Algorithm checkpoint has been created inside directory: 'TrainingResult(checkpoint=Checkpoint(filesystem=local, path=/mnt/c/Users/nicol/Desktop/Universit/tesi/experiments/RL_experiments/algos/KeepTheDistance?dst=5&visible_nbrs=3&spawn_area=100), metrics={'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'grad_gnorm': 10.382454860429439, 'cur_kl_coeff': 1.0124999999999997, 'cur_lr': 0.0010000000000000005, 'total_loss': 5.930729507555173, 'policy_loss': -0.0028303747193086063, 'vf_loss': 5.918511411023578, 'vf_explained_var': 0.23783627534162968, 'kl': 0.014862683368205116, 'entropy': 3.9788712683625107, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 128.0, 'num_grad_updates_lifetime': 112395.5, 'diff_num_grad_updates_vs_sampler_policy': 1904.5}}, 'num_env_steps_sampled': 122760, 'num_env_steps_trained': 122760, 'num_agent_steps_sampled': 491040, 'num_agent_steps_trained': 491040}, 'sampler_results': {'episode_reward_max': 1781.4621150949242, 'episode_reward_min': 1095.0338855979771, 'episode_reward_mean': 1501.5235576712848, 'episode_len_mean': 300.0, 'episode_media': {}, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [1425.1835123728322, 1332.2759505333186, 1440.9514760426282, 1265.3286811617165, 1225.0324585246747, 1529.5268607091336, 1421.6745034008031, 1095.0338855979771, 1522.5662513849243, 1446.008028493726, 1500.463889734888, 1440.6321232456019, 1403.0420624112326, 1459.03585235409, 1485.1767838382382, 1548.0822584007558, 1499.4410100767589, 1311.8644862378435, 1340.8831080117143, 1547.9748789161943, 1329.8759286109866, 1249.3597949170128, 1649.812997868585, 1338.4415480647663, 1532.8221447579347, 1510.4741805066542, 1237.509042060123, 1395.43279183443, 1325.4655966985474, 1607.7347405734683, 1439.1185179960441, 1467.0858912973429, 1513.196020728525, 1575.7709519600862, 1465.739650110941, 1413.6821901588726, 1308.6708950181783, 1444.2223890703272, 1433.4019893646441, 1701.0713250248996, 1351.3631829755986, 1559.0229041877374, 1440.4546078353787, 1410.949765000097, 1549.8899427909573, 1431.682431948472, 1359.0206414687282, 1468.8193562521528, 1781.4621150949242, 1488.4193296211618, 1505.0120160068313, 1688.2224253113156, 1527.6472643195755, 1562.0789087758992, 1449.26453036808, 1385.7342833074638, 1774.0796149676808, 1460.9930920742643, 1658.2644359982507, 1414.088849415588, 1537.7598689915603, 1726.077557497781, 1660.0611324883105, 1668.3107024762764, 1584.2442786004972, 1667.3281769246776, 1546.3764528417555, 1489.0067679532413, 1581.804382140497, 1503.9560401784026, 1448.213460623461, 1714.651425594168, 1365.1422994649065, 1742.6265037922599, 1241.8672323700657, 1494.6455931098985, 1600.428228040378, 1573.6225528523448, 1636.7618023031873, 1778.7869814494263, 1463.5577834544406, 1476.4034839162953, 1586.996929877974, 1716.3979402997657, 1553.5050601010569, 1411.577884897845, 1533.859939578627, 1442.112660777657, 1751.2322338299198, 1654.0450981295542, 1745.8004506786124, 1645.4592507239809, 1457.2632350304775, 1646.8806476290858, 1422.1282874702708, 1638.8624613777317, 1628.0437579443972, 1506.6075499994965, 1532.211209374932, 1332.2021165517149], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.8046015990277688, 'mean_inference_ms': 1.210794713272244, 'mean_action_processing_ms': 0.4987923798740064, 'mean_env_wait_ms': 1.1899951381657745, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.009039640426635742, 'StateBufferConnector_ms': 0.009721517562866211, 'ViewRequirementAgentConnector_ms': 0.30426454544067383}, 'num_episodes': 14, 'episode_return_max': 1781.4621150949242, 'episode_return_min': 1095.0338855979771, 'episode_return_mean': 1501.5235576712848}, 'env_runner_results': {'episode_reward_max': 1781.4621150949242, 'episode_reward_min': 1095.0338855979771, 'episode_reward_mean': 1501.5235576712848, 'episode_len_mean': 300.0, 'episode_media': {}, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [1425.1835123728322, 1332.2759505333186, 1440.9514760426282, 1265.3286811617165, 1225.0324585246747, 1529.5268607091336, 1421.6745034008031, 1095.0338855979771, 1522.5662513849243, 1446.008028493726, 1500.463889734888, 1440.6321232456019, 1403.0420624112326, 1459.03585235409, 1485.1767838382382, 1548.0822584007558, 1499.4410100767589, 1311.8644862378435, 1340.8831080117143, 1547.9748789161943, 1329.8759286109866, 1249.3597949170128, 1649.812997868585, 1338.4415480647663, 1532.8221447579347, 1510.4741805066542, 1237.509042060123, 1395.43279183443, 1325.4655966985474, 1607.7347405734683, 1439.1185179960441, 1467.0858912973429, 1513.196020728525, 1575.7709519600862, 1465.739650110941, 1413.6821901588726, 1308.6708950181783, 1444.2223890703272, 1433.4019893646441, 1701.0713250248996, 1351.3631829755986, 1559.0229041877374, 1440.4546078353787, 1410.949765000097, 1549.8899427909573, 1431.682431948472, 1359.0206414687282, 1468.8193562521528, 1781.4621150949242, 1488.4193296211618, 1505.0120160068313, 1688.2224253113156, 1527.6472643195755, 1562.0789087758992, 1449.26453036808, 1385.7342833074638, 1774.0796149676808, 1460.9930920742643, 1658.2644359982507, 1414.088849415588, 1537.7598689915603, 1726.077557497781, 1660.0611324883105, 1668.3107024762764, 1584.2442786004972, 1667.3281769246776, 1546.3764528417555, 1489.0067679532413, 1581.804382140497, 1503.9560401784026, 1448.213460623461, 1714.651425594168, 1365.1422994649065, 1742.6265037922599, 1241.8672323700657, 1494.6455931098985, 1600.428228040378, 1573.6225528523448, 1636.7618023031873, 1778.7869814494263, 1463.5577834544406, 1476.4034839162953, 1586.996929877974, 1716.3979402997657, 1553.5050601010569, 1411.577884897845, 1533.859939578627, 1442.112660777657, 1751.2322338299198, 1654.0450981295542, 1745.8004506786124, 1645.4592507239809, 1457.2632350304775, 1646.8806476290858, 1422.1282874702708, 1638.8624613777317, 1628.0437579443972, 1506.6075499994965, 1532.211209374932, 1332.2021165517149], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.8046015990277688, 'mean_inference_ms': 1.210794713272244, 'mean_action_processing_ms': 0.4987923798740064, 'mean_env_wait_ms': 1.1899951381657745, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.009039640426635742, 'StateBufferConnector_ms': 0.009721517562866211, 'ViewRequirementAgentConnector_ms': 0.30426454544067383}, 'num_episodes': 14, 'episode_return_max': 1781.4621150949242, 'episode_return_min': 1095.0338855979771, 'episode_return_mean': 1501.5235576712848}, 'episode_reward_max': 1781.4621150949242, 'episode_reward_min': 1095.0338855979771, 'episode_reward_mean': 1501.5235576712848, 'episode_len_mean': 300.0, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [1425.1835123728322, 1332.2759505333186, 1440.9514760426282, 1265.3286811617165, 1225.0324585246747, 1529.5268607091336, 1421.6745034008031, 1095.0338855979771, 1522.5662513849243, 1446.008028493726, 1500.463889734888, 1440.6321232456019, 1403.0420624112326, 1459.03585235409, 1485.1767838382382, 1548.0822584007558, 1499.4410100767589, 1311.8644862378435, 1340.8831080117143, 1547.9748789161943, 1329.8759286109866, 1249.3597949170128, 1649.812997868585, 1338.4415480647663, 1532.8221447579347, 1510.4741805066542, 1237.509042060123, 1395.43279183443, 1325.4655966985474, 1607.7347405734683, 1439.1185179960441, 1467.0858912973429, 1513.196020728525, 1575.7709519600862, 1465.739650110941, 1413.6821901588726, 1308.6708950181783, 1444.2223890703272, 1433.4019893646441, 1701.0713250248996, 1351.3631829755986, 1559.0229041877374, 1440.4546078353787, 1410.949765000097, 1549.8899427909573, 1431.682431948472, 1359.0206414687282, 1468.8193562521528, 1781.4621150949242, 1488.4193296211618, 1505.0120160068313, 1688.2224253113156, 1527.6472643195755, 1562.0789087758992, 1449.26453036808, 1385.7342833074638, 1774.0796149676808, 1460.9930920742643, 1658.2644359982507, 1414.088849415588, 1537.7598689915603, 1726.077557497781, 1660.0611324883105, 1668.3107024762764, 1584.2442786004972, 1667.3281769246776, 1546.3764528417555, 1489.0067679532413, 1581.804382140497, 1503.9560401784026, 1448.213460623461, 1714.651425594168, 1365.1422994649065, 1742.6265037922599, 1241.8672323700657, 1494.6455931098985, 1600.428228040378, 1573.6225528523448, 1636.7618023031873, 1778.7869814494263, 1463.5577834544406, 1476.4034839162953, 1586.996929877974, 1716.3979402997657, 1553.5050601010569, 1411.577884897845, 1533.859939578627, 1442.112660777657, 1751.2322338299198, 1654.0450981295542, 1745.8004506786124, 1645.4592507239809, 1457.2632350304775, 1646.8806476290858, 1422.1282874702708, 1638.8624613777317, 1628.0437579443972, 1506.6075499994965, 1532.211209374932, 1332.2021165517149], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.8046015990277688, 'mean_inference_ms': 1.210794713272244, 'mean_action_processing_ms': 0.4987923798740064, 'mean_env_wait_ms': 1.1899951381657745, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.009039640426635742, 'StateBufferConnector_ms': 0.009721517562866211, 'ViewRequirementAgentConnector_ms': 0.30426454544067383}, 'num_episodes': 14, 'episode_return_max': 1781.4621150949242, 'episode_return_min': 1095.0338855979771, 'episode_return_mean': 1501.5235576712848, 'num_healthy_workers': 1, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 491040, 'num_agent_steps_trained': 491040, 'num_env_steps_sampled': 122760, 'num_env_steps_trained': 122760, 'num_env_steps_sampled_this_iter': 4092, 'num_env_steps_trained_this_iter': 4092, 'num_env_steps_sampled_throughput_per_sec': 96.034397487011, 'num_env_steps_trained_throughput_per_sec': 96.034397487011, 'timesteps_total': 122760, 'num_env_steps_sampled_lifetime': 122760, 'num_agent_steps_sampled_lifetime': 491040, 'num_steps_trained_this_iter': 4092, 'agent_timesteps_total': 491040, 'timers': {'training_iteration_time_ms': 42865.943, 'restore_workers_time_ms': 0.018, 'training_step_time_ms': 42865.893, 'sample_time_ms': 15498.814, 'load_time_ms': 1.074, 'load_throughput': 3811225.539, 'learn_time_ms': 27357.513, 'learn_throughput': 149.575, 'synch_weights_time_ms': 6.228}, 'counters': {'num_env_steps_sampled': 122760, 'num_env_steps_trained': 122760, 'num_agent_steps_sampled': 491040, 'num_agent_steps_trained': 491040}, 'done': False, 'episodes_total': 409, 'training_iteration': 30, 'trial_id': 'default', 'date': '2024-05-28_17-32-10', 'timestamp': 1716910330, 'time_this_iter_s': 42.617337465286255, 'time_total_s': 1277.5251846313477, 'pid': 15814, 'hostname': 'LAPTOP-9AD2MD1C', 'node_ip': '172.23.87.11', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'enable_rl_module_and_learner': False, 'enable_env_runner_and_connector_v2': False, 'env': 'KeepTheDistance?dst=5&visible_nbrs=3&spawn_area=100', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'env_task_fn': None, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_env_runner': 1, 'validate_env_runners_after_construction': True, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'enable_connectors': True, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.95, 'lr': 0.001, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 4092, 'train_batch_size_per_learner': None, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x7f18f2b842c0>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'always_attach_evaluation_results': True, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_run_training_always_in_thread': False, '_evaluation_parallel_to_training_wo_thread': False, 'ignore_env_runner_failures': False, 'recreate_failed_env_runners': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30, 'env_runner_restore_timeout_s': 1800, '_model_config_dict': {}, '_rl_module_spec': None, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'disable_env_checking': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'mini_batch_size_per_learner': None, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 1}, 'time_since_restore': 1277.5251846313477, 'iterations_since_restore': 30, 'perf': {'cpu_util_percent': 39.045588235294126, 'ram_util_percent': 76.19117647058823}})'.\n"
     ]
    }
   ],
   "source": [
    "save_algo(algo, \"KeepTheDistance?dst=5&visible_nbrs=3&spawn_area=100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KeepTheDistance?dst=10&visible_nbrs=3&spawn_area=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "base_algo = load_algo(\"KeepTheDistance?dst=5&visible_nbrs=3&spawn_area=100\")\n",
    "env_config = EnvironmentConfiguration(n_agents=4, visible_nbrs=3, target_distance=10, max_steps=300, speed=1, spawn_area=100)\n",
    "register_env(\"KeepTheDistance?dst=10&visible_nbrs=3&spawn_area=100\", lambda _: KeepTheDistance(env_config))\n",
    "\n",
    "algo = (\n",
    "    PPOConfig()\n",
    "    .training(gamma = 0.95, \n",
    "              lr = 0.001,\n",
    "              train_batch_size = 4092, \n",
    "              sgd_minibatch_size = 128, \n",
    "              num_sgd_iter = 30,\n",
    "              #entropy_coeff=0.005,\n",
    "              )\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"KeepTheDistance?dst=10&visible_nbrs=3&spawn_area=100\")\n",
    "    .build()\n",
    ")\n",
    "clear_output()\n",
    "algo.set_weights(base_algo.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-29 09:36:48,327\tWARNING deprecation.py:50 -- DeprecationWarning: `WorkerSet(num_workers=... OR local_worker=...)` has been deprecated. Use `EnvRunnerGroup(num_env_runners=... AND local_env_runner=...)` instead. This will raise an error in the future!\n",
      "2024-05-29 09:36:53,543\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "algo = load_algo(\"KeepTheDistance?dst=10&visible_nbrs=3&spawn_area=100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:521: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2024-05-28 18:31:44,767\tWARNING deprecation.py:50 -- DeprecationWarning: `WorkerSet(num_workers=... OR local_worker=...)` has been deprecated. Use `EnvRunnerGroup(num_env_runners=... AND local_env_runner=...)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 20\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtime_limit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TimeLimit\n\u001b[1;32m      6\u001b[0m trainin_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m\n\u001b[1;32m      8\u001b[0m algo \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      9\u001b[0m     \u001b[43mPPOConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m              \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m              \u001b[49m\u001b[43mtrain_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4092\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m              \u001b[49m\u001b[43msgd_minibatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m              \u001b[49m\u001b[43mnum_sgd_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m              \u001b[49m\u001b[38;5;66;43;03m#entropy_coeff=0.005,\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m              \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_runners\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_env_runners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresources\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvironment\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mKeepTheDistance?dst=10&visible_nbrs=3&spawn_area=100\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m clear_output()\n\u001b[1;32m     24\u001b[0m env_show \u001b[38;5;241m=\u001b[39m RenderableKeepTheDistance(env_config)\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm_config.py:859\u001b[0m, in \u001b[0;36mAlgorithmConfig.build\u001b[0;34m(self, env, logger_creator, use_copy)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgo_class, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    857\u001b[0m     algo_class \u001b[38;5;241m=\u001b[39m get_trainable_cls(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgo_class)\n\u001b[0;32m--> 859\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgo_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_copy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:554\u001b[0m, in \u001b[0;36mAlgorithm.__init__\u001b[0;34m(self, config, env, logger_creator, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;66;03m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;66;03m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;66;03m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;66;03m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;66;03m# TODO: Don't dump sampler results into top-level.\u001b[39;00m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    551\u001b[0m     },\n\u001b[1;32m    552\u001b[0m }\n\u001b[0;32m--> 554\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/trainable/trainable.py:158\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, storage)\u001b[0m\n\u001b[1;32m    154\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStorageContext on the TRAINABLE:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mstorage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open_logfiles(stdout_file, stderr_file)\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m setup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m setup_time \u001b[38;5;241m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:640\u001b[0m, in \u001b[0;36mAlgorithm.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moff_policy_estimation_methods \u001b[38;5;241m=\u001b[39m ope_dict\n\u001b[1;32m    639\u001b[0m \u001b[38;5;66;03m# Create a set of env runner actors via a EnvRunnerGroup.\u001b[39;00m\n\u001b[0;32m--> 640\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers \u001b[38;5;241m=\u001b[39m \u001b[43mEnvRunnerGroup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_policy_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_policy_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_env_runners\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;66;03m# Ensure remote workers are initially in sync with the local worker.\u001b[39;00m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers\u001b[38;5;241m.\u001b[39msync_weights(inference_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py:169\u001b[0m, in \u001b[0;36mEnvRunnerGroup.__init__\u001b[0;34m(self, env_creator, validate_env, default_policy_class, config, num_env_runners, local_env_runner, logdir, _setup, num_workers, local_worker)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _setup:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_env_runners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_env_runners\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_env_runner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_env_runner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;66;03m# EnvRunnerGroup creation possibly fails, if some (remote) workers cannot\u001b[39;00m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;66;03m# be initialized properly (due to some errors in the EnvRunners's\u001b[39;00m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;66;03m# constructor).\u001b[39;00m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m RayActorError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;66;03m# In case of an actor (remote worker) init failure, the remote worker\u001b[39;00m\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;66;03m# may still exist and will be accessible, however, e.g. calling\u001b[39;00m\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;66;03m# its `sample.remote()` would result in strange \"property not found\"\u001b[39;00m\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;66;03m# errors.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py:239\u001b[0m, in \u001b[0;36mEnvRunnerGroup._setup\u001b[0;34m(self, validate_env, config, num_env_runners, local_env_runner)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ds_shards \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# Create a number of @ray.remote workers.\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_workers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_env_runners\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_env_runners_after_construction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# If num_workers > 0 and we don't have an env on the local worker,\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m# get the observation- and action spaces for each policy from\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# the first remote worker (which does have an env).\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    248\u001b[0m     local_env_runner\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker_manager\u001b[38;5;241m.\u001b[39mnum_actors() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39mobservation_space \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39maction_space)\n\u001b[1;32m    253\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py:748\u001b[0m, in \u001b[0;36mEnvRunnerGroup.add_workers\u001b[0;34m(self, num_workers, validate)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;66;03m# Validate here, whether all remote workers have been constructed properly\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;66;03m# and are \"up and running\". Establish initial states.\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate:\n\u001b[0;32m--> 748\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_worker_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeach_actor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_healthy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    751\u001b[0m         \u001b[38;5;66;03m# Simiply raise the error, which will get handled by the try-except\u001b[39;00m\n\u001b[1;32m    752\u001b[0m         \u001b[38;5;66;03m# clause around the _setup().\u001b[39;00m\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39mok:\n\u001b[1;32m    754\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m result\u001b[38;5;241m.\u001b[39mget()\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/utils/actor_manager.py:622\u001b[0m, in \u001b[0;36mFaultTolerantActorManager.foreach_actor\u001b[0;34m(self, func, healthy_only, remote_actor_ids, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[1;32m    616\u001b[0m remote_calls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_actors(\n\u001b[1;32m    617\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m    618\u001b[0m     remote_actor_ids\u001b[38;5;241m=\u001b[39mremote_actor_ids,\n\u001b[1;32m    619\u001b[0m )\n\u001b[1;32m    621\u001b[0m \u001b[38;5;66;03m# Collect remote request results (if available given timeout and/or errors).\u001b[39;00m\n\u001b[0;32m--> 622\u001b[0m _, remote_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_actor_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremote_actor_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremote_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmark_healthy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmark_healthy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m remote_results\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/utils/actor_manager.py:476\u001b[0m, in \u001b[0;36mFaultTolerantActorManager._fetch_result\u001b[0;34m(self, remote_actor_ids, remote_calls, tags, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m remote_calls:\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [], RemoteCallResults()\n\u001b[0;32m--> 476\u001b[0m ready, _ \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Make sure remote results are fetched locally in parallel.\u001b[39;49;00m\n\u001b[1;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfetch_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# Remote data should already be fetched to local object store at this point.\u001b[39;00m\n\u001b[1;32m    485\u001b[0m remote_results \u001b[38;5;241m=\u001b[39m RemoteCallResults()\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py:21\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauto_init_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     20\u001b[0m     auto_init_ray()\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py:2854\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(ray_waitables, num_returns, timeout, fetch_local)\u001b[0m\n\u001b[1;32m   2852\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeout \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m6\u001b[39m\n\u001b[1;32m   2853\u001b[0m timeout_milliseconds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m-> 2854\u001b[0m ready_ids, remaining_ids \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mray_waitables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_milliseconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_task_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfetch_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2861\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ready_ids, remaining_ids\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:3812\u001b[0m, in \u001b[0;36mray._raylet.CoreWorker.wait\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:571\u001b[0m, in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "from gymnasium.wrappers.time_limit import TimeLimit\n",
    "\n",
    "trainin_steps = 30\n",
    "\n",
    "algo = (\n",
    "    PPOConfig()\n",
    "    .training(gamma = 0.95, \n",
    "              lr = 0.001,\n",
    "              train_batch_size = 4092, \n",
    "              sgd_minibatch_size = 128, \n",
    "              num_sgd_iter = 30,\n",
    "              #entropy_coeff=0.005,\n",
    "              )\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"KeepTheDistance?dst=10&visible_nbrs=3&spawn_area=100\")\n",
    "    .build()\n",
    ")\n",
    "clear_output()\n",
    "\n",
    "env_show = RenderableKeepTheDistance(env_config)\n",
    "for i in range(trainin_steps):\n",
    "    result = algo.train()\n",
    "    simulate_episode(env_show, algo, 150, sleep_between_frames=0.03, print_info=False)\n",
    "    print(ppo_result_format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45c849f071d0433b9fca937db6f39842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CanvasWithBorders(height=300, width=300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env_config_2 = EnvironmentConfiguration(n_agents=20, visible_nbrs=3, target_distance=10, max_steps=500, speed=1, spawn_area=100)\n",
    "simulate_episode(RenderableKeepTheDistance(env_config_2), algo, 500, sleep_between_frames=0.03, print_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An Algorithm checkpoint has been created inside directory: 'TrainingResult(checkpoint=Checkpoint(filesystem=local, path=/mnt/c/Users/nicol/Desktop/Universit/tesi/experiments/RL_experiments/algos/KeepTheDistance?dst=10&visible_nbrs=3&spawn_area=100), metrics={'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'grad_gnorm': 13.893579628899342, 'cur_kl_coeff': 1.0124999999999997, 'cur_lr': 0.0010000000000000005, 'total_loss': 5.905990294521562, 'policy_loss': -0.012485336998277136, 'vf_loss': 5.903962938992057, 'vf_explained_var': 0.29801289379753154, 'kl': 0.014333522689229024, 'entropy': 4.284107104749505, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 128.0, 'num_grad_updates_lifetime': 112395.5, 'diff_num_grad_updates_vs_sampler_policy': 1904.5}}, 'num_env_steps_sampled': 122760, 'num_env_steps_trained': 122760, 'num_agent_steps_sampled': 491040, 'num_agent_steps_trained': 491040}, 'sampler_results': {'episode_reward_max': 1468.8297722677182, 'episode_reward_min': 745.171178340648, 'episode_reward_mean': 1090.5520943972133, 'episode_len_mean': 300.0, 'episode_media': {}, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [781.5421448271898, 895.8147878468372, 1043.2205456443123, 1084.0518953451603, 1031.5246741535852, 1043.1801712549068, 1089.7398324178735, 861.9721692997325, 1201.6520711458359, 973.2397725629621, 1021.4433709830389, 976.1688076824132, 869.233566900942, 1027.1541315203722, 745.171178340648, 1005.2251699605579, 1056.5226007443086, 1128.8774657888043, 920.2160327360052, 1243.1735511119587, 903.4940333028477, 921.2712968426539, 942.2408777912334, 945.7123023564, 845.2336650745435, 1037.46962462781, 1090.9079247825036, 857.6978538820205, 1190.423467599135, 947.443616555195, 1122.7974504100487, 1077.3104457520803, 1067.5516776423865, 1032.9584565194755, 1085.0236756996333, 1192.2219814237205, 991.2442017930332, 1056.253324074556, 882.1035506647615, 1192.8073603114758, 819.4977333880344, 952.92932779019, 1037.09818882403, 955.6157375577313, 1146.01621684555, 1027.1992763748503, 1382.185024916337, 1164.970061624971, 1034.2380128002433, 977.7326814821206, 1092.677705872261, 1266.72542840609, 1380.1632861249625, 1163.5806780500282, 1105.6034298956945, 1159.3136243667884, 1295.9496379868244, 901.1664365037885, 1276.3745920438864, 967.4398248448141, 1157.0064638100162, 1012.8569236861222, 933.1061661313055, 1120.0282707313463, 1131.9919026412965, 1271.4871030054956, 1039.6458400081215, 950.01244623798, 1468.8297722677182, 1019.9874886841765, 1130.9402128474233, 1250.534048260656, 1353.8994904190638, 982.5163954649379, 1169.6993810310187, 1254.9205792394334, 1188.3182417043, 1144.643496428629, 1064.7407811753828, 974.0663829120015, 1232.9363610071787, 1139.6624185558899, 1143.9549737008058, 1280.9559402194088, 1318.1234267081982, 1290.6394933954005, 1307.3334939886338, 867.9418443482991, 1245.1292176456343, 1328.3770190610571, 1250.0330269999433, 1144.9710803179794, 898.8366004455341, 1215.7055002469208, 1262.778270130191, 1006.6569834685304, 1424.194916531349, 1269.5507561389627, 1206.6050748234368, 1113.8240222293869], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.8379311957470388, 'mean_inference_ms': 1.2640695687812515, 'mean_action_processing_ms': 0.5288971027348052, 'mean_env_wait_ms': 1.2527307434797743, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.010395288467407227, 'StateBufferConnector_ms': 0.008207321166992188, 'ViewRequirementAgentConnector_ms': 0.31988096237182617}, 'num_episodes': 14, 'episode_return_max': 1468.8297722677182, 'episode_return_min': 745.171178340648, 'episode_return_mean': 1090.5520943972133}, 'env_runner_results': {'episode_reward_max': 1468.8297722677182, 'episode_reward_min': 745.171178340648, 'episode_reward_mean': 1090.5520943972133, 'episode_len_mean': 300.0, 'episode_media': {}, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [781.5421448271898, 895.8147878468372, 1043.2205456443123, 1084.0518953451603, 1031.5246741535852, 1043.1801712549068, 1089.7398324178735, 861.9721692997325, 1201.6520711458359, 973.2397725629621, 1021.4433709830389, 976.1688076824132, 869.233566900942, 1027.1541315203722, 745.171178340648, 1005.2251699605579, 1056.5226007443086, 1128.8774657888043, 920.2160327360052, 1243.1735511119587, 903.4940333028477, 921.2712968426539, 942.2408777912334, 945.7123023564, 845.2336650745435, 1037.46962462781, 1090.9079247825036, 857.6978538820205, 1190.423467599135, 947.443616555195, 1122.7974504100487, 1077.3104457520803, 1067.5516776423865, 1032.9584565194755, 1085.0236756996333, 1192.2219814237205, 991.2442017930332, 1056.253324074556, 882.1035506647615, 1192.8073603114758, 819.4977333880344, 952.92932779019, 1037.09818882403, 955.6157375577313, 1146.01621684555, 1027.1992763748503, 1382.185024916337, 1164.970061624971, 1034.2380128002433, 977.7326814821206, 1092.677705872261, 1266.72542840609, 1380.1632861249625, 1163.5806780500282, 1105.6034298956945, 1159.3136243667884, 1295.9496379868244, 901.1664365037885, 1276.3745920438864, 967.4398248448141, 1157.0064638100162, 1012.8569236861222, 933.1061661313055, 1120.0282707313463, 1131.9919026412965, 1271.4871030054956, 1039.6458400081215, 950.01244623798, 1468.8297722677182, 1019.9874886841765, 1130.9402128474233, 1250.534048260656, 1353.8994904190638, 982.5163954649379, 1169.6993810310187, 1254.9205792394334, 1188.3182417043, 1144.643496428629, 1064.7407811753828, 974.0663829120015, 1232.9363610071787, 1139.6624185558899, 1143.9549737008058, 1280.9559402194088, 1318.1234267081982, 1290.6394933954005, 1307.3334939886338, 867.9418443482991, 1245.1292176456343, 1328.3770190610571, 1250.0330269999433, 1144.9710803179794, 898.8366004455341, 1215.7055002469208, 1262.778270130191, 1006.6569834685304, 1424.194916531349, 1269.5507561389627, 1206.6050748234368, 1113.8240222293869], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.8379311957470388, 'mean_inference_ms': 1.2640695687812515, 'mean_action_processing_ms': 0.5288971027348052, 'mean_env_wait_ms': 1.2527307434797743, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.010395288467407227, 'StateBufferConnector_ms': 0.008207321166992188, 'ViewRequirementAgentConnector_ms': 0.31988096237182617}, 'num_episodes': 14, 'episode_return_max': 1468.8297722677182, 'episode_return_min': 745.171178340648, 'episode_return_mean': 1090.5520943972133}, 'episode_reward_max': 1468.8297722677182, 'episode_reward_min': 745.171178340648, 'episode_reward_mean': 1090.5520943972133, 'episode_len_mean': 300.0, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [781.5421448271898, 895.8147878468372, 1043.2205456443123, 1084.0518953451603, 1031.5246741535852, 1043.1801712549068, 1089.7398324178735, 861.9721692997325, 1201.6520711458359, 973.2397725629621, 1021.4433709830389, 976.1688076824132, 869.233566900942, 1027.1541315203722, 745.171178340648, 1005.2251699605579, 1056.5226007443086, 1128.8774657888043, 920.2160327360052, 1243.1735511119587, 903.4940333028477, 921.2712968426539, 942.2408777912334, 945.7123023564, 845.2336650745435, 1037.46962462781, 1090.9079247825036, 857.6978538820205, 1190.423467599135, 947.443616555195, 1122.7974504100487, 1077.3104457520803, 1067.5516776423865, 1032.9584565194755, 1085.0236756996333, 1192.2219814237205, 991.2442017930332, 1056.253324074556, 882.1035506647615, 1192.8073603114758, 819.4977333880344, 952.92932779019, 1037.09818882403, 955.6157375577313, 1146.01621684555, 1027.1992763748503, 1382.185024916337, 1164.970061624971, 1034.2380128002433, 977.7326814821206, 1092.677705872261, 1266.72542840609, 1380.1632861249625, 1163.5806780500282, 1105.6034298956945, 1159.3136243667884, 1295.9496379868244, 901.1664365037885, 1276.3745920438864, 967.4398248448141, 1157.0064638100162, 1012.8569236861222, 933.1061661313055, 1120.0282707313463, 1131.9919026412965, 1271.4871030054956, 1039.6458400081215, 950.01244623798, 1468.8297722677182, 1019.9874886841765, 1130.9402128474233, 1250.534048260656, 1353.8994904190638, 982.5163954649379, 1169.6993810310187, 1254.9205792394334, 1188.3182417043, 1144.643496428629, 1064.7407811753828, 974.0663829120015, 1232.9363610071787, 1139.6624185558899, 1143.9549737008058, 1280.9559402194088, 1318.1234267081982, 1290.6394933954005, 1307.3334939886338, 867.9418443482991, 1245.1292176456343, 1328.3770190610571, 1250.0330269999433, 1144.9710803179794, 898.8366004455341, 1215.7055002469208, 1262.778270130191, 1006.6569834685304, 1424.194916531349, 1269.5507561389627, 1206.6050748234368, 1113.8240222293869], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.8379311957470388, 'mean_inference_ms': 1.2640695687812515, 'mean_action_processing_ms': 0.5288971027348052, 'mean_env_wait_ms': 1.2527307434797743, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.010395288467407227, 'StateBufferConnector_ms': 0.008207321166992188, 'ViewRequirementAgentConnector_ms': 0.31988096237182617}, 'num_episodes': 14, 'episode_return_max': 1468.8297722677182, 'episode_return_min': 745.171178340648, 'episode_return_mean': 1090.5520943972133, 'num_healthy_workers': 1, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 491040, 'num_agent_steps_trained': 491040, 'num_env_steps_sampled': 122760, 'num_env_steps_trained': 122760, 'num_env_steps_sampled_this_iter': 4092, 'num_env_steps_trained_this_iter': 4092, 'num_env_steps_sampled_throughput_per_sec': 99.29533934369205, 'num_env_steps_trained_throughput_per_sec': 99.29533934369205, 'timesteps_total': 122760, 'num_env_steps_sampled_lifetime': 122760, 'num_agent_steps_sampled_lifetime': 491040, 'num_steps_trained_this_iter': 4092, 'agent_timesteps_total': 491040, 'timers': {'training_iteration_time_ms': 42932.798, 'restore_workers_time_ms': 0.019, 'training_step_time_ms': 42932.748, 'sample_time_ms': 15347.103, 'load_time_ms': 0.86, 'load_throughput': 4759724.886, 'learn_time_ms': 27576.57, 'learn_throughput': 148.387, 'synch_weights_time_ms': 5.841}, 'counters': {'num_env_steps_sampled': 122760, 'num_env_steps_trained': 122760, 'num_agent_steps_sampled': 491040, 'num_agent_steps_trained': 491040}, 'done': False, 'episodes_total': 409, 'training_iteration': 30, 'trial_id': 'default', 'date': '2024-05-28_18-02-41', 'timestamp': 1716912161, 'time_this_iter_s': 41.215588331222534, 'time_total_s': 1337.1167497634888, 'pid': 15814, 'hostname': 'LAPTOP-9AD2MD1C', 'node_ip': '172.23.87.11', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'enable_rl_module_and_learner': False, 'enable_env_runner_and_connector_v2': False, 'env': 'KeepTheDistance?dst=10&visible_nbrs=3&spawn_area=100', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'env_task_fn': None, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_env_runner': 1, 'validate_env_runners_after_construction': True, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'enable_connectors': True, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.95, 'lr': 0.001, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 4092, 'train_batch_size_per_learner': None, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x7f18f2b842c0>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'always_attach_evaluation_results': True, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_run_training_always_in_thread': False, '_evaluation_parallel_to_training_wo_thread': False, 'ignore_env_runner_failures': False, 'recreate_failed_env_runners': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30, 'env_runner_restore_timeout_s': 1800, '_model_config_dict': {}, '_rl_module_spec': None, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'disable_env_checking': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'mini_batch_size_per_learner': None, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 1}, 'time_since_restore': 1337.1167497634888, 'iterations_since_restore': 30, 'perf': {'cpu_util_percent': 39.22686567164179, 'ram_util_percent': 77.62835820895523}})'.\n"
     ]
    }
   ],
   "source": [
    "save_algo(algo, \"KeepTheDistance?dst=10&visible_nbrs=3&spawn_area=100\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tianEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
