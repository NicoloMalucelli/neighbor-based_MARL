{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect the items - discrete actions\n",
    "\n",
    "in order to be able to use rllib's DQN is necessary to implement a new version of the environment in which actions are discrete instead of continuous. \n",
    "\n",
    "Still the agents' goal is the same as before: collecting all the items in the environment minimizing the number of steps by collaborating together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import and utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.vectors import Vector2D\n",
    "from utils.canvas import CanvasWithBorders\n",
    "from utils.algo_utils import (save_algo, load_algo)\n",
    "from utils.simulations import (simulate_episode, simulate_random_episode, dqn_result_format)\n",
    "\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from gymnasium.spaces import Discrete, Box, Dict, Tuple, MultiDiscrete\n",
    "from gymnasium.spaces.utils import flatten, flatten_space\n",
    "from IPython.display import clear_output\n",
    "from ipycanvas import Canvas, hold_canvas\n",
    "from typing import Set\n",
    "import random as rnd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvironmentConfiguration: \n",
    "    def __init__(self, n_agents, n_targets, agent_range, movement_granularity, speed_granularity, spawn_area=100, visible_nbrs=1, visible_targets=1, max_steps=None, cache_size=1):\n",
    "        # parameters that shouldn't affect the agents' behaviour\n",
    "        self.n_agents = n_agents\n",
    "        self.n_targets = n_targets\n",
    "        self.spawn_area = spawn_area\n",
    "        self.max_steps = max_steps\n",
    "        # parameters that affect the agents' behavious\n",
    "        self.agent_range = agent_range\n",
    "        # parameters that affect the observation space\n",
    "        self.visible_nbrs = visible_nbrs\n",
    "        self.visible_targets = visible_targets\n",
    "        self.cache_size = cache_size\n",
    "        # parameters that affect the action space\n",
    "        self.movement_granularity = movement_granularity\n",
    "        self.speed_granularity = speed_granularity\n",
    "\n",
    "    def __deepcopy__(self, memo):\n",
    "        return EnvironmentConfiguration(\n",
    "            self.n_agents, \n",
    "            self.n_targets, \n",
    "            self.agent_range, \n",
    "            self.movement_granularity, \n",
    "            self.speed_granularity, \n",
    "            self.spawn_area, \n",
    "            self.visible_nbrs, \n",
    "            self.visible_targets, \n",
    "            self.max_steps, \n",
    "            self.cache_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollectTheItems(MultiAgentEnv):\n",
    "    canvas = None\n",
    "    CANVAS_WIDTH, CANVAS_HEIGHT = 300.0, 300.0\n",
    "\n",
    "    def __init__(self, config: EnvironmentConfiguration):\n",
    "        assert config.n_agents > config.visible_nbrs\n",
    "        assert config.movement_granularity % 2 == 1\n",
    "\n",
    "        self.n_agents = config.n_agents\n",
    "        self.n_targets = config.n_targets\n",
    "        self.spawn_area = config.spawn_area\n",
    "        self.max_steps = config.max_steps\n",
    "        self.agent_range = config.agent_range\n",
    "        self.visible_nbrs = config.visible_nbrs\n",
    "        self.visible_targets = config.visible_targets\n",
    "        self.cache_size = config.cache_size\n",
    "        self.movement_granularity = config.movement_granularity\n",
    "        self.speed_granularity = config.speed_granularity\n",
    "\n",
    "        self.agents_ids = ['agent-' + str(i) for i in range(self.n_agents)]\n",
    "        self.observation_space = self.observation_space('agent-0')\n",
    "        self.action_space = self.action_space('agent-0')\n",
    "\n",
    "    def unflatten_observation_space(self, agent):\n",
    "        direction = Box(low=-1, high=1, shape=(2,1), dtype=np.float32)\n",
    "        distance = Box(low=-np.inf, high=np.inf, shape=(1,1), dtype=np.float32)\n",
    "\n",
    "        nbrs = Dict({f\"nbr-{i}\": Dict({'direction': direction, 'distance': distance}) for i in range(self.visible_nbrs)})\n",
    "        targets = Dict({f\"target-{i}\": Dict({'direction': direction, 'distance': distance}) for i in range(self.visible_targets)})\n",
    "\n",
    "        time_t_obs = Dict({\"nbrs\": nbrs, \"targets\": targets})\n",
    "\n",
    "        return Dict({f\"t[-{t}]\": time_t_obs for t in range(0, self.cache_size)})\n",
    "\n",
    "    def observation_space(self, agent):\n",
    "        return flatten_space(self.unflatten_observation_space(agent))\n",
    "\n",
    "    def __continuous_action(self, discrete_action):\n",
    "        action_tuple = (discrete_action // (self.movement_granularity*self.speed_granularity), \n",
    "                        (discrete_action % (self.movement_granularity*self.speed_granularity)) // (self.speed_granularity), \n",
    "                        discrete_action % self.speed_granularity)\n",
    "\n",
    "        return [(2*(action_tuple[0] / (self.movement_granularity-1))-1),\n",
    "                (2*(action_tuple[1] / (self.movement_granularity-1))-1),\n",
    "                (action_tuple[2]) / float(self.speed_granularity-1)]\n",
    "\n",
    "    def action_space(self, agent):\n",
    "        \"\"\"\n",
    "        direction_x = Discrete(self.movement_granularity)#Box(low=-1.0, high=1.0, shape=(2,1), dtype=np.float32)\n",
    "        direction_y = Discrete(self.movement_granularity)\n",
    "        speed = Discrete(self.speed_granularity)#Box(0.0, 1.0, dtype=np.float32)\n",
    "        return Tuple([direction_x, direction_y, speed])\n",
    "        \"\"\"\n",
    "        return Discrete(self.movement_granularity * self.movement_granularity * self.speed_granularity)\n",
    "    \n",
    "    def __get_time_t_observation(self, agent):\n",
    "        nbrs_distance_vectors = [Vector2D.distance_vector(self.agents_pos[agent], self.agents_pos[nbr])  \n",
    "                            for nbr in self.__get_n_closest_neighbours(agent, self.visible_nbrs)]\n",
    "\n",
    "        targets_distance_vectors = [Vector2D.distance_vector(self.agents_pos[agent], self.targets_pos[target])  \n",
    "                            for target in self.__get_n_closest_targets(agent, self.visible_targets)]\n",
    "\n",
    "        nbrs = {\n",
    "            f\"nbr-{i}\": {\n",
    "                \"direction\": Vector2D.unit_vector(nbrs_distance_vectors[i]).to_np_array(),\n",
    "                \"distance\": np.log(1 + Vector2D.norm(nbrs_distance_vectors[i])) #1 - np.exp(-alpha * x)\n",
    "            }\n",
    "            for i in range(len(nbrs_distance_vectors))\n",
    "        }\n",
    "    \n",
    "        targets = {\n",
    "            f\"target-{i}\": {\n",
    "                \"direction\": Vector2D.unit_vector(targets_distance_vectors[i]).to_np_array(),\n",
    "                \"distance\": np.log(1 + Vector2D.norm(targets_distance_vectors[i])) #1 - np.exp(-alpha * x)\n",
    "            }\n",
    "            for i in range(len(targets_distance_vectors))\n",
    "        }\n",
    "        \n",
    "        for i in range(len(targets_distance_vectors), self.visible_targets):\n",
    "            targets[f\"target-{i}\"] = {\n",
    "                \"direction\": np.array([0,0], dtype=np.int32),\n",
    "                \"distance\": -1 #1 - np.exp(-alpha * x)\n",
    "            }\n",
    "\n",
    "        obs = {\n",
    "            \"nbrs\": nbrs,\n",
    "            \"targets\": targets\n",
    "        }\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def __get_observation(self, agent):\n",
    "        if len(self.observation_cache[agent]) == 0:\n",
    "            self.observation_cache[agent] = [self.__get_time_t_observation(agent)]*self.cache_size\n",
    "        else:\n",
    "            self.observation_cache[agent] = [self.__get_time_t_observation(agent)] + self.observation_cache[agent]\n",
    "            self.observation_cache[agent].pop()\n",
    "\n",
    "        obs = {\n",
    "            f\"t[-{t}]\": self.observation_cache[agent][t]\n",
    "            for t in range(0, self.cache_size)\n",
    "        }\n",
    "\n",
    "        return flatten(self.unflatten_observation_space(agent), obs)\n",
    "\n",
    "    def rgb_to_hex(self, r, g, b):\n",
    "        return f'#{r:02x}{g:02x}{b:02x}'\n",
    "\n",
    "    def __get_local_reward(self, agent, action):\n",
    "        # reward_1: small bonus if the agent collects an item\n",
    "        reward_1 = +5 if agent in self.collectors else 0\n",
    "\n",
    "        # reward_2: malus if the agent collides with another agent \n",
    "        reward_2= sum([-2 if Vector2D.distance(self.agents_pos[agent], self.agents_pos[nbr]) < self.agent_range*2 else 0 for nbr in self.__get_other_agents(agent)])\n",
    "\n",
    "        # reward_3: -1 at each step\n",
    "        reward_3 = -1\n",
    "\n",
    "        # reward_4: positive reward if the agent moves toward the closest targets, negative otherwise\n",
    "        distance_diff = ([Vector2D.distance(self.agent_old_pos[agent], self.targets_pos[target]) -\n",
    "                    Vector2D.distance(self.agents_pos[agent], self.targets_pos[target])\n",
    "            for target in self.closest_targets[agent]])\n",
    "        \n",
    "        reward_4 = max(distance_diff) if len(distance_diff) > 0 else 0\n",
    "\n",
    "        self.info[agent] = {\"info\": {f\"r2: {reward_2}, r3: {reward_3}, r4: {reward_4}\"}}\n",
    "        return  reward_2 + reward_3 + reward_4*3\n",
    "\n",
    "    def __get_global_reward(self):\n",
    "        return self.global_reward * 100\n",
    "    \n",
    "    def __get_other_agents(self, agent):\n",
    "        return [other for other in self.agents_ids if other != agent]\n",
    "\n",
    "    def __get_n_closest_neighbours(self, agent, n=1):\n",
    "        distances = {other: Vector2D.distance(self.agents_pos[agent], self.agents_pos[other]) for other in self.__get_other_agents(agent)}\n",
    "        return [neighbour[0] for neighbour in sorted(list(distances.items()), key=lambda d: d[1])[:n]]\n",
    "        # return {neighbour[0]: neighbour[1] for neighbour in sorted(list(dst.items()), key=lambda d: d[0])[:n]}\n",
    "\n",
    "    def __get_n_closest_targets(self, agent, n=1):\n",
    "        n = min(n, len(self.targets_pos.keys()))\n",
    "        distances = {target: Vector2D.distance(self.agents_pos[agent], pos) for target, pos in self.targets_pos.items()}\n",
    "        self.closest_targets[agent] = [target[0] for target in sorted(list(distances.items()), key=lambda d: d[1])[:n]]\n",
    "        return self.closest_targets[agent]\n",
    "\n",
    "    def __update_agent_position(self, agent, action):\n",
    "        unit_movement = Vector2D(action[0], action[1])\n",
    "        self.agent_old_pos[agent] = self.agents_pos[agent]\n",
    "        self.agents_pos[agent] = Vector2D.sum(self.agents_pos[agent], Vector2D.mul(unit_movement, action[2]))\n",
    "\n",
    "    def __collect_items(self):\n",
    "        self.collectors = []\n",
    "        uncollected_targets = {}\n",
    "        for target, target_pos in self.targets_pos.items():\n",
    "            collected = False\n",
    "            for agent in self.agents_pos.values():\n",
    "                if Vector2D.distance(target_pos, agent) < self.agent_range:\n",
    "                    collected = True\n",
    "                    self.collectors.append(agent)\n",
    "            if not collected:\n",
    "                uncollected_targets[target] = target_pos\n",
    "        self.targets_pos = uncollected_targets\n",
    "\n",
    "    def __collect_items_and_compute_global_reward(self):\n",
    "        old_uncollected_items = len(self.targets_pos.keys())\n",
    "        self.__collect_items()\n",
    "        updated_uncollected_items = len(self.targets_pos.keys())\n",
    "        self.global_reward = old_uncollected_items - updated_uncollected_items\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.steps = 0\n",
    "        self.agents_pos = {agent: Vector2D.get_random_point(max_x=self.spawn_area, max_y=self.spawn_area) for agent in self.agents_ids}\n",
    "        self.agent_old_pos = dict(self.agents_pos)\n",
    "        self.targets_pos = {f\"target-{i}\": Vector2D.get_random_point(max_x=self.spawn_area, max_y=self.spawn_area) for i in range(self.n_targets)}\n",
    "        self.collectors = []\n",
    "        self.closest_targets = {}\n",
    "        self.info = {}\n",
    "        self.observation_cache = {agent: [] for agent in self.agents_ids}\n",
    "        return {agent: self.__get_observation(agent) for agent in self.agents_ids}, {}\n",
    "     \n",
    "    def step(self, actions):\n",
    "        self.steps += 1\n",
    "        observations, rewards, terminated, truncated, infos = {}, {}, {}, {}, {}\n",
    "\n",
    "        for agent, action in actions.items():\n",
    "            self.__update_agent_position(agent, self.__continuous_action(action))\n",
    "\n",
    "        self.__collect_items_and_compute_global_reward()\n",
    "\n",
    "        for agent, action in actions.items():\n",
    "            observations[agent] = self.__get_observation(agent)\n",
    "            rewards[agent] = self.__get_local_reward(agent, self.__continuous_action(action)) + self.__get_global_reward()\n",
    "            terminated[agent] = False\n",
    "            truncated[agent] = False\n",
    "            infos[agent] = self.info[agent]\n",
    "\n",
    "        truncated['__all__'] = False\n",
    "        if len(self.targets_pos.keys()) == 0:\n",
    "            terminated['__all__'] = True\n",
    "        elif self.max_steps != None and self.steps == self.max_steps:\n",
    "            terminated['__all__'] = True\n",
    "        else:\n",
    "            terminated['__all__'] = False\n",
    "\n",
    "        return observations, rewards, terminated, truncated, infos\n",
    "     \n",
    "    def rgb_to_hex(self, r, g, b):\n",
    "        return f'#{r:02x}{g:02x}{b:02x}'\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    def get_agent_ids(self):\n",
    "       return self.agents\n",
    "\n",
    "\n",
    "class RenderableCollectTheItems(CollectTheItems):\n",
    "    def __init__(self, config: EnvironmentConfiguration):\n",
    "        super().__init__(config)\n",
    "        self.agent_colors = {agent: self.rgb_to_hex(rnd.randint(0, 255), rnd.randint(0, 255), rnd.randint(0, 255)) for agent in self.agents_ids}\n",
    "\n",
    "        self.unit = self.CANVAS_WIDTH/float(self.spawn_area)\n",
    "        self.render_size_agent = max(self.unit,1)\n",
    "        self.render_size_agent_range = self.unit*self.agent_range\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def __position_in_frame(self, position_in_env):\n",
    "        return [((self.spawn_area-position_in_env[0])/self.spawn_area)*self.CANVAS_WIDTH,\n",
    "                        ((self.spawn_area-position_in_env[1])/self.spawn_area)*self.CANVAS_HEIGHT,]\n",
    "\n",
    "    def render(self):\n",
    "        with hold_canvas():\n",
    "            if self.canvas == None:\n",
    "                self.canvas = CanvasWithBorders(width=self.CANVAS_WIDTH, height=self.CANVAS_HEIGHT)\n",
    "                display(self.canvas)\n",
    "\n",
    "            self.canvas.clear()\n",
    "\n",
    "            self.canvas.fill_style = \"red\"\n",
    "            for target in self.targets_pos.values():\n",
    "                self.canvas.draw_circle(pos=self.__position_in_frame(target.to_np_array()), \n",
    "                                        radius=1, \n",
    "                                        fill_color=\"red\")\n",
    "\n",
    "            for agent in self.agents_ids:\n",
    "                self.canvas.draw_circle(pos=self.__position_in_frame(self.agents_pos[agent].to_np_array()), \n",
    "                                        radius=self.render_size_agent/2.0, \n",
    "                                        fill_color=self.agent_colors[agent],\n",
    "                                        border_color=\"black\")\n",
    "\n",
    "                self.canvas.draw_circle(pos=self.__position_in_frame(self.agents_pos[agent].to_np_array()), \n",
    "                                        radius=self.render_size_agent_range, \n",
    "                                        border_color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### environment demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action space:  Discrete(125)\n",
      "observation space:  Dict('t[-0]': Dict('nbrs': Dict('nbr-0': Dict('direction': Box(-1.0, 1.0, (2, 1), float32), 'distance': Box(-inf, inf, (1, 1), float32)), 'nbr-1': Dict('direction': Box(-1.0, 1.0, (2, 1), float32), 'distance': Box(-inf, inf, (1, 1), float32)), 'nbr-2': Dict('direction': Box(-1.0, 1.0, (2, 1), float32), 'distance': Box(-inf, inf, (1, 1), float32))), 'targets': Dict('target-0': Dict('direction': Box(-1.0, 1.0, (2, 1), float32), 'distance': Box(-inf, inf, (1, 1), float32)), 'target-1': Dict('direction': Box(-1.0, 1.0, (2, 1), float32), 'distance': Box(-inf, inf, (1, 1), float32)), 'target-2': Dict('direction': Box(-1.0, 1.0, (2, 1), float32), 'distance': Box(-inf, inf, (1, 1), float32)))), 't[-1]': Dict('nbrs': Dict('nbr-0': Dict('direction': Box(-1.0, 1.0, (2, 1), float32), 'distance': Box(-inf, inf, (1, 1), float32)), 'nbr-1': Dict('direction': Box(-1.0, 1.0, (2, 1), float32), 'distance': Box(-inf, inf, (1, 1), float32)), 'nbr-2': Dict('direction': Box(-1.0, 1.0, (2, 1), float32), 'distance': Box(-inf, inf, (1, 1), float32))), 'targets': Dict('target-0': Dict('direction': Box(-1.0, 1.0, (2, 1), float32), 'distance': Box(-inf, inf, (1, 1), float32)), 'target-1': Dict('direction': Box(-1.0, 1.0, (2, 1), float32), 'distance': Box(-inf, inf, (1, 1), float32)), 'target-2': Dict('direction': Box(-1.0, 1.0, (2, 1), float32), 'distance': Box(-inf, inf, (1, 1), float32)))), 't[-2]': Dict('nbrs': Dict('nbr-0': Dict('direction': Box(-1.0, 1.0, (2, 1), float32), 'distance': Box(-inf, inf, (1, 1), float32)), 'nbr-1': Dict('direction': Box(-1.0, 1.0, (2, 1), float32), 'distance': Box(-inf, inf, (1, 1), float32)), 'nbr-2': Dict('direction': Box(-1.0, 1.0, (2, 1), float32), 'distance': Box(-inf, inf, (1, 1), float32))), 'targets': Dict('target-0': Dict('direction': Box(-1.0, 1.0, (2, 1), float32), 'distance': Box(-inf, inf, (1, 1), float32)), 'target-1': Dict('direction': Box(-1.0, 1.0, (2, 1), float32), 'distance': Box(-inf, inf, (1, 1), float32)), 'target-2': Dict('direction': Box(-1.0, 1.0, (2, 1), float32), 'distance': Box(-inf, inf, (1, 1), float32)))))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6111ccd8bbc94442a79d1ffbf984dc91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CanvasWithBorders(height=300, width=300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env_config = EnvironmentConfiguration(\n",
    "    n_agents = 4,\n",
    "    n_targets = 2,\n",
    "    spawn_area = 100,\n",
    "    max_steps=300,\n",
    "    agent_range = 5,\n",
    "    visible_nbrs = 3,\n",
    "    visible_targets = 3,\n",
    "    cache_size=3,\n",
    "    movement_granularity=5,\n",
    "    speed_granularity=5)\n",
    "\n",
    "env = RenderableCollectTheItems(env_config)\n",
    "\n",
    "print(\"action space: \", env.action_space)\n",
    "print(\"observation space: \", env.unflatten_observation_space(\"agent-0\"))\n",
    "\n",
    "simulate_random_episode(env, 100, sleep_between_frames=0.03, print_info=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preparing the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "\n",
    "env_config = EnvironmentConfiguration(\n",
    "    n_agents = 4,\n",
    "    n_targets = 10,\n",
    "    spawn_area = 100,\n",
    "    max_steps=300,\n",
    "    agent_range = 5,\n",
    "    visible_nbrs = 3,\n",
    "    visible_targets = 3,\n",
    "    cache_size=3,\n",
    "    movement_granularity=5,\n",
    "    speed_granularity=5)\n",
    "\n",
    "register_env(\"collect_the_items?algo=DQN&method=CTDE\", lambda _: RenderableCollectTheItems(env_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### restart ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration [1] => episode_reward_mean: 292.569663468884, episode_len_mean: 300.0\n",
      "iteration [2] => episode_reward_mean: 514.4395438856817, episode_len_mean: 300.0\n",
      "iteration [3] => episode_reward_mean: 1224.8661033384365, episode_len_mean: 300.0\n",
      "iteration [4] => episode_reward_mean: 1601.8209172025317, episode_len_mean: 300.0\n",
      "iteration [5] => episode_reward_mean: 1648.7731396200656, episode_len_mean: 300.0\n",
      "iteration [6] => episode_reward_mean: 1969.7434287282315, episode_len_mean: 295.75\n",
      "iteration [7] => episode_reward_mean: 2103.9601095128724, episode_len_mean: 296.30434782608694\n",
      "iteration [8] => episode_reward_mean: 2504.4256709625006, episode_len_mean: 282.92857142857144\n",
      "iteration [9] => episode_reward_mean: 2879.2079598458117, episode_len_mean: 254.28571428571428\n",
      "iteration [10] => episode_reward_mean: 3132.5300482558982, episode_len_mean: 236.54761904761904\n",
      "iteration [11] => episode_reward_mean: 3312.7992208001947, episode_len_mean: 218.64\n",
      "iteration [12] => episode_reward_mean: 3513.2518834505818, episode_len_mean: 199.43333333333334\n",
      "iteration [13] => episode_reward_mean: 3635.890501331556, episode_len_mean: 190.16176470588235\n",
      "iteration [14] => episode_reward_mean: 3755.225479355786, episode_len_mean: 180.97402597402598\n",
      "iteration [15] => episode_reward_mean: 3823.887748682943, episode_len_mean: 175.01176470588234\n",
      "iteration [16] => episode_reward_mean: 3876.056155237812, episode_len_mean: 168.23157894736843\n",
      "iteration [17] => episode_reward_mean: 4141.483002654775, episode_len_mean: 151.54\n",
      "iteration [18] => episode_reward_mean: 4336.56424462195, episode_len_mean: 134.59\n",
      "iteration [19] => episode_reward_mean: 4449.535663756007, episode_len_mean: 118.68\n",
      "iteration [20] => episode_reward_mean: 4478.234606007859, episode_len_mean: 111.76\n",
      "iteration [21] => episode_reward_mean: 4517.031606928501, episode_len_mean: 104.84\n",
      "iteration [22] => episode_reward_mean: 4501.522553845748, episode_len_mean: 105.79\n",
      "iteration [23] => episode_reward_mean: 4483.352252689531, episode_len_mean: 109.02\n",
      "iteration [24] => episode_reward_mean: 4477.5742901911835, episode_len_mean: 106.75\n",
      "iteration [25] => episode_reward_mean: 4463.204167449904, episode_len_mean: 106.14\n",
      "iteration [26] => episode_reward_mean: 4468.635936541425, episode_len_mean: 103.62\n",
      "iteration [27] => episode_reward_mean: 4456.434315841103, episode_len_mean: 104.49\n",
      "iteration [28] => episode_reward_mean: 4472.751875029354, episode_len_mean: 103.71\n",
      "iteration [29] => episode_reward_mean: 4482.05955041292, episode_len_mean: 103.39\n",
      "iteration [30] => episode_reward_mean: 4464.716127997774, episode_len_mean: 102.82\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f4c53daae24d488b327fe4857b5a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CanvasWithBorders(height=300, width=300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ray.rllib.algorithms.dqn.dqn import DQNConfig\n",
    "\n",
    "training_iterations = 30\n",
    "\n",
    "algo = (DQNConfig()\n",
    "        .training(\n",
    "            gamma=0.95,\n",
    "            lr=0.001,\n",
    "            train_batch_size=32,\n",
    "            n_step=1,\n",
    "            target_network_update_freq=500,\n",
    "            double_q=True,\n",
    "            dueling=True)\n",
    "        .env_runners(num_env_runners=1)\n",
    "        .resources(num_gpus=0)  \n",
    "        .environment(\"collect_the_items?algo=DQN&method=CTDE\")\n",
    "    ).build()\n",
    "\n",
    "clear_output()\n",
    "\n",
    "out = \"\"\n",
    "for i in range(training_iterations):\n",
    "    result = algo.train()\n",
    "    clear_output()\n",
    "    out += dqn_result_format(result) + \"\\n\"\n",
    "    print(out)\n",
    "    simulate_episode(RenderableCollectTheItems(env_config), algo, 500, sleep_between_frames=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An Algorithm checkpoint has been created inside directory: 'TrainingResult(checkpoint=Checkpoint(filesystem=local, path=/mnt/c/Users/nicol/Desktop/Universit√†/tesi/thesis-MARL_in_Aggregate_Computing/CTDE/algos/collect_the_items?algo=DQN&method=CTDE), metrics={'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'grad_gnorm': 4.8280158042907715, 'mean_q': 157.53335571289062, 'min_q': 38.10633087158203, 'max_q': 371.29168701171875, 'cur_lr': 0.001}, 'td_error': array([-1.7874939e+01, -9.1105263e+01, -6.3267975e+00,  8.7053680e+00,\n",
      "       -8.5085907e+01,  5.4938927e+00, -2.9582428e+01, -3.7129089e+01,\n",
      "       -7.5896912e+00, -1.1905212e+00, -9.2197113e+01, -3.8870682e+01,\n",
      "        5.7523041e+00,  2.1991730e+01, -3.7046227e+01,  2.6754852e+01,\n",
      "        2.2421265e+00,  9.1459351e+00,  6.8840714e+00, -3.3142090e-02,\n",
      "        1.9923599e+01,  1.0348709e+01,  1.6755657e+01,  4.3000183e+00,\n",
      "        9.7249298e+00,  2.6508865e+00, -6.5407593e+01, -2.1958405e+01,\n",
      "        9.9109192e+00,  2.0137985e+01,  1.0655914e+01, -8.4368484e+01],\n",
      "      dtype=float32), 'mean_td_error': -13.262104034423828, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 32.0, 'num_grad_updates_lifetime': 29000.0, 'diff_num_grad_updates_vs_sampler_policy': 28999.0}}, 'num_env_steps_sampled': 30000, 'num_env_steps_trained': 928000, 'num_agent_steps_sampled': 120000, 'num_agent_steps_trained': 928000, 'last_target_update_ts': 29501, 'num_target_updates': 58}, 'sampler_results': {'episode_reward_max': 4885.6792777105165, 'episode_reward_min': 3326.8182022875144, 'episode_reward_mean': 4464.716127997774, 'episode_len_mean': 102.82, 'episode_media': {}, 'episodes_this_iter': 10, 'episodes_timesteps_total': 10282, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [4371.139048112276, 4647.459588447587, 4341.651763329823, 4489.032745830773, 4684.628515182816, 4728.64092707628, 4366.5763882525025, 4778.707497397271, 4777.275610301914, 4434.00413055645, 4585.712772870368, 4453.187425415697, 4530.665798573628, 4566.8309314528615, 4076.4471681146424, 4602.851667228285, 4125.429935810194, 4630.761167550075, 4673.311792462489, 4418.279517333859, 4304.310614377563, 4532.741499550974, 4343.953479439146, 4282.511369606445, 4368.766852853708, 4686.929504478114, 3326.8182022875144, 4035.0772759968677, 4331.126270796121, 4551.267521439597, 4451.796624986866, 4616.829454870641, 4503.172824596826, 4724.93423545276, 4462.464827895799, 4666.516083817104, 4367.498222205582, 4484.277211762297, 4520.774957629879, 3925.6951455783437, 4289.129081431618, 4383.449117277514, 4510.137299140841, 4534.44143336124, 4405.173358291737, 4520.123495400791, 4412.905759834543, 4471.887562555516, 4457.794841289782, 4793.535710424853, 4450.235116610424, 4305.918366721859, 4124.422520600463, 4630.817002479366, 4734.526868108845, 4553.398688219829, 4420.857948201293, 4515.351072502304, 4072.2941210124986, 4215.95960084207, 4761.44376307436, 4483.404050574186, 4342.557573246748, 4531.824315917967, 4351.884041809604, 4489.09935160497, 4288.129906799681, 4436.790301301207, 4349.370995969521, 3730.6082581308306, 4637.284610859078, 4702.2923099266245, 4381.536614821169, 4636.175316021624, 4380.10999205179, 4477.976391686119, 4885.6792777105165, 4530.609470175225, 4449.070930674221, 4369.879446731486, 4808.191189852259, 4404.48064757349, 4749.22340934214, 4124.251308947615, 4738.901109786016, 4365.899024479627, 4627.563814799363, 4744.946769763423, 4718.57272643973, 4757.34205584477, 4474.7278525489, 4377.488899393125, 4468.896105273723, 4034.7338979510823, 4303.963670149615, 4457.949137591255, 4375.905595647575, 4870.409033010708, 4540.874119376026, 4739.149977690593], 'episode_lengths': [59, 144, 121, 84, 90, 88, 58, 95, 116, 59, 75, 73, 114, 133, 135, 68, 185, 91, 156, 95, 117, 124, 117, 55, 84, 137, 177, 135, 120, 200, 157, 74, 108, 100, 73, 100, 99, 70, 99, 163, 67, 51, 93, 173, 91, 131, 102, 160, 91, 105, 99, 88, 94, 113, 88, 66, 56, 97, 67, 76, 97, 124, 47, 124, 132, 71, 75, 72, 57, 214, 94, 129, 96, 100, 67, 73, 102, 104, 114, 97, 125, 56, 109, 134, 82, 109, 188, 86, 108, 90, 177, 77, 77, 104, 49, 63, 103, 110, 100, 90]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 2.08855443856198, 'mean_inference_ms': 1.7262376932184071, 'mean_action_processing_ms': 0.24761266134652993, 'mean_env_wait_ms': 1.94549102200804, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.013257980346679688, 'StateBufferConnector_ms': 0.012418031692504883, 'ViewRequirementAgentConnector_ms': 0.4386258125305176}, 'num_episodes': 10, 'episode_return_max': 4885.6792777105165, 'episode_return_min': 3326.8182022875144, 'episode_return_mean': 4464.716127997774}, 'env_runner_results': {'episode_reward_max': 4885.6792777105165, 'episode_reward_min': 3326.8182022875144, 'episode_reward_mean': 4464.716127997774, 'episode_len_mean': 102.82, 'episode_media': {}, 'episodes_this_iter': 10, 'episodes_timesteps_total': 10282, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [4371.139048112276, 4647.459588447587, 4341.651763329823, 4489.032745830773, 4684.628515182816, 4728.64092707628, 4366.5763882525025, 4778.707497397271, 4777.275610301914, 4434.00413055645, 4585.712772870368, 4453.187425415697, 4530.665798573628, 4566.8309314528615, 4076.4471681146424, 4602.851667228285, 4125.429935810194, 4630.761167550075, 4673.311792462489, 4418.279517333859, 4304.310614377563, 4532.741499550974, 4343.953479439146, 4282.511369606445, 4368.766852853708, 4686.929504478114, 3326.8182022875144, 4035.0772759968677, 4331.126270796121, 4551.267521439597, 4451.796624986866, 4616.829454870641, 4503.172824596826, 4724.93423545276, 4462.464827895799, 4666.516083817104, 4367.498222205582, 4484.277211762297, 4520.774957629879, 3925.6951455783437, 4289.129081431618, 4383.449117277514, 4510.137299140841, 4534.44143336124, 4405.173358291737, 4520.123495400791, 4412.905759834543, 4471.887562555516, 4457.794841289782, 4793.535710424853, 4450.235116610424, 4305.918366721859, 4124.422520600463, 4630.817002479366, 4734.526868108845, 4553.398688219829, 4420.857948201293, 4515.351072502304, 4072.2941210124986, 4215.95960084207, 4761.44376307436, 4483.404050574186, 4342.557573246748, 4531.824315917967, 4351.884041809604, 4489.09935160497, 4288.129906799681, 4436.790301301207, 4349.370995969521, 3730.6082581308306, 4637.284610859078, 4702.2923099266245, 4381.536614821169, 4636.175316021624, 4380.10999205179, 4477.976391686119, 4885.6792777105165, 4530.609470175225, 4449.070930674221, 4369.879446731486, 4808.191189852259, 4404.48064757349, 4749.22340934214, 4124.251308947615, 4738.901109786016, 4365.899024479627, 4627.563814799363, 4744.946769763423, 4718.57272643973, 4757.34205584477, 4474.7278525489, 4377.488899393125, 4468.896105273723, 4034.7338979510823, 4303.963670149615, 4457.949137591255, 4375.905595647575, 4870.409033010708, 4540.874119376026, 4739.149977690593], 'episode_lengths': [59, 144, 121, 84, 90, 88, 58, 95, 116, 59, 75, 73, 114, 133, 135, 68, 185, 91, 156, 95, 117, 124, 117, 55, 84, 137, 177, 135, 120, 200, 157, 74, 108, 100, 73, 100, 99, 70, 99, 163, 67, 51, 93, 173, 91, 131, 102, 160, 91, 105, 99, 88, 94, 113, 88, 66, 56, 97, 67, 76, 97, 124, 47, 124, 132, 71, 75, 72, 57, 214, 94, 129, 96, 100, 67, 73, 102, 104, 114, 97, 125, 56, 109, 134, 82, 109, 188, 86, 108, 90, 177, 77, 77, 104, 49, 63, 103, 110, 100, 90]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 2.08855443856198, 'mean_inference_ms': 1.7262376932184071, 'mean_action_processing_ms': 0.24761266134652993, 'mean_env_wait_ms': 1.94549102200804, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.013257980346679688, 'StateBufferConnector_ms': 0.012418031692504883, 'ViewRequirementAgentConnector_ms': 0.4386258125305176}, 'num_episodes': 10, 'episode_return_max': 4885.6792777105165, 'episode_return_min': 3326.8182022875144, 'episode_return_mean': 4464.716127997774}, 'episode_reward_max': 4885.6792777105165, 'episode_reward_min': 3326.8182022875144, 'episode_reward_mean': 4464.716127997774, 'episode_len_mean': 102.82, 'episodes_this_iter': 10, 'episodes_timesteps_total': 10282, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [4371.139048112276, 4647.459588447587, 4341.651763329823, 4489.032745830773, 4684.628515182816, 4728.64092707628, 4366.5763882525025, 4778.707497397271, 4777.275610301914, 4434.00413055645, 4585.712772870368, 4453.187425415697, 4530.665798573628, 4566.8309314528615, 4076.4471681146424, 4602.851667228285, 4125.429935810194, 4630.761167550075, 4673.311792462489, 4418.279517333859, 4304.310614377563, 4532.741499550974, 4343.953479439146, 4282.511369606445, 4368.766852853708, 4686.929504478114, 3326.8182022875144, 4035.0772759968677, 4331.126270796121, 4551.267521439597, 4451.796624986866, 4616.829454870641, 4503.172824596826, 4724.93423545276, 4462.464827895799, 4666.516083817104, 4367.498222205582, 4484.277211762297, 4520.774957629879, 3925.6951455783437, 4289.129081431618, 4383.449117277514, 4510.137299140841, 4534.44143336124, 4405.173358291737, 4520.123495400791, 4412.905759834543, 4471.887562555516, 4457.794841289782, 4793.535710424853, 4450.235116610424, 4305.918366721859, 4124.422520600463, 4630.817002479366, 4734.526868108845, 4553.398688219829, 4420.857948201293, 4515.351072502304, 4072.2941210124986, 4215.95960084207, 4761.44376307436, 4483.404050574186, 4342.557573246748, 4531.824315917967, 4351.884041809604, 4489.09935160497, 4288.129906799681, 4436.790301301207, 4349.370995969521, 3730.6082581308306, 4637.284610859078, 4702.2923099266245, 4381.536614821169, 4636.175316021624, 4380.10999205179, 4477.976391686119, 4885.6792777105165, 4530.609470175225, 4449.070930674221, 4369.879446731486, 4808.191189852259, 4404.48064757349, 4749.22340934214, 4124.251308947615, 4738.901109786016, 4365.899024479627, 4627.563814799363, 4744.946769763423, 4718.57272643973, 4757.34205584477, 4474.7278525489, 4377.488899393125, 4468.896105273723, 4034.7338979510823, 4303.963670149615, 4457.949137591255, 4375.905595647575, 4870.409033010708, 4540.874119376026, 4739.149977690593], 'episode_lengths': [59, 144, 121, 84, 90, 88, 58, 95, 116, 59, 75, 73, 114, 133, 135, 68, 185, 91, 156, 95, 117, 124, 117, 55, 84, 137, 177, 135, 120, 200, 157, 74, 108, 100, 73, 100, 99, 70, 99, 163, 67, 51, 93, 173, 91, 131, 102, 160, 91, 105, 99, 88, 94, 113, 88, 66, 56, 97, 67, 76, 97, 124, 47, 124, 132, 71, 75, 72, 57, 214, 94, 129, 96, 100, 67, 73, 102, 104, 114, 97, 125, 56, 109, 134, 82, 109, 188, 86, 108, 90, 177, 77, 77, 104, 49, 63, 103, 110, 100, 90]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 2.08855443856198, 'mean_inference_ms': 1.7262376932184071, 'mean_action_processing_ms': 0.24761266134652993, 'mean_env_wait_ms': 1.94549102200804, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.013257980346679688, 'StateBufferConnector_ms': 0.012418031692504883, 'ViewRequirementAgentConnector_ms': 0.4386258125305176}, 'num_episodes': 10, 'episode_return_max': 4885.6792777105165, 'episode_return_min': 3326.8182022875144, 'episode_return_mean': 4464.716127997774, 'num_healthy_workers': 1, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 120000, 'num_agent_steps_trained': 928000, 'num_env_steps_sampled': 30000, 'num_env_steps_trained': 928000, 'num_env_steps_sampled_this_iter': 1000, 'num_env_steps_trained_this_iter': 32000, 'num_env_steps_sampled_throughput_per_sec': 30.740293847407475, 'num_env_steps_trained_throughput_per_sec': 983.6894031170392, 'timesteps_total': 30000, 'num_env_steps_sampled_lifetime': 30000, 'num_agent_steps_sampled_lifetime': 120000, 'num_steps_trained_this_iter': 32000, 'agent_timesteps_total': 120000, 'timers': {'training_iteration_time_ms': 33311.696, 'restore_workers_time_ms': 0.008, 'training_step_time_ms': 30.989, 'sample_time_ms': 12.037, 'load_time_ms': 0.132, 'load_throughput': 242401.532, 'learn_time_ms': 6.882, 'learn_throughput': 4649.747, 'synch_weights_time_ms': 3.771}, 'counters': {'num_env_steps_sampled': 30000, 'num_env_steps_trained': 928000, 'num_agent_steps_sampled': 120000, 'num_agent_steps_trained': 928000, 'last_target_update_ts': 29501, 'num_target_updates': 58}, 'done': False, 'episodes_total': 231, 'training_iteration': 30, 'trial_id': 'default', 'date': '2024-07-04_10-57-24', 'timestamp': 1720083444, 'time_this_iter_s': 32.5365993976593, 'time_total_s': 1011.770566701889, 'pid': 7519, 'hostname': 'LAPTOP-9AD2MD1C', 'node_ip': '172.22.114.43', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'enable_rl_module_and_learner': False, 'enable_env_runner_and_connector_v2': False, 'env': 'collect_the_items?algo=DQN&method=CTDE', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'env_task_fn': None, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_env_runner': 1, 'validate_env_runners_after_construction': True, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'enable_connectors': True, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.95, 'lr': 0.001, 'grad_clip': 40.0, 'grad_clip_by': 'global_norm', 'train_batch_size': 32, 'train_batch_size_per_learner': None, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, 'explore': True, 'exploration_config': {'type': 'EpsilonGreedy', 'initial_epsilon': 1.0, 'final_epsilon': 0.02, 'epsilon_timesteps': 10000}, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x7fec789102c0>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': {'explore': False}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'always_attach_evaluation_results': True, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 1000, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_run_training_always_in_thread': False, '_evaluation_parallel_to_training_wo_thread': False, 'ignore_env_runner_failures': False, 'recreate_failed_env_runners': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30, 'env_runner_restore_timeout_s': 1800, '_model_config_dict': {}, '_rl_module_spec': None, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'disable_env_checking': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'epsilon': [(0, 1.0), (10000, 0.05)], 'target_network_update_freq': 500, 'num_steps_sampled_before_learning_starts': 1000, 'store_buffer_in_checkpoints': False, 'lr_schedule': None, 'adam_epsilon': 1e-08, 'tau': 1.0, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': True, 'hiddens': [256], 'double_q': True, 'n_step': 1, 'before_learn_on_batch': None, 'training_intensity': None, 'td_error_loss_fn': 'huber', 'categorical_distribution_temperature': 1.0, 'replay_buffer_config': {'type': <class 'ray.rllib.utils.replay_buffers.multi_agent_prioritized_replay_buffer.MultiAgentPrioritizedReplayBuffer'>, 'prioritized_replay': -1, 'capacity': 50000, 'prioritized_replay_alpha': 0.6, 'prioritized_replay_beta': 0.4, 'prioritized_replay_eps': 1e-06, 'replay_sequence_length': 1, 'worker_side_prioritization': False}, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 1}, 'time_since_restore': 1011.770566701889, 'iterations_since_restore': 30, 'perf': {'cpu_util_percent': 40.793749999999996, 'ram_util_percent': 92.02916666666668}})'.\n"
     ]
    }
   ],
   "source": [
    "save_algo(algo, \"collect_the_items?algo=DQN&method=CTDE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = load_algo(\"collect_the_items?algo=DQN&method=CTDE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c882c50d3a6442fd88f63c9cbc56460e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CanvasWithBorders(height=300, width=300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#new_config = env_config.__deepcopy__()\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#new_config.spawn_area = 500\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#new_config.n_targets = 20\u001b[39;00m\n\u001b[1;32m      4\u001b[0m env \u001b[38;5;241m=\u001b[39m RenderableCollectTheItems(env_config)\n\u001b[0;32m----> 5\u001b[0m \u001b[43msimulate_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msleep_between_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.03\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/utils/simulations.py:16\u001b[0m, in \u001b[0;36msimulate_episode\u001b[0;34m(env, policy, steps, sleep_between_frames, print_info, print_action, print_reward, print_ob)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#actions = {agent: np.array([rnd.random()*2-1, rnd.random()*2-1, 1.0], np.float32) for agent in obs.keys()}\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#actions = {agent: env.action_space.sample() for agent in obs.keys()}\u001b[39;00m\n\u001b[1;32m     15\u001b[0m obs, reward, terminated, _, infos \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(actions)\n\u001b[0;32m---> 16\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, sleep_between_frames \u001b[38;5;241m-\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m last_frame)))\n\u001b[1;32m     17\u001b[0m last_frame \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     18\u001b[0m env\u001b[38;5;241m.\u001b[39mrender()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#new_config = env_config.__deepcopy__()\n",
    "#new_config.spawn_area = 500\n",
    "#new_config.n_targets = 20\n",
    "env = RenderableCollectTheItems(env_config)\n",
    "simulate_episode(env, algo, steps=300, sleep_between_frames=0.03)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rayEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
