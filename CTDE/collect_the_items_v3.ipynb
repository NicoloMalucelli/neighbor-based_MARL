{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect the items - discrete actions\n",
    "\n",
    "in order to be able to use rllib's DQN is necessary to implement a new version of the environment in which actions are discrete instead of continuous. \n",
    "\n",
    "Still the agents' goal is the same as before: collecting all the items in the environment minimizing the number of steps by collaborating together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import and utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.vectors import Vector2D\n",
    "from utils.canvas import CanvasWithBorders\n",
    "from utils.algo_utils import (save_algo, load_algo)\n",
    "from utils.simulations import (simulate_episode, simulate_random_episode, ppo_result_format)\n",
    "\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from gymnasium.spaces import Discrete, Box, Dict, Tuple, MultiDiscrete\n",
    "from gymnasium.spaces.utils import flatten, flatten_space\n",
    "from IPython.display import clear_output\n",
    "from ipycanvas import Canvas, hold_canvas\n",
    "from typing import Set\n",
    "import random as rnd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvironmentConfiguration: \n",
    "    def __init__(self, n_agents, n_targets, agent_range, movement_granularity, speed_granularity, spawn_area=100, visible_nbrs=1, visible_targets=1, max_steps=None, cache_size=1):\n",
    "        # parameters that shouldn't affect the agents' behaviour\n",
    "        self.n_agents = n_agents\n",
    "        self.n_targets = n_targets\n",
    "        self.spawn_area = spawn_area\n",
    "        self.max_steps = max_steps\n",
    "        # parameters that affect the agents' behavious\n",
    "        self.agent_range = agent_range\n",
    "        # parameters that affect the observation space\n",
    "        self.visible_nbrs = visible_nbrs\n",
    "        self.visible_targets = visible_targets\n",
    "        self.cache_size = cache_size\n",
    "        # parameters that affect the action space\n",
    "        self.movement_granularity = movement_granularity\n",
    "        self.speed_granularity = speed_granularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollectTheItems(MultiAgentEnv):\n",
    "    canvas = None\n",
    "    CANVAS_WIDTH, CANVAS_HEIGHT = 300.0, 300.0\n",
    "\n",
    "    def __init__(self, config: EnvironmentConfiguration):\n",
    "        assert config.n_agents > config.visible_nbrs\n",
    "\n",
    "        self.n_agents = config.n_agents\n",
    "        self.n_targets = config.n_targets\n",
    "        self.spawn_area = config.spawn_area\n",
    "        self.max_steps = config.max_steps\n",
    "        self.agent_range = config.agent_range\n",
    "        self.visible_nbrs = config.visible_nbrs\n",
    "        self.visible_targets = config.visible_targets\n",
    "        self.cache_size = config.cache_size\n",
    "\n",
    "        self.agents_ids = ['agent-' + str(i) for i in range(self.n_agents)]\n",
    "        self.observation_space = self.observation_space('agent-0')\n",
    "        self.action_space = self.action_space('agent-0')\n",
    "\n",
    "    def unflatten_observation_space(self, agent):\n",
    "        direction = Box(low=-1, high=1, shape=(2,1), dtype=np.float32)\n",
    "        distance = Box(low=-np.inf, high=np.inf, shape=(1,1), dtype=np.float32)\n",
    "\n",
    "        nbrs = Dict({f\"nbr-{i}\": Dict({'direction': direction, 'distance': distance}) for i in range(self.visible_nbrs)})\n",
    "        targets = Dict({f\"target-{i}\": Dict({'direction': direction, 'distance': distance}) for i in range(self.visible_targets)})\n",
    "\n",
    "        time_t_obs = Dict({\"nbrs\": nbrs, \"targets\": targets})\n",
    "\n",
    "        return Dict({f\"t[-{t}]\": time_t_obs for t in range(0, self.cache_size)})\n",
    "\n",
    "    def observation_space(self, agent):\n",
    "        return flatten_space(self.unflatten_observation_space(agent))\n",
    "\n",
    "    def action_space(self, agent):\n",
    "        direction = Box(low=-1.0, high=1.0, shape=(2,1), dtype=np.float32)\n",
    "        speed = Box(0.0, 1.0, dtype=np.float32)\n",
    "        return flatten_space(Tuple([direction, speed]))\n",
    "    \n",
    "    def __get_time_t_observation(self, agent):\n",
    "        nbrs_distance_vectors = [Vector2D.distance_vector(self.agents_pos[agent], self.agents_pos[nbr])  \n",
    "                            for nbr in self.__get_n_closest_neighbours(agent, self.visible_nbrs)]\n",
    "\n",
    "        targets_distance_vectors = [Vector2D.distance_vector(self.agents_pos[agent], self.targets_pos[target])  \n",
    "                            for target in self.__get_n_closest_targets(agent, self.visible_targets)]\n",
    "\n",
    "        nbrs = {\n",
    "            f\"nbr-{i}\": {\n",
    "                \"direction\": Vector2D.unit_vector(nbrs_distance_vectors[i]).to_np_array(),\n",
    "                \"distance\": np.log(1 + Vector2D.norm(nbrs_distance_vectors[i])) #1 - np.exp(-alpha * x)\n",
    "            }\n",
    "            for i in range(len(nbrs_distance_vectors))\n",
    "        }\n",
    "    \n",
    "        targets = {\n",
    "            f\"target-{i}\": {\n",
    "                \"direction\": Vector2D.unit_vector(targets_distance_vectors[i]).to_np_array(),\n",
    "                \"distance\": np.log(1 + Vector2D.norm(targets_distance_vectors[i])) #1 - np.exp(-alpha * x)\n",
    "            }\n",
    "            for i in range(len(targets_distance_vectors))\n",
    "        }\n",
    "        \n",
    "        for i in range(len(targets_distance_vectors), self.visible_targets):\n",
    "            targets[f\"target-{i}\"] = {\n",
    "                \"direction\": np.array([0,0], dtype=np.int32),\n",
    "                \"distance\": -1 #1 - np.exp(-alpha * x)\n",
    "            }\n",
    "\n",
    "        obs = {\n",
    "            \"nbrs\": nbrs,\n",
    "            \"targets\": targets\n",
    "        }\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def __get_observation(self, agent):\n",
    "        if len(self.observation_cache[agent]) == 0:\n",
    "            self.observation_cache[agent] = [self.__get_time_t_observation(agent)]*self.cache_size\n",
    "        else:\n",
    "            self.observation_cache[agent] = [self.__get_time_t_observation(agent)] + self.observation_cache[agent]\n",
    "            self.observation_cache[agent].pop()\n",
    "\n",
    "        obs = {\n",
    "            f\"t[-{t}]\": self.observation_cache[agent][t]\n",
    "            for t in range(0, self.cache_size)\n",
    "        }\n",
    "\n",
    "        return flatten(self.unflatten_observation_space(agent), obs)\n",
    "\n",
    "    def rgb_to_hex(self, r, g, b):\n",
    "        return f'#{r:02x}{g:02x}{b:02x}'\n",
    "\n",
    "    def __get_local_reward(self, agent, action):\n",
    "        # reward_1: small bonus if the agent collects an item\n",
    "        reward_1 = +5 if agent in self.collectors else 0\n",
    "\n",
    "        # reward_2: malus if the agent collides with another agent \n",
    "        reward_2= sum([-2 if Vector2D.distance(self.agents_pos[agent], self.agents_pos[nbr]) < self.agent_range*2 else 0 for nbr in self.__get_other_agents(agent)])\n",
    "\n",
    "        # reward_3: -1 at each step\n",
    "        reward_3 = -1\n",
    "\n",
    "        # reward_4: positive reward if the agent moves toward the closest targets, negative otherwise\n",
    "        distance_diff = ([Vector2D.distance(self.agent_old_pos[agent], self.targets_pos[target]) -\n",
    "                    Vector2D.distance(self.agents_pos[agent], self.targets_pos[target])\n",
    "            for target in self.closest_targets[agent]])\n",
    "        \n",
    "        reward_4 = max(distance_diff) if len(distance_diff) > 0 else 0\n",
    "\n",
    "        self.info[agent] = {\"info\": {f\"r2: {reward_2}, r3: {reward_3}, r4: {reward_4}\"}}\n",
    "        return  reward_2 + reward_3 + reward_4*3\n",
    "\n",
    "    def __get_global_reward(self):\n",
    "        return self.global_reward * 100\n",
    "    \n",
    "    def __get_other_agents(self, agent):\n",
    "        return [other for other in self.agents_ids if other != agent]\n",
    "\n",
    "    def __get_n_closest_neighbours(self, agent, n=1):\n",
    "        distances = {other: Vector2D.distance(self.agents_pos[agent], self.agents_pos[other]) for other in self.__get_other_agents(agent)}\n",
    "        return [neighbour[0] for neighbour in sorted(list(distances.items()), key=lambda d: d[1])[:n]]\n",
    "        # return {neighbour[0]: neighbour[1] for neighbour in sorted(list(dst.items()), key=lambda d: d[0])[:n]}\n",
    "\n",
    "    def __get_n_closest_targets(self, agent, n=1):\n",
    "        n = min(n, len(self.targets_pos.keys()))\n",
    "        distances = {target: Vector2D.distance(self.agents_pos[agent], pos) for target, pos in self.targets_pos.items()}\n",
    "        self.closest_targets[agent] = [target[0] for target in sorted(list(distances.items()), key=lambda d: d[1])[:n]]\n",
    "        return self.closest_targets[agent]\n",
    "\n",
    "    def __update_agent_position(self, agent, action):\n",
    "        unit_movement = Vector2D(action[0], action[1])\n",
    "        self.agent_old_pos[agent] = self.agents_pos[agent]\n",
    "        self.agents_pos[agent] = Vector2D.sum(self.agents_pos[agent], Vector2D.mul(unit_movement, action[2]))\n",
    "\n",
    "    def __collect_items(self):\n",
    "        self.collectors = []\n",
    "        uncollected_targets = {}\n",
    "        for target, target_pos in self.targets_pos.items():\n",
    "            collected = False\n",
    "            for agent in self.agents_pos.values():\n",
    "                if Vector2D.distance(target_pos, agent) < self.agent_range:\n",
    "                    collected = True\n",
    "                    self.collectors.append(agent)\n",
    "            if not collected:\n",
    "                uncollected_targets[target] = target_pos\n",
    "        self.targets_pos = uncollected_targets\n",
    "\n",
    "    def __collect_items_and_compute_global_reward(self):\n",
    "        old_uncollected_items = len(self.targets_pos.keys())\n",
    "        self.__collect_items()\n",
    "        updated_uncollected_items = len(self.targets_pos.keys())\n",
    "        self.global_reward = old_uncollected_items - updated_uncollected_items\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.steps = 0\n",
    "        self.agents_pos = {agent: Vector2D.get_random_point(max_x=self.spawn_area, max_y=self.spawn_area) for agent in self.agents_ids}\n",
    "        self.agent_old_pos = dict(self.agents_pos)\n",
    "        self.targets_pos = {f\"target-{i}\": Vector2D.get_random_point(max_x=self.spawn_area, max_y=self.spawn_area) for i in range(self.n_targets)}\n",
    "        self.collectors = []\n",
    "        self.closest_targets = {}\n",
    "        self.info = {}\n",
    "        self.observation_cache = {agent: [] for agent in self.agents_ids}\n",
    "        return {agent: self.__get_observation(agent) for agent in self.agents_ids}, {}\n",
    "     \n",
    "    def step(self, actions):\n",
    "        self.steps += 1\n",
    "        observations, rewards, terminated, truncated, infos = {}, {}, {}, {}, {}\n",
    "\n",
    "        for agent, action in actions.items():\n",
    "            self.__update_agent_position(agent, action)\n",
    "\n",
    "        self.__collect_items_and_compute_global_reward()\n",
    "\n",
    "        for agent, action in actions.items():\n",
    "            observations[agent] = self.__get_observation(agent)\n",
    "            rewards[agent] = self.__get_local_reward(agent, action) + self.__get_global_reward()\n",
    "            terminated[agent] = False\n",
    "            truncated[agent] = False\n",
    "            infos[agent] = self.info[agent]\n",
    "\n",
    "        truncated['__all__'] = False\n",
    "        if len(self.targets_pos.keys()) == 0:\n",
    "            terminated['__all__'] = True\n",
    "        elif self.max_steps != None and self.steps == self.max_steps:\n",
    "            terminated['__all__'] = True\n",
    "        else:\n",
    "            terminated['__all__'] = False\n",
    "\n",
    "        return observations, rewards, terminated, truncated, infos\n",
    "     \n",
    "    def rgb_to_hex(self, r, g, b):\n",
    "        return f'#{r:02x}{g:02x}{b:02x}'\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    def get_agent_ids(self):\n",
    "       return self.agents\n",
    "\n",
    "\n",
    "class RenderableCollectTheItems(CollectTheItems):\n",
    "    def __init__(self, config: EnvironmentConfiguration):\n",
    "        super().__init__(config)\n",
    "        self.agent_colors = {agent: self.rgb_to_hex(rnd.randint(0, 255), rnd.randint(0, 255), rnd.randint(0, 255)) for agent in self.agents_ids}\n",
    "        self.canvas = CanvasWithBorders(width=self.CANVAS_WIDTH, height=self.CANVAS_HEIGHT)\n",
    "        display(self.canvas)\n",
    "\n",
    "        self.unit = self.CANVAS_WIDTH/float(self.spawn_area)\n",
    "        self.render_size_agent = max(self.unit,1)\n",
    "        self.render_size_agent_range = self.unit*self.agent_range\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def __position_in_frame(self, position_in_env):\n",
    "        return [((self.spawn_area-position_in_env[0])/self.spawn_area)*self.CANVAS_WIDTH,\n",
    "                        ((self.spawn_area-position_in_env[1])/self.spawn_area)*self.CANVAS_HEIGHT,]\n",
    "\n",
    "    def render(self):\n",
    "        with hold_canvas():\n",
    "            self.canvas.clear()\n",
    "\n",
    "            self.canvas.fill_style = \"red\"\n",
    "            for target in self.targets_pos.values():\n",
    "                self.canvas.draw_circle(pos=self.__position_in_frame(target.to_np_array()), \n",
    "                                        radius=1, \n",
    "                                        fill_color=\"red\")\n",
    "\n",
    "            for agent in self.agents_ids:\n",
    "                self.canvas.draw_circle(pos=self.__position_in_frame(self.agents_pos[agent].to_np_array()), \n",
    "                                        radius=self.render_size_agent/2.0, \n",
    "                                        fill_color=self.agent_colors[agent],\n",
    "                                        border_color=\"black\")\n",
    "\n",
    "                self.canvas.draw_circle(pos=self.__position_in_frame(self.agents_pos[agent].to_np_array()), \n",
    "                                        radius=self.render_size_agent_range, \n",
    "                                        border_color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### environment demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e15d0d0161b04d07b8b371adfb95e209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CanvasWithBorders(height=300, width=300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action space:  Box([-1. -1.  0.], 1.0, (3,), float32)\n",
      "observation space:  Box([ -1.  -1. -inf  -1.  -1. -inf  -1.  -1. -inf  -1.  -1. -inf  -1.  -1.\n",
      " -inf  -1.  -1. -inf  -1.  -1. -inf  -1.  -1. -inf  -1.  -1. -inf  -1.\n",
      "  -1. -inf  -1.  -1. -inf  -1.  -1. -inf  -1.  -1. -inf  -1.  -1. -inf\n",
      "  -1.  -1. -inf  -1.  -1. -inf  -1.  -1. -inf  -1.  -1. -inf], [ 1.  1. inf  1.  1. inf  1.  1. inf  1.  1. inf  1.  1. inf  1.  1. inf\n",
      "  1.  1. inf  1.  1. inf  1.  1. inf  1.  1. inf  1.  1. inf  1.  1. inf\n",
      "  1.  1. inf  1.  1. inf  1.  1. inf  1.  1. inf  1.  1. inf  1.  1. inf], (54,), float32)\n"
     ]
    }
   ],
   "source": [
    "env_config = EnvironmentConfiguration(\n",
    "    n_agents = 4,\n",
    "    n_targets = 2,\n",
    "    spawn_area = 100,\n",
    "    max_steps=300,\n",
    "    agent_range = 5,\n",
    "    visible_nbrs = 3,\n",
    "    visible_targets = 3,\n",
    "    cache_size=3,\n",
    "    movement_granularity=16,\n",
    "    speed_granularity=8)\n",
    "\n",
    "env = RenderableCollectTheItems(env_config)\n",
    "\n",
    "print(\"action space: \", env.action_space)\n",
    "print(\"observation space: \", env.observation_space)\n",
    "\n",
    "env.render()\n",
    "simulate_random_episode(env, 100, sleep_between_frames=0.03, print_info=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rayEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
