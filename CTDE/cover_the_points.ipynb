{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "import random as rnd\n",
    "from gymnasium.spaces import Discrete, Box, Dict\n",
    "from gymnasium.spaces.utils import flatten, flatten_space\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class PointCoverageEnv(MultiAgentEnv):\n",
    "\n",
    "    actions_dict = [(0,-1),(0,1),(1,0),(-1,0)]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.observations_memory = config[\"observations_memory\"] if \"observations_memory\" in config.keys() else 1\n",
    "        self.width = config[\"width\"]\n",
    "        self.height = config[\"height\"]\n",
    "        self.n_agents = config[\"n_agents\"]\n",
    "        self.n_targets = config[\"n_targets\"]\n",
    "        self.max_steps = config[\"max_steps\"] if \"max_steps\" in config.keys() else None\n",
    "        self.use_nested_observation = config[\"use_nested_observation\"] if \"use_nested_observation\" in config.keys() else False\n",
    "        self.agents = ['agent-' + str(i) for i in range(self.n_agents)]\n",
    "        self.observation_space = self.observation_space('agent-0')\n",
    "        self.action_space = Discrete(4)\n",
    "\n",
    "    def unflatten_observation_space(self, agent):\n",
    "        coordinates_space = Box(low=np.array([0.0, 0.0], dtype=np.float32), high=np.array([1.0, 1.0], dtype=np.float32), dtype=np.float32)\n",
    "        obs_space = {\"position\": coordinates_space,\n",
    "                     \"targets\": Dict({f\"target-{i}\": coordinates_space for i in range(self.n_targets)})}\n",
    "        if self.n_agents > 1:\n",
    "            obs_space = {\"position\": coordinates_space,\n",
    "                        \"other_agents\": Dict({f\"other_agent-{i}\": coordinates_space for i in range(self.n_agents-1)}),\n",
    "                        \"targets\": Dict({f\"target-{i}\": coordinates_space for i in range(self.n_targets)})}\n",
    "        \n",
    "        obs_space = Dict(obs_space)\n",
    "\n",
    "        if self.observations_memory > 1:\n",
    "            return Dict({f\"t(-{i})\": obs_space for i in range(self.observations_memory)})\n",
    "        return obs_space\n",
    "\n",
    "    def observation_space(self, agent):\n",
    "       if self.use_nested_observation:\n",
    "           return self.unflatten_observation_space(agent)\n",
    "       return flatten_space(self.unflatten_observation_space(agent))\n",
    "\n",
    "    def action_space(self, agent):\n",
    "        return Discrete(5)\n",
    "    \n",
    "    def __get_other_agents(self, agent):\n",
    "        return [other for other in self.agents if other != agent]\n",
    "\n",
    "    def __get_random_point(self):\n",
    "        return (rnd.randint(0, self.width-1), rnd.randint(0, self.height-1))\n",
    "    \n",
    "    def __get_normalized_position(self, position):\n",
    "        return (position[0]/self.width, position[1]/self.height)\n",
    "\n",
    "    def __get_unflatten_time_t_observation(self, agent):\n",
    "        time_t_obs = {\"position\": self.__get_normalized_position(self.agent_pos[agent]),\n",
    "               \"targets\": {f\"target-{i}\": self.__get_normalized_position(pos) for i, pos in enumerate(self.targets)}}\n",
    "        if self.n_agents > 1:\n",
    "            time_t_obs = {\"position\": self.__get_normalized_position(self.agent_pos[agent]),\n",
    "               \"other_agents\": {f\"other_agent-{i}\": self.__get_normalized_position(self.agent_pos[other]) for i, other in enumerate(self.__get_other_agents(agent))},\n",
    "               \"targets\": {f\"target-{i}\": self.__get_normalized_position(pos) for i, pos in enumerate(self.targets)}}\n",
    "        return time_t_obs\n",
    "\n",
    "    def __get_observation(self, agent):\n",
    "        time_t_obs = self.__get_unflatten_time_t_observation(agent)\n",
    "\n",
    "        obs = {}\n",
    "        if self.observations_memory > 1:\n",
    "            self.agents_memory[agent].pop(0)\n",
    "            self.agents_memory[agent].append(time_t_obs)\n",
    "            obs = {f\"t(-{i})\": self.agents_memory[agent][self.observations_memory-1-i] for i in range(self.observations_memory)}\n",
    "        else:\n",
    "            obs = time_t_obs\n",
    "\n",
    "        if self.use_nested_observation:\n",
    "            return obs\n",
    "        return flatten(self.unflatten_observation_space(agent), obs)\n",
    "\n",
    "    def __get_not_covered_targets(self):\n",
    "        return set(self.targets) - set(self.agent_pos.values())\n",
    "\n",
    "    def __is_target_contended(self, target):\n",
    "        return list(self.agent_pos.values()).count(target) > 1\n",
    "\n",
    "    def __get_reward(self, agent):\n",
    "        return -1 + self.__get_global_reward()\n",
    "        if self.agent_pos[agent] in self.targets:\n",
    "            if self.agent_pos[agent] in [pos[1] for pos in self.old_agent_pos if pos[0] != agent]:\n",
    "                return -1 # someone was already covering the target -> no +10 reward\n",
    "            if self.__is_target_contended(self.agent_pos[agent]):\n",
    "                return -2 # someone arrived at the target at the same time of me -> someone has to leave\n",
    "            return 10\n",
    "        else:\n",
    "            return -1\n",
    "    \n",
    "    def __get_global_reward(self):\n",
    "        return 0#(len(self.not_covered_target) - len(set(self.not_covered_target) - set(self.agent_pos.values())))*10\n",
    "    \n",
    "    def __update_agent_position(self, agent, x, y):\n",
    "        self.agent_pos[agent] = (max(min(self.agent_pos[agent][0] + x, self.width-1), 0),\n",
    "                                 max(min(self.agent_pos[agent][1] + y, self.height-1), 0))\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.agent_pos = {agent: self.__get_random_point() for agent in self.agents}\n",
    "        self.targets = [self.__get_random_point() for _ in range(self.n_targets)]\n",
    "        self.not_covered_target = self.targets.copy()\n",
    "        self.steps = 0;\n",
    "        self.agents_memory = {agent: [self.__get_unflatten_time_t_observation(agent)]*self.observations_memory for agent in self.agents}\n",
    "        return {agent: self.__get_observation(agent) for agent in self.agents}, {}\n",
    "     \n",
    "    def step(self, actions):\n",
    "        self.steps += 1\n",
    "        observations, rewards, terminated, truncated, infos = {}, {}, {}, {}, {}\n",
    "\n",
    "        self.old_agent_pos = self.agent_pos.copy()\n",
    "        for agent, action in actions.items():\n",
    "            self.__update_agent_position(agent, self.actions_dict[action][0], self.actions_dict[action][1])\n",
    "\n",
    "        for agent in actions.keys():\n",
    "            if not (self.agent_pos[agent] in self.targets and not self.__is_target_contended(self.agent_pos[agent])):\n",
    "                observations[agent] = self.__get_observation(agent)\n",
    "                rewards[agent] = self.__get_reward(agent)\n",
    "                terminated[agent] = False\n",
    "                truncated[agent] = False\n",
    "                infos[agent] = {}\n",
    "        \n",
    "        if self.max_steps != None and self.steps > self.max_steps:\n",
    "            truncated['__all__'] = True\n",
    "        else:\n",
    "            truncated['__all__'] = False\n",
    "\n",
    "        self.not_covered_target = list(set(self.not_covered_target) - set(self.agent_pos.values())) \n",
    "\n",
    "        terminated['__all__'] = len(self.__get_not_covered_targets()) == 0\n",
    "        return observations, rewards, terminated, truncated, infos\n",
    "     \n",
    "    def render(self, mode='text'):\n",
    "        str = '_' * (self.width+2) + '\\n'\n",
    "        for i in range(self.height):\n",
    "            str = str + \"|\"\n",
    "            for j in range(self.width):\n",
    "                if (j,i) in self.agent_pos.values() and (j,i) in self.targets:\n",
    "                    str = str + '*'\n",
    "                elif (j,i) in self.agent_pos.values():\n",
    "                    str = str + 'o'\n",
    "                elif (j,i) in self.targets:\n",
    "                    str = str + 'x'\n",
    "                else:\n",
    "                    str = str + ' '\n",
    "            str = str + '|\\n'\n",
    "        str = str + '‾' * (self.width+2)\n",
    "        print(str)\n",
    "\n",
    "    def get_agent_ids(self):\n",
    "       return self.agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent-0': array([0.3, 0.3, 0.4, 0.2, 0.2, 0.8, 0.2, 0.1, 0.3, 0.3, 0.4, 0.2, 0.2,\n",
      "       0.8, 0.2, 0.1], dtype=float32), 'agent-1': array([0.4, 0.2, 0.3, 0.3, 0.2, 0.8, 0.2, 0.1, 0.4, 0.2, 0.3, 0.3, 0.2,\n",
      "       0.8, 0.2, 0.1], dtype=float32)}\n",
      "____________\n",
      "|          |\n",
      "|  x       |\n",
      "|    o     |\n",
      "|   o      |\n",
      "|          |\n",
      "|          |\n",
      "|          |\n",
      "|          |\n",
      "|  x       |\n",
      "|          |\n",
      "‾‾‾‾‾‾‾‾‾‾‾‾\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "observations_memory = 2\n",
    "\n",
    "env = PointCoverageEnv({\"height\": 10, \"width\": 10, \"n_agents\": 2, \"n_targets\": 2, \"max_steps\": 100, \"use_nested_observation\": False, \"observations_memory\": observations_memory})\n",
    "obs, _ = env.reset() \n",
    "print(obs)\n",
    "#print(json.dumps(obs['agent-0'], indent=2))\n",
    "env.render()\n",
    "\n",
    "#obs, _, _, _, _ = env.step({'agent-0': 1, 'agent-1': 2})\n",
    "#print(json.dumps(obs['agent-0'], indent=2))\n",
    "\n",
    "#obs, _, _, _, _ = env.step({'agent-0': 1, 'agent-1': 2})\n",
    "#print(json.dumps(obs['agent-0'], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import ray\n",
    "\n",
    "def customResultPrint(result):\n",
    "    print(f\"iteration [{result['training_iteration']}] => \" +\n",
    "          f\"episode_reward_mean: {result['sampler_results']['episode_reward_mean']}, \" +\n",
    "          f\"episode_len_mean: {result['sampler_results']['episode_len_mean']}, \" +\n",
    "          f\"agent_steps_trained: {result['info']['num_agent_steps_trained']}, \" +\n",
    "          f\"env_steps_trained: {result['info']['num_env_steps_trained']}, \" + \n",
    "          f\"entropy: {result['info']['learner']['default_policy']['learner_stats']['entropy']}, \" +\n",
    "          f\"learning_rate: {result['info']['learner']['default_policy']['learner_stats']['cur_lr']}\")\n",
    "\n",
    "#ray.shutdown()\n",
    "#ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-20 10:57:53,771\tWARNING deprecation.py:50 -- DeprecationWarning: `WorkerSet(num_workers=... OR local_worker=...)` has been deprecated. Use `EnvRunnerGroup(num_env_runners=... AND local_env_runner=...)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of different environment steps: 61440\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[137], line 30\u001b[0m\n\u001b[1;32m     14\u001b[0m total_env_steps \u001b[38;5;241m=\u001b[39m trainings\u001b[38;5;241m*\u001b[39mtrain_batch_size\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber of different environment steps: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_env_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m algo \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     20\u001b[0m     \u001b[43mPPOConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m              \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m              \u001b[49m\u001b[43mkl_coeff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m              \u001b[49m\u001b[43mtrain_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m              \u001b[49m\u001b[43msgd_minibatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msgd_minibatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m              \u001b[49m\u001b[43mnum_sgd_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_sgd_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_runners\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_env_runners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresources\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvironment\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmy_env\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 30\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(trainings):\n\u001b[1;32m     34\u001b[0m     result \u001b[38;5;241m=\u001b[39m algo\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm_config.py:859\u001b[0m, in \u001b[0;36mAlgorithmConfig.build\u001b[0;34m(self, env, logger_creator, use_copy)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgo_class, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    857\u001b[0m     algo_class \u001b[38;5;241m=\u001b[39m get_trainable_cls(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgo_class)\n\u001b[0;32m--> 859\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgo_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_copy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:554\u001b[0m, in \u001b[0;36mAlgorithm.__init__\u001b[0;34m(self, config, env, logger_creator, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;66;03m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;66;03m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;66;03m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;66;03m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;66;03m# TODO: Don't dump sampler results into top-level.\u001b[39;00m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    551\u001b[0m     },\n\u001b[1;32m    552\u001b[0m }\n\u001b[0;32m--> 554\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/trainable/trainable.py:158\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, storage)\u001b[0m\n\u001b[1;32m    154\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStorageContext on the TRAINABLE:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mstorage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open_logfiles(stdout_file, stderr_file)\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m setup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m setup_time \u001b[38;5;241m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:640\u001b[0m, in \u001b[0;36mAlgorithm.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moff_policy_estimation_methods \u001b[38;5;241m=\u001b[39m ope_dict\n\u001b[1;32m    639\u001b[0m \u001b[38;5;66;03m# Create a set of env runner actors via a EnvRunnerGroup.\u001b[39;00m\n\u001b[0;32m--> 640\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers \u001b[38;5;241m=\u001b[39m \u001b[43mEnvRunnerGroup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_policy_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_policy_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_env_runners\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;66;03m# Ensure remote workers are initially in sync with the local worker.\u001b[39;00m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers\u001b[38;5;241m.\u001b[39msync_weights(inference_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py:169\u001b[0m, in \u001b[0;36mEnvRunnerGroup.__init__\u001b[0;34m(self, env_creator, validate_env, default_policy_class, config, num_env_runners, local_env_runner, logdir, _setup, num_workers, local_worker)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _setup:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_env_runners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_env_runners\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_env_runner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_env_runner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;66;03m# EnvRunnerGroup creation possibly fails, if some (remote) workers cannot\u001b[39;00m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;66;03m# be initialized properly (due to some errors in the EnvRunners's\u001b[39;00m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;66;03m# constructor).\u001b[39;00m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m RayActorError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;66;03m# In case of an actor (remote worker) init failure, the remote worker\u001b[39;00m\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;66;03m# may still exist and will be accessible, however, e.g. calling\u001b[39;00m\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;66;03m# its `sample.remote()` would result in strange \"property not found\"\u001b[39;00m\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;66;03m# errors.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py:239\u001b[0m, in \u001b[0;36mEnvRunnerGroup._setup\u001b[0;34m(self, validate_env, config, num_env_runners, local_env_runner)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ds_shards \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# Create a number of @ray.remote workers.\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_workers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_env_runners\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_env_runners_after_construction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# If num_workers > 0 and we don't have an env on the local worker,\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m# get the observation- and action spaces for each policy from\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# the first remote worker (which does have an env).\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    248\u001b[0m     local_env_runner\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker_manager\u001b[38;5;241m.\u001b[39mnum_actors() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39mobservation_space \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39maction_space)\n\u001b[1;32m    253\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py:748\u001b[0m, in \u001b[0;36mEnvRunnerGroup.add_workers\u001b[0;34m(self, num_workers, validate)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;66;03m# Validate here, whether all remote workers have been constructed properly\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;66;03m# and are \"up and running\". Establish initial states.\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate:\n\u001b[0;32m--> 748\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_worker_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeach_actor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_healthy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    751\u001b[0m         \u001b[38;5;66;03m# Simiply raise the error, which will get handled by the try-except\u001b[39;00m\n\u001b[1;32m    752\u001b[0m         \u001b[38;5;66;03m# clause around the _setup().\u001b[39;00m\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39mok:\n\u001b[1;32m    754\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m result\u001b[38;5;241m.\u001b[39mget()\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/utils/actor_manager.py:622\u001b[0m, in \u001b[0;36mFaultTolerantActorManager.foreach_actor\u001b[0;34m(self, func, healthy_only, remote_actor_ids, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[1;32m    616\u001b[0m remote_calls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_actors(\n\u001b[1;32m    617\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m    618\u001b[0m     remote_actor_ids\u001b[38;5;241m=\u001b[39mremote_actor_ids,\n\u001b[1;32m    619\u001b[0m )\n\u001b[1;32m    621\u001b[0m \u001b[38;5;66;03m# Collect remote request results (if available given timeout and/or errors).\u001b[39;00m\n\u001b[0;32m--> 622\u001b[0m _, remote_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_actor_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremote_actor_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremote_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmark_healthy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmark_healthy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m remote_results\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/utils/actor_manager.py:476\u001b[0m, in \u001b[0;36mFaultTolerantActorManager._fetch_result\u001b[0;34m(self, remote_actor_ids, remote_calls, tags, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m remote_calls:\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [], RemoteCallResults()\n\u001b[0;32m--> 476\u001b[0m ready, _ \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Make sure remote results are fetched locally in parallel.\u001b[39;49;00m\n\u001b[1;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfetch_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# Remote data should already be fetched to local object store at this point.\u001b[39;00m\n\u001b[1;32m    485\u001b[0m remote_results \u001b[38;5;241m=\u001b[39m RemoteCallResults()\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py:21\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauto_init_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     20\u001b[0m     auto_init_ray()\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py:2854\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(ray_waitables, num_returns, timeout, fetch_local)\u001b[0m\n\u001b[1;32m   2852\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeout \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m6\u001b[39m\n\u001b[1;32m   2853\u001b[0m timeout_milliseconds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m-> 2854\u001b[0m ready_ids, remaining_ids \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mray_waitables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_milliseconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_task_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfetch_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2861\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ready_ids, remaining_ids\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:3812\u001b[0m, in \u001b[0;36mray._raylet.CoreWorker.wait\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:571\u001b[0m, in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.tune.registry import register_env\n",
    "from gymnasium.wrappers.time_limit import TimeLimit\n",
    "\n",
    "observations_memory = 1\n",
    "register_env(\"my_env\", lambda _: PointCoverageEnv({\"height\": 10, \"width\": 10, \"n_agents\": 1, \"n_targets\": 1, \"max_steps\": 100, \"use_nested_observation\": False, \"observations_memory\": observations_memory}))\n",
    "\n",
    "train_batch_size = 2048\n",
    "sgd_minibatch_size = 256\n",
    "num_sgd_iter = 10\n",
    "trainings = 30\n",
    "\n",
    "total_env_steps = trainings*train_batch_size\n",
    "\n",
    "print(f\"number of different environment steps: {total_env_steps}\")\n",
    "\n",
    "\n",
    "algo = (\n",
    "    PPOConfig()\n",
    "    .training(gamma=0.99, \n",
    "              lr=0.001,\n",
    "              kl_coeff=0.5, \n",
    "              train_batch_size=train_batch_size, \n",
    "              sgd_minibatch_size=sgd_minibatch_size, \n",
    "              num_sgd_iter=num_sgd_iter)\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"my_env\")\n",
    "    .build()\n",
    ")\n",
    "\n",
    "for i in range(trainings):\n",
    "    result = algo.train()\n",
    "    customResultPrint(result)\n",
    "    if i % 5 == 0:\n",
    "        checkpoint_dir = algo.save().checkpoint.path\n",
    "        print(f\"Checkpoint saved in directory {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### memory = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-17 08:52:10,055\tWARNING deprecation.py:50 -- DeprecationWarning: `WorkerSet(num_workers=... OR local_worker=...)` has been deprecated. Use `EnvRunnerGroup(num_env_runners=... AND local_env_runner=...)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of different environment steps: 61440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-17 08:52:13,732\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration [1] => episode_reward_mean: -86.17391304347827, episode_len_mean: 86.43478260869566, agent_steps_trained: 2048, env_steps_trained: 2048, entropy: 1.6000442996621131, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmp5lv0z5ba\n",
      "iteration [2] => episode_reward_mean: -75.31481481481481, episode_len_mean: 75.70370370370371, agent_steps_trained: 4096, env_steps_trained: 4096, entropy: 1.5908732578158378, learning_rate: 0.0010000000000000002\n",
      "iteration [3] => episode_reward_mean: -76.29113924050633, episode_len_mean: 76.65822784810126, agent_steps_trained: 6144, env_steps_trained: 6144, entropy: 1.5680509522557258, learning_rate: 0.0010000000000000002\n",
      "iteration [4] => episode_reward_mean: -74.5, episode_len_mean: 74.91, agent_steps_trained: 8192, env_steps_trained: 8192, entropy: 1.5522387847304344, learning_rate: 0.0010000000000000002\n",
      "iteration [5] => episode_reward_mean: -68.86, episode_len_mean: 69.36, agent_steps_trained: 10240, env_steps_trained: 10240, entropy: 1.5177503615617751, learning_rate: 0.0010000000000000002\n",
      "iteration [6] => episode_reward_mean: -58.18, episode_len_mean: 58.86, agent_steps_trained: 12288, env_steps_trained: 12288, entropy: 1.5156457915902137, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmpqnrbfvar\n",
      "iteration [7] => episode_reward_mean: -43.15, episode_len_mean: 44.04, agent_steps_trained: 14336, env_steps_trained: 14336, entropy: 1.4703963965177536, learning_rate: 0.0010000000000000002\n",
      "iteration [8] => episode_reward_mean: -29.9, episode_len_mean: 30.9, agent_steps_trained: 16384, env_steps_trained: 16384, entropy: 1.4203141137957573, learning_rate: 0.0010000000000000002\n",
      "iteration [9] => episode_reward_mean: -18.805825242718445, episode_len_mean: 19.805825242718445, agent_steps_trained: 18432, env_steps_trained: 18432, entropy: 1.3312469080090523, learning_rate: 0.0010000000000000002\n",
      "iteration [10] => episode_reward_mean: -14.373134328358208, episode_len_mean: 15.373134328358208, agent_steps_trained: 20480, env_steps_trained: 20480, entropy: 1.2653407901525497, learning_rate: 0.0010000000000000002\n",
      "iteration [11] => episode_reward_mean: -11.406060606060606, episode_len_mean: 12.406060606060606, agent_steps_trained: 22528, env_steps_trained: 22528, entropy: 1.1287067592144013, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmpyyalivzf\n",
      "iteration [12] => episode_reward_mean: -9.26, episode_len_mean: 10.26, agent_steps_trained: 24576, env_steps_trained: 24576, entropy: 1.0199550978839398, learning_rate: 0.0010000000000000002\n",
      "iteration [13] => episode_reward_mean: -9.083743842364532, episode_len_mean: 10.083743842364532, agent_steps_trained: 26624, env_steps_trained: 26624, entropy: 0.9199140965938568, learning_rate: 0.0010000000000000002\n",
      "iteration [14] => episode_reward_mean: -7.682203389830509, episode_len_mean: 8.682203389830509, agent_steps_trained: 28672, env_steps_trained: 28672, entropy: 0.7803926788270473, learning_rate: 0.0010000000000000002\n",
      "iteration [15] => episode_reward_mean: -6.9921875, episode_len_mean: 7.9921875, agent_steps_trained: 30720, env_steps_trained: 30720, entropy: 0.6797437109053135, learning_rate: 0.0010000000000000002\n",
      "iteration [16] => episode_reward_mean: -6.638059701492537, episode_len_mean: 7.638059701492537, agent_steps_trained: 32768, env_steps_trained: 32768, entropy: 0.5933313466608524, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmp0pged8qb\n",
      "iteration [17] => episode_reward_mean: -6.498168498168498, episode_len_mean: 7.498168498168498, agent_steps_trained: 34816, env_steps_trained: 34816, entropy: 0.5466358903795481, learning_rate: 0.0010000000000000002\n",
      "iteration [18] => episode_reward_mean: -6.354838709677419, episode_len_mean: 7.354838709677419, agent_steps_trained: 36864, env_steps_trained: 36864, entropy: 0.47229198813438417, learning_rate: 0.0010000000000000002\n",
      "iteration [19] => episode_reward_mean: -5.888888888888889, episode_len_mean: 6.888888888888889, agent_steps_trained: 38912, env_steps_trained: 38912, entropy: 0.4191170774400234, learning_rate: 0.0010000000000000002\n",
      "iteration [20] => episode_reward_mean: -5.865771812080537, episode_len_mean: 6.865771812080537, agent_steps_trained: 40960, env_steps_trained: 40960, entropy: 0.4240105677396059, learning_rate: 0.0010000000000000002\n",
      "iteration [21] => episode_reward_mean: -5.803986710963455, episode_len_mean: 6.803986710963455, agent_steps_trained: 43008, env_steps_trained: 43008, entropy: 0.3924148317426443, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmpr1jn8dkx\n",
      "iteration [22] => episode_reward_mean: -5.4637223974763405, episode_len_mean: 6.4637223974763405, agent_steps_trained: 45056, env_steps_trained: 45056, entropy: 0.3684031777083874, learning_rate: 0.0010000000000000002\n",
      "iteration [23] => episode_reward_mean: -5.765676567656766, episode_len_mean: 6.765676567656766, agent_steps_trained: 47104, env_steps_trained: 47104, entropy: 0.35473692417144775, learning_rate: 0.0010000000000000002\n",
      "iteration [24] => episode_reward_mean: -5.764900662251655, episode_len_mean: 6.764900662251655, agent_steps_trained: 49152, env_steps_trained: 49152, entropy: 0.35000532269477846, learning_rate: 0.0010000000000000002\n",
      "iteration [25] => episode_reward_mean: -5.523809523809524, episode_len_mean: 6.523809523809524, agent_steps_trained: 51200, env_steps_trained: 51200, entropy: 0.35265416093170643, learning_rate: 0.0010000000000000002\n",
      "iteration [26] => episode_reward_mean: -5.611650485436893, episode_len_mean: 6.611650485436893, agent_steps_trained: 53248, env_steps_trained: 53248, entropy: 0.317489979788661, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmpoja03hme\n",
      "iteration [27] => episode_reward_mean: -5.373831775700935, episode_len_mean: 6.373831775700935, agent_steps_trained: 55296, env_steps_trained: 55296, entropy: 0.3263606283813715, learning_rate: 0.0010000000000000002\n",
      "iteration [28] => episode_reward_mean: -5.583333333333333, episode_len_mean: 6.583333333333333, agent_steps_trained: 57344, env_steps_trained: 57344, entropy: 0.3252231139689684, learning_rate: 0.0010000000000000002\n",
      "iteration [29] => episode_reward_mean: -5.6742671009771986, episode_len_mean: 6.6742671009771986, agent_steps_trained: 59392, env_steps_trained: 59392, entropy: 0.3212065491825342, learning_rate: 0.0010000000000000002\n",
      "iteration [30] => episode_reward_mean: -5.732673267326732, episode_len_mean: 6.732673267326732, agent_steps_trained: 61440, env_steps_trained: 61440, entropy: 0.3227567713707685, learning_rate: 0.0010000000000000002\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.tune.registry import register_env\n",
    "from gymnasium.wrappers.time_limit import TimeLimit\n",
    "\n",
    "observations_memory = 2\n",
    "register_env(\"my_env\", lambda _: PointCoverageEnv({\"height\": 10, \"width\": 10, \"n_agents\": 1, \"n_targets\": 1, \"max_steps\": 100, \"observations_memory\": observations_memory}))\n",
    "\n",
    "train_batch_size = 2048\n",
    "sgd_minibatch_size = 256\n",
    "num_sgd_iter = 10\n",
    "trainings = 30\n",
    "\n",
    "total_env_steps = trainings*train_batch_size\n",
    "\n",
    "print(f\"number of different environment steps: {total_env_steps}\")\n",
    "\n",
    "\n",
    "algo = (\n",
    "    PPOConfig()\n",
    "    .training(gamma=0.99, \n",
    "              lr=0.001,\n",
    "              kl_coeff=0.5, \n",
    "              train_batch_size=train_batch_size, \n",
    "              sgd_minibatch_size=sgd_minibatch_size, \n",
    "              num_sgd_iter=num_sgd_iter)\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"my_env\")\n",
    "    .build()\n",
    ")\n",
    "\n",
    "for i in range(trainings):\n",
    "    result = algo.train()\n",
    "    customResultPrint(result)\n",
    "    if i % 5 == 0:\n",
    "        checkpoint_dir = algo.save().checkpoint.path\n",
    "        print(f\"Checkpoint saved in directory {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### memory = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-20 08:27:35,673\tWARNING deprecation.py:50 -- DeprecationWarning: `WorkerSet(num_workers=... OR local_worker=...)` has been deprecated. Use `EnvRunnerGroup(num_env_runners=... AND local_env_runner=...)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of different environment steps: 61440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-20 08:27:45,404\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration [1] => episode_reward_mean: 32.32, episode_len_mean: 80.8, agent_steps_trained: 2048, env_steps_trained: 2048, entropy: 1.5994981303811073, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmpo1c36i6l\n",
      "iteration [2] => episode_reward_mean: 41.924528301886795, episode_len_mean: 77.11320754716981, agent_steps_trained: 4096, env_steps_trained: 4096, entropy: 1.5748886257410049, learning_rate: 0.0010000000000000002\n",
      "iteration [3] => episode_reward_mean: 55.19767441860465, episode_len_mean: 71.1046511627907, agent_steps_trained: 6144, env_steps_trained: 6144, entropy: 1.5543632209300995, learning_rate: 0.0010000000000000002\n",
      "iteration [4] => episode_reward_mean: 73.73, episode_len_mean: 57.25, agent_steps_trained: 8192, env_steps_trained: 8192, entropy: 1.5233711302280426, learning_rate: 0.0010000000000000002\n",
      "iteration [5] => episode_reward_mean: 93.93, episode_len_mean: 36.91, agent_steps_trained: 10240, env_steps_trained: 10240, entropy: 1.489880283176899, learning_rate: 0.0010000000000000002\n",
      "iteration [6] => episode_reward_mean: 99.99, episode_len_mean: 27.82, agent_steps_trained: 12288, env_steps_trained: 12288, entropy: 1.4228958562016487, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmp3sziqiv6\n",
      "iteration [7] => episode_reward_mean: 101.0, episode_len_mean: 19.074766355140188, agent_steps_trained: 14336, env_steps_trained: 14336, entropy: 1.3464582502841949, learning_rate: 0.0010000000000000002\n",
      "iteration [8] => episode_reward_mean: 101.0, episode_len_mean: 15.776923076923078, agent_steps_trained: 16384, env_steps_trained: 16384, entropy: 1.223842868208885, learning_rate: 0.0010000000000000002\n",
      "iteration [9] => episode_reward_mean: 101.0, episode_len_mean: 13.904761904761905, agent_steps_trained: 18432, env_steps_trained: 18432, entropy: 1.1435803756117822, learning_rate: 0.0010000000000000002\n",
      "iteration [10] => episode_reward_mean: 101.0, episode_len_mean: 11.544943820224718, agent_steps_trained: 20480, env_steps_trained: 20480, entropy: 1.0234788954257965, learning_rate: 0.0010000000000000002\n",
      "iteration [11] => episode_reward_mean: 101.0, episode_len_mean: 10.914893617021276, agent_steps_trained: 22528, env_steps_trained: 22528, entropy: 0.9501141496002674, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmpftn6mrr1\n",
      "iteration [12] => episode_reward_mean: 101.0, episode_len_mean: 8.86147186147186, agent_steps_trained: 24576, env_steps_trained: 24576, entropy: 0.8313995130360127, learning_rate: 0.0010000000000000002\n",
      "iteration [13] => episode_reward_mean: 101.0, episode_len_mean: 8.352459016393443, agent_steps_trained: 26624, env_steps_trained: 26624, entropy: 0.8007967613637448, learning_rate: 0.0010000000000000002\n",
      "iteration [14] => episode_reward_mean: 101.0, episode_len_mean: 8.261044176706827, agent_steps_trained: 28672, env_steps_trained: 28672, entropy: 0.7394637033343315, learning_rate: 0.0010000000000000002\n",
      "iteration [15] => episode_reward_mean: 101.0, episode_len_mean: 7.559259259259259, agent_steps_trained: 30720, env_steps_trained: 30720, entropy: 0.7208200797438622, learning_rate: 0.0010000000000000002\n",
      "iteration [16] => episode_reward_mean: 101.0, episode_len_mean: 8.357723577235772, agent_steps_trained: 32768, env_steps_trained: 32768, entropy: 0.6805790588259697, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmpu6dun6yx\n",
      "iteration [17] => episode_reward_mean: 101.0, episode_len_mean: 7.413043478260869, agent_steps_trained: 34816, env_steps_trained: 34816, entropy: 0.645844479650259, learning_rate: 0.0010000000000000002\n",
      "iteration [18] => episode_reward_mean: 101.0, episode_len_mean: 7.761363636363637, agent_steps_trained: 36864, env_steps_trained: 36864, entropy: 0.6525492556393147, learning_rate: 0.0010000000000000002\n",
      "iteration [19] => episode_reward_mean: 101.0, episode_len_mean: 7.062068965517241, agent_steps_trained: 38912, env_steps_trained: 38912, entropy: 0.6038753472268581, learning_rate: 0.0010000000000000002\n",
      "iteration [20] => episode_reward_mean: 101.0, episode_len_mean: 7.062068965517241, agent_steps_trained: 40960, env_steps_trained: 40960, entropy: 0.5778724998235703, learning_rate: 0.0010000000000000002\n",
      "iteration [21] => episode_reward_mean: 101.0, episode_len_mean: 7.215547703180212, agent_steps_trained: 43008, env_steps_trained: 43008, entropy: 0.5889144673943519, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmplxetmuow\n",
      "iteration [22] => episode_reward_mean: 101.0, episode_len_mean: 7.280141843971631, agent_steps_trained: 45056, env_steps_trained: 45056, entropy: 0.5741400502622127, learning_rate: 0.0010000000000000002\n",
      "iteration [23] => episode_reward_mean: 101.0, episode_len_mean: 6.918918918918919, agent_steps_trained: 47104, env_steps_trained: 47104, entropy: 0.5526967611163854, learning_rate: 0.0010000000000000002\n",
      "iteration [24] => episode_reward_mean: 101.0, episode_len_mean: 7.121527777777778, agent_steps_trained: 49152, env_steps_trained: 49152, entropy: 0.5421368520706892, learning_rate: 0.0010000000000000002\n",
      "iteration [25] => episode_reward_mean: 101.0, episode_len_mean: 7.020618556701031, agent_steps_trained: 51200, env_steps_trained: 51200, entropy: 0.5461992233991623, learning_rate: 0.0010000000000000002\n",
      "iteration [26] => episode_reward_mean: 101.0, episode_len_mean: 7.0446735395189, agent_steps_trained: 53248, env_steps_trained: 53248, entropy: 0.5456732869148254, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmp4xj5xjgi\n",
      "iteration [27] => episode_reward_mean: 101.0, episode_len_mean: 7.0899653979238755, agent_steps_trained: 55296, env_steps_trained: 55296, entropy: 0.5273935705423355, learning_rate: 0.0010000000000000002\n",
      "iteration [28] => episode_reward_mean: 101.0, episode_len_mean: 7.0446735395189, agent_steps_trained: 57344, env_steps_trained: 57344, entropy: 0.5259552497416735, learning_rate: 0.0010000000000000002\n",
      "iteration [29] => episode_reward_mean: 101.0, episode_len_mean: 7.375451263537906, agent_steps_trained: 59392, env_steps_trained: 59392, entropy: 0.5458237342536449, learning_rate: 0.0010000000000000002\n",
      "iteration [30] => episode_reward_mean: 101.0, episode_len_mean: 7.149825783972125, agent_steps_trained: 61440, env_steps_trained: 61440, entropy: 0.5146411173045635, learning_rate: 0.0010000000000000002\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.tune.registry import register_env\n",
    "from gymnasium.wrappers.time_limit import TimeLimit\n",
    "\n",
    "observations_memory = 3\n",
    "register_env(\"my_env\", lambda _: PointCoverageEnv({\"height\": 10, \"width\": 10, \"n_agents\": 1, \"n_targets\": 1, \"max_steps\": 100, \"observations_memory\": observations_memory}))\n",
    "\n",
    "train_batch_size = 2048\n",
    "sgd_minibatch_size = 256\n",
    "num_sgd_iter = 10\n",
    "trainings = 30\n",
    "\n",
    "total_env_steps = trainings*train_batch_size\n",
    "\n",
    "print(f\"number of different environment steps: {total_env_steps}\")\n",
    "\n",
    "\n",
    "algo = (\n",
    "    PPOConfig()\n",
    "    .training(gamma=0.99, \n",
    "              lr=0.001,\n",
    "              kl_coeff=0.5, \n",
    "              train_batch_size=train_batch_size, \n",
    "              sgd_minibatch_size=sgd_minibatch_size, \n",
    "              num_sgd_iter=num_sgd_iter)\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"my_env\")\n",
    "    .build()\n",
    ")\n",
    "\n",
    "for i in range(trainings):\n",
    "    result = algo.train()\n",
    "    customResultPrint(result)\n",
    "    if i % 5 == 0:\n",
    "        checkpoint_dir = algo.save().checkpoint.path\n",
    "        print(f\"Checkpoint saved in directory {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[99]\n",
      "______________________________________________________________________________________________________\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                   x|\n",
      "|                                                                                                   o|\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾\n",
      "{'agent-0': array([0.99, 0.8 , 0.99, 0.75], dtype=float32)}\n",
      "{'agent-0': 0}\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "import torch\n",
    "from gymnasium.spaces.utils import flatten\n",
    "\n",
    "env = PointCoverageEnv({\"height\": 20, \"width\": 100, \"n_agents\": 1, \"n_targets\": 1, \"observations_memory\": observations_memory})\n",
    "obs_space = env.observation_space\n",
    "obs, _ = env.reset()\n",
    "print(obs)\n",
    "env.render()\n",
    "\n",
    "for i in range(100):\n",
    "    actions = algo.compute_actions({agent: o for agent, o in obs.items()})\n",
    "    print(actions, \"\\n\")\n",
    "    \n",
    "    obs, reward, terminated, truncated, info = env.step(actions)\n",
    "    clear_output()\n",
    "    print(f\"[{i}]\")\n",
    "    env.render()\n",
    "    print(obs)\n",
    "    print(reward)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    if terminated['__all__'] or truncated['__all__']:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-20 10:23:28,170\tWARNING deprecation.py:50 -- DeprecationWarning: `WorkerSet(num_workers=... OR local_worker=...)` has been deprecated. Use `EnvRunnerGroup(num_env_runners=... AND local_env_runner=...)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of different environment steps: 245760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-20 10:23:31,165\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration [1] => episode_reward_mean: -133.48235294117646, episode_len_mean: 95.31764705882352, agent_steps_trained: 12282, env_steps_trained: 8192, entropy: 1.597996175116387, learning_rate: 0.0005000000000000001\n",
      "Checkpoint saved in directory /tmp/tmp5yoe4j_1\n",
      "iteration [2] => episode_reward_mean: -118.35, episode_len_mean: 88.41, agent_steps_trained: 24373, env_steps_trained: 16384, entropy: 1.5717069007348323, learning_rate: 0.0005000000000000001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 34\u001b[0m\n\u001b[1;32m     19\u001b[0m algo2 \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     20\u001b[0m     PPOConfig()\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;241m.\u001b[39mtraining(gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;241m.\u001b[39mbuild()\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(trainings):\n\u001b[0;32m---> 34\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43malgo2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     customResultPrint(result)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/trainable/trainable.py:328\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    326\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    330\u001b[0m     skipped \u001b[38;5;241m=\u001b[39m skip_exceptions(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:873\u001b[0m, in \u001b[0;36mAlgorithm.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    863\u001b[0m     (\n\u001b[1;32m    864\u001b[0m         train_results,\n\u001b[1;32m    865\u001b[0m         eval_results,\n\u001b[1;32m    866\u001b[0m         train_iter_ctx,\n\u001b[1;32m    867\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_one_training_iteration_and_evaluation_in_parallel()\n\u001b[1;32m    869\u001b[0m \u001b[38;5;66;03m# - No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;66;03m# - We have to evaluate in this training iteration, but no parallelism ->\u001b[39;00m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;66;03m#   evaluate after the training iteration is entirely done.\u001b[39;00m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 873\u001b[0m     train_results, train_iter_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_one_training_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# Sequential: Train (already done above), then evaluate.\u001b[39;00m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluate_this_iter \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mevaluation_parallel_to_training:\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:3156\u001b[0m, in \u001b[0;36mAlgorithm._run_one_training_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3154\u001b[0m             \u001b[38;5;66;03m# Try to train one step.\u001b[39;00m\n\u001b[1;32m   3155\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timers[TRAINING_STEP_TIMER]:\n\u001b[0;32m-> 3156\u001b[0m                 results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results, train_iter_ctx\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/ppo/ppo.py:428\u001b[0m, in \u001b[0;36mPPO.training_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_training_step_new_api_stack()\n\u001b[1;32m    425\u001b[0m \u001b[38;5;66;03m# Old and hybrid API stacks (Policy, RolloutWorker, Connector, maybe RLModule,\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;66;03m# maybe Learner).\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 428\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_training_step_old_and_hybrid_api_stacks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/ppo/ppo.py:562\u001b[0m, in \u001b[0;36mPPO._training_step_old_and_hybrid_api_stacks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    557\u001b[0m     train_batch \u001b[38;5;241m=\u001b[39m synchronous_parallel_sample(\n\u001b[1;32m    558\u001b[0m         worker_set\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers,\n\u001b[1;32m    559\u001b[0m         max_agent_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtotal_train_batch_size,\n\u001b[1;32m    560\u001b[0m     )\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 562\u001b[0m     train_batch \u001b[38;5;241m=\u001b[39m \u001b[43msynchronous_parallel_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworker_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_env_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtotal_train_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m train_batch \u001b[38;5;241m=\u001b[39m train_batch\u001b[38;5;241m.\u001b[39mas_multi_agent()\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_counters[NUM_AGENT_STEPS_SAMPLED] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_batch\u001b[38;5;241m.\u001b[39magent_steps()\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/execution/rollout_ops.py:97\u001b[0m, in \u001b[0;36msynchronous_parallel_sample\u001b[0;34m(worker_set, max_agent_steps, max_env_steps, concat, sample_timeout_s, _uses_new_env_runners, _return_metrics)\u001b[0m\n\u001b[1;32m     94\u001b[0m         stats_dicts \u001b[38;5;241m=\u001b[39m [worker_set\u001b[38;5;241m.\u001b[39mlocal_worker()\u001b[38;5;241m.\u001b[39mget_metrics()]\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Loop over remote workers' `sample()` method in parallel.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     sampled_data \u001b[38;5;241m=\u001b[39m \u001b[43mworker_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeach_worker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_return_metrics\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_timeout_s\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# Nothing was returned (maybe all workers are stalling) or no healthy\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# remote workers left: Break.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# There is no point staying in this loop, since we will not be able to\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;66;03m# get any new samples if we don't have any healthy remote workers left.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sampled_data \u001b[38;5;129;01mor\u001b[39;00m worker_set\u001b[38;5;241m.\u001b[39mnum_healthy_remote_workers() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py:840\u001b[0m, in \u001b[0;36mEnvRunnerGroup.foreach_worker\u001b[0;34m(self, func, local_worker, healthy_only, remote_worker_ids, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker_manager\u001b[38;5;241m.\u001b[39mactor_ids():\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m local_result\n\u001b[0;32m--> 840\u001b[0m remote_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_worker_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeach_actor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhealthy_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhealthy_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_actor_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremote_worker_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmark_healthy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmark_healthy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    849\u001b[0m _handle_remote_call_result_errors(\n\u001b[1;32m    850\u001b[0m     remote_results, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ignore_env_runner_failures\n\u001b[1;32m    851\u001b[0m )\n\u001b[1;32m    853\u001b[0m \u001b[38;5;66;03m# With application errors handled, return good results.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/utils/actor_manager.py:622\u001b[0m, in \u001b[0;36mFaultTolerantActorManager.foreach_actor\u001b[0;34m(self, func, healthy_only, remote_actor_ids, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[1;32m    616\u001b[0m remote_calls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_actors(\n\u001b[1;32m    617\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m    618\u001b[0m     remote_actor_ids\u001b[38;5;241m=\u001b[39mremote_actor_ids,\n\u001b[1;32m    619\u001b[0m )\n\u001b[1;32m    621\u001b[0m \u001b[38;5;66;03m# Collect remote request results (if available given timeout and/or errors).\u001b[39;00m\n\u001b[0;32m--> 622\u001b[0m _, remote_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_actor_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremote_actor_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremote_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmark_healthy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmark_healthy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m remote_results\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/utils/actor_manager.py:476\u001b[0m, in \u001b[0;36mFaultTolerantActorManager._fetch_result\u001b[0;34m(self, remote_actor_ids, remote_calls, tags, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m remote_calls:\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [], RemoteCallResults()\n\u001b[0;32m--> 476\u001b[0m ready, _ \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Make sure remote results are fetched locally in parallel.\u001b[39;49;00m\n\u001b[1;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfetch_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# Remote data should already be fetched to local object store at this point.\u001b[39;00m\n\u001b[1;32m    485\u001b[0m remote_results \u001b[38;5;241m=\u001b[39m RemoteCallResults()\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py:21\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauto_init_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     20\u001b[0m     auto_init_ray()\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py:2854\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(ray_waitables, num_returns, timeout, fetch_local)\u001b[0m\n\u001b[1;32m   2852\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeout \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m6\u001b[39m\n\u001b[1;32m   2853\u001b[0m timeout_milliseconds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m-> 2854\u001b[0m ready_ids, remaining_ids \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mray_waitables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_milliseconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_task_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfetch_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2861\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ready_ids, remaining_ids\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:3812\u001b[0m, in \u001b[0;36mray._raylet.CoreWorker.wait\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:571\u001b[0m, in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.tune.registry import register_env\n",
    "from gymnasium.wrappers.time_limit import TimeLimit\n",
    "\n",
    "observations_memory = 1\n",
    "register_env(\"my_env\", lambda _: PointCoverageEnv({\"height\": 10, \"width\": 10, \"n_agents\": 2, \"n_targets\": 2, \"max_steps\": 100, \"observations_memory\": observations_memory}))\n",
    "\n",
    "train_batch_size = 4096*2\n",
    "sgd_minibatch_size = 256*2\n",
    "num_sgd_iter = 30\n",
    "trainings = 30\n",
    "\n",
    "total_env_steps = trainings*train_batch_size\n",
    "\n",
    "print(f\"number of different environment steps: {total_env_steps}\")\n",
    "\n",
    "\n",
    "algo2 = (\n",
    "    PPOConfig()\n",
    "    .training(gamma=0.95, \n",
    "              lr=0.0005,\n",
    "              kl_coeff=0.2, \n",
    "              train_batch_size=train_batch_size, \n",
    "              sgd_minibatch_size=sgd_minibatch_size, \n",
    "              num_sgd_iter=num_sgd_iter)\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"my_env\")\n",
    "    .build()\n",
    ")\n",
    "\n",
    "for i in range(trainings):\n",
    "    result = algo2.train()\n",
    "    customResultPrint(result)\n",
    "    if i % 5 == 0:\n",
    "        checkpoint_dir = algo2.save().checkpoint.path\n",
    "        print(f\"Checkpoint saved in directory {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## memory = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-20 10:26:32,329\tWARNING deprecation.py:50 -- DeprecationWarning: `WorkerSet(num_workers=... OR local_worker=...)` has been deprecated. Use `EnvRunnerGroup(num_env_runners=... AND local_env_runner=...)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of different environment steps: 245760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-20 10:26:40,895\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration [1] => episode_reward_mean: -140.55172413793105, episode_len_mean: 93.36781609195403, agent_steps_trained: 12447, env_steps_trained: 8192, entropy: 1.3740812677476142, learning_rate: 0.0005000000000000001\n",
      "iteration [2] => episode_reward_mean: -121.89, episode_len_mean: 84.47, agent_steps_trained: 24352, env_steps_trained: 16384, entropy: 1.3502507567405702, learning_rate: 0.0005000000000000001\n",
      "iteration [3] => episode_reward_mean: -92.80672268907563, episode_len_mean: 68.78991596638656, agent_steps_trained: 35590, env_steps_trained: 24576, entropy: 1.310470469981905, learning_rate: 0.0005000000000000001\n",
      "iteration [4] => episode_reward_mean: -64.40718562874251, episode_len_mean: 49.07185628742515, agent_steps_trained: 46671, env_steps_trained: 32768, entropy: 1.2618905988950577, learning_rate: 0.0005000000000000001\n",
      "iteration [5] => episode_reward_mean: -34.43653250773994, episode_len_mean: 25.325077399380806, agent_steps_trained: 58438, env_steps_trained: 40960, entropy: 1.1549571165532777, learning_rate: 0.0005000000000000001\n",
      "iteration [6] => episode_reward_mean: -25.36046511627907, episode_len_mean: 19.088372093023256, agent_steps_trained: 70185, env_steps_trained: 49152, entropy: 1.0685895184675853, learning_rate: 0.0005000000000000001\n",
      "iteration [7] => episode_reward_mean: -20.76425855513308, episode_len_mean: 15.600760456273765, agent_steps_trained: 82132, env_steps_trained: 57344, entropy: 0.9668261378571608, learning_rate: 0.0005000000000000001\n",
      "iteration [8] => episode_reward_mean: -17.342532467532468, episode_len_mean: 13.293831168831169, agent_steps_trained: 94044, env_steps_trained: 65536, entropy: 0.8758561231951782, learning_rate: 0.0005000000000000001\n",
      "iteration [9] => episode_reward_mean: -15.108298171589311, episode_len_mean: 11.517580872011251, agent_steps_trained: 106199, env_steps_trained: 73728, entropy: 0.7912503918875818, learning_rate: 0.0005000000000000001\n",
      "iteration [10] => episode_reward_mean: -13.58898847631242, episode_len_mean: 10.489116517285531, agent_steps_trained: 118366, env_steps_trained: 81920, entropy: 0.7083923002084096, learning_rate: 0.0005000000000000001\n",
      "iteration [11] => episode_reward_mean: -12.554524361948955, episode_len_mean: 9.504640371229698, agent_steps_trained: 130909, env_steps_trained: 90112, entropy: 0.6194141601522763, learning_rate: 0.0005000000000000001\n",
      "iteration [12] => episode_reward_mean: -11.986254295532646, episode_len_mean: 9.38602520045819, agent_steps_trained: 143107, env_steps_trained: 98304, entropy: 0.5922278611556343, learning_rate: 0.0005000000000000001\n",
      "iteration [13] => episode_reward_mean: -11.425601750547045, episode_len_mean: 8.959518599562363, agent_steps_trained: 155371, env_steps_trained: 106496, entropy: 0.5570092031921166, learning_rate: 0.0005000000000000001\n",
      "iteration [14] => episode_reward_mean: -10.633165829145728, episode_len_mean: 8.240201005025126, agent_steps_trained: 167917, env_steps_trained: 114688, entropy: 0.48782419541643723, learning_rate: 0.0005000000000000001\n",
      "iteration [15] => episode_reward_mean: -10.797916666666667, episode_len_mean: 8.533333333333333, agent_steps_trained: 180190, env_steps_trained: 122880, entropy: 0.4744681895211123, learning_rate: 0.0005000000000000001\n",
      "iteration [16] => episode_reward_mean: -10.580284552845528, episode_len_mean: 8.323170731707316, agent_steps_trained: 192557, env_steps_trained: 131072, entropy: 0.43422386849092115, learning_rate: 0.0005000000000000001\n",
      "iteration [17] => episode_reward_mean: -10.132804757185331, episode_len_mean: 8.114965312190288, agent_steps_trained: 204800, env_steps_trained: 139264, entropy: 0.4196773044872975, learning_rate: 0.0005000000000000001\n",
      "iteration [18] => episode_reward_mean: -9.919923736892278, episode_len_mean: 7.8102955195424215, agent_steps_trained: 217290, env_steps_trained: 147456, entropy: 0.3923053199218379, learning_rate: 0.0005000000000000001\n",
      "iteration [19] => episode_reward_mean: -10.004916420845625, episode_len_mean: 8.052114060963618, agent_steps_trained: 229493, env_steps_trained: 155648, entropy: 0.3934313277835431, learning_rate: 0.0005000000000000001\n",
      "iteration [20] => episode_reward_mean: -9.88686481303931, episode_len_mean: 7.856184084372003, agent_steps_trained: 241875, env_steps_trained: 163840, entropy: 0.3783711567107174, learning_rate: 0.0005000000000000001\n",
      "iteration [21] => episode_reward_mean: -10.06511175898931, episode_len_mean: 7.961127308066083, agent_steps_trained: 254274, env_steps_trained: 172032, entropy: 0.37024876698851583, learning_rate: 0.0005000000000000001\n",
      "iteration [22] => episode_reward_mean: -9.847342995169083, episode_len_mean: 7.9169082125603865, agent_steps_trained: 266518, env_steps_trained: 180224, entropy: 0.36500745605731355, learning_rate: 0.0005000000000000001\n",
      "iteration [23] => episode_reward_mean: -10.319018404907975, episode_len_mean: 8.377300613496933, agent_steps_trained: 278555, env_steps_trained: 188416, entropy: 0.3786602818447611, learning_rate: 0.0005000000000000001\n",
      "iteration [24] => episode_reward_mean: -10.153617443012884, episode_len_mean: 8.121902874132804, agent_steps_trained: 290807, env_steps_trained: 196608, entropy: 0.37301733485166577, learning_rate: 0.0005000000000000001\n",
      "iteration [25] => episode_reward_mean: -9.717535545023697, episode_len_mean: 7.756398104265402, agent_steps_trained: 303174, env_steps_trained: 204800, entropy: 0.34665457403494254, learning_rate: 0.0005000000000000001\n",
      "iteration [26] => episode_reward_mean: -9.759276879162702, episode_len_mean: 7.797335870599429, agent_steps_trained: 315516, env_steps_trained: 212992, entropy: 0.34054915441407096, learning_rate: 0.0005000000000000001\n",
      "iteration [27] => episode_reward_mean: -9.791666666666666, episode_len_mean: 7.942829457364341, agent_steps_trained: 327666, env_steps_trained: 221184, entropy: 0.35178148936534276, learning_rate: 0.0005000000000000001\n",
      "iteration [28] => episode_reward_mean: -9.699905033238366, episode_len_mean: 7.773029439696106, agent_steps_trained: 339987, env_steps_trained: 229376, entropy: 0.34549212919341193, learning_rate: 0.0005000000000000001\n",
      "iteration [29] => episode_reward_mean: -10.186304128902316, episode_len_mean: 8.255790533736153, agent_steps_trained: 352064, env_steps_trained: 237568, entropy: 0.3611100999773412, learning_rate: 0.0005000000000000001\n",
      "iteration [30] => episode_reward_mean: -9.881944444444445, episode_len_mean: 8.124007936507937, agent_steps_trained: 364033, env_steps_trained: 245760, entropy: 0.36025375041408814, learning_rate: 0.0005000000000000001\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.tune.registry import register_env\n",
    "from gymnasium.wrappers.time_limit import TimeLimit\n",
    "\n",
    "observations_memory = 2\n",
    "register_env(\"my_env\", lambda _: PointCoverageEnv({\"height\": 10, \"width\": 10, \"n_agents\": 2, \"n_targets\": 2, \"max_steps\": 100, \"observations_memory\": observations_memory}))\n",
    "\n",
    "train_batch_size = 4096*2\n",
    "sgd_minibatch_size = 256*2\n",
    "num_sgd_iter = 30\n",
    "trainings = 30\n",
    "\n",
    "total_env_steps = trainings*train_batch_size\n",
    "\n",
    "print(f\"number of different environment steps: {total_env_steps}\")\n",
    "\n",
    "#def my_policy_mapping_fn(agent_id, episode):\n",
    "    # return \"agent-policy\"\n",
    "\n",
    "algo2 = (\n",
    "    PPOConfig()\n",
    "    .training(gamma=0.95, \n",
    "              lr=0.0005,\n",
    "              kl_coeff=0.2, \n",
    "              train_batch_size=train_batch_size, \n",
    "              sgd_minibatch_size=sgd_minibatch_size, \n",
    "              num_sgd_iter=num_sgd_iter)\n",
    "    .env_runners(num_env_runners=1)\n",
    "    #.multi_agent(policies={\"agent-policy\": (None, env.observation_space, env.action_space, {})},\n",
    "    #             policy_mapping_fn=my_policy_mapping_fn)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"my_env\")\n",
    "    .build()\n",
    ")\n",
    "\n",
    "#print(algo2.config.is_multi_agent())\n",
    "\n",
    "for i in range(trainings):\n",
    "    result = algo2.train()\n",
    "    customResultPrint(result)\n",
    "    #if i % 5 == 0:\n",
    "    #    checkpoint_dir = algo2.save().checkpoint.path\n",
    "    #    print(f\"Checkpoint saved in directory {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11]\n",
      "______________________\n",
      "|                    |\n",
      "|                   *|\n",
      "|                    |\n",
      "|                    |\n",
      "|                    |\n",
      "|                    |\n",
      "|                    |\n",
      "|                    |\n",
      "|                    |\n",
      "|                *   |\n",
      "|                    |\n",
      "|                    |\n",
      "|                    |\n",
      "|                    |\n",
      "|                    |\n",
      "|                    |\n",
      "|                    |\n",
      "|                    |\n",
      "|                    |\n",
      "|                    |\n",
      "‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "observations_memory = 2\n",
    "env = PointCoverageEnv({\"height\": 20, \"width\": 20, \"n_agents\": 2, \"n_targets\": 2, \"observations_memory\": observations_memory})\n",
    "obs, _ = env.reset()\n",
    "env.render()\n",
    "\n",
    "for i in range(100):\n",
    "    actions = algo2.compute_actions(obs)\n",
    "    print(actions, \"\\n\")\n",
    "    obs, reward, terminated, truncated, info = env.step(actions)\n",
    "    clear_output()\n",
    "    print(f\"[{i}]\")\n",
    "    env.render()\n",
    "    #print(obs)\n",
    "    print(reward)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    if terminated['__all__'] or truncated['__all__']:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-20 11:00:32,698\tWARNING deprecation.py:50 -- DeprecationWarning: `WorkerSet(num_workers=... OR local_worker=...)` has been deprecated. Use `EnvRunnerGroup(num_env_runners=... AND local_env_runner=...)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of different environment steps: 327680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-20 11:00:34,802\tINFO worker.py:1749 -- Started a local Ray instance.\n",
      "2024-05-20 11:00:45,881\tINFO trainable.py:161 -- Trainable.setup took 13.185 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2024-05-20 11:00:45,885\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration [1] => episode_reward_mean: -173.85882352941175, episode_len_mean: 95.82352941176471, agent_steps_trained: 15032, env_steps_trained: 8192, entropy: 1.3733490586280823, learning_rate: 0.0005000000000000002\n",
      "iteration [2] => episode_reward_mean: -161.99, episode_len_mean: 94.38, agent_steps_trained: 29244, env_steps_trained: 16384, entropy: 1.3558745746259335, learning_rate: 0.0005000000000000001\n",
      "iteration [3] => episode_reward_mean: -157.53, episode_len_mean: 89.84, agent_steps_trained: 43751, env_steps_trained: 24576, entropy: 1.3404492764245897, learning_rate: 0.0005000000000000002\n",
      "iteration [4] => episode_reward_mean: -125.77570093457943, episode_len_mean: 76.44859813084112, agent_steps_trained: 57459, env_steps_trained: 32768, entropy: 1.3144010079212678, learning_rate: 0.0005000000000000001\n",
      "iteration [5] => episode_reward_mean: -118.28571428571429, episode_len_mean: 73.08035714285714, agent_steps_trained: 70985, env_steps_trained: 40960, entropy: 1.300217405343667, learning_rate: 0.0005000000000000001\n",
      "iteration [6] => episode_reward_mean: -97.78571428571429, episode_len_mean: 65.44444444444444, agent_steps_trained: 83552, env_steps_trained: 49152, entropy: 1.2642853912380008, learning_rate: 0.0005000000000000001\n",
      "iteration [7] => episode_reward_mean: -86.17687074829932, episode_len_mean: 55.714285714285715, agent_steps_trained: 96621, env_steps_trained: 57344, entropy: 1.22758287080129, learning_rate: 0.0005000000000000001\n",
      "iteration [8] => episode_reward_mean: -67.06395348837209, episode_len_mean: 47.593023255813954, agent_steps_trained: 108642, env_steps_trained: 65536, entropy: 1.1798755049705505, learning_rate: 0.0005000000000000001\n",
      "iteration [9] => episode_reward_mean: -66.04838709677419, episode_len_mean: 43.973118279569896, agent_steps_trained: 121468, env_steps_trained: 73728, entropy: 1.1660680584907532, learning_rate: 0.0005000000000000001\n",
      "iteration [10] => episode_reward_mean: -53.82062780269058, episode_len_mean: 36.780269058295964, agent_steps_trained: 134111, env_steps_trained: 81920, entropy: 1.1294506683945655, learning_rate: 0.0005000000000000001\n",
      "iteration [11] => episode_reward_mean: -44.555160142348754, episode_len_mean: 29.185053380782918, agent_steps_trained: 147423, env_steps_trained: 90112, entropy: 1.0563346739762869, learning_rate: 0.0005000000000000001\n",
      "iteration [12] => episode_reward_mean: -41.04333333333334, episode_len_mean: 27.296666666666667, agent_steps_trained: 160624, env_steps_trained: 98304, entropy: 1.0303212689558665, learning_rate: 0.0005000000000000001\n",
      "iteration [13] => episode_reward_mean: -35.08888888888889, episode_len_mean: 22.619444444444444, agent_steps_trained: 174357, env_steps_trained: 106496, entropy: 0.9815968894041501, learning_rate: 0.0005000000000000001\n",
      "iteration [14] => episode_reward_mean: -31.2360097323601, episode_len_mean: 20.063260340632603, agent_steps_trained: 188347, env_steps_trained: 114688, entropy: 0.9391346697454099, learning_rate: 0.0005000000000000001\n",
      "iteration [15] => episode_reward_mean: -28.825, episode_len_mean: 18.625, agent_steps_trained: 202313, env_steps_trained: 122880, entropy: 0.889698819760923, learning_rate: 0.0005000000000000001\n",
      "iteration [16] => episode_reward_mean: -26.169014084507044, episode_len_mean: 16.438631790744466, agent_steps_trained: 216837, env_steps_trained: 131072, entropy: 0.8501120685111909, learning_rate: 0.0005000000000000002\n",
      "iteration [17] => episode_reward_mean: -24.166077738515902, episode_len_mean: 14.501766784452297, agent_steps_trained: 232151, env_steps_trained: 139264, entropy: 0.8042087583706297, learning_rate: 0.0005000000000000002\n",
      "iteration [18] => episode_reward_mean: -21.90066225165563, episode_len_mean: 13.56953642384106, agent_steps_trained: 247171, env_steps_trained: 147456, entropy: 0.7599891895535349, learning_rate: 0.0005000000000000002\n",
      "iteration [19] => episode_reward_mean: -22.263069139966273, episode_len_mean: 13.807757166947724, agent_steps_trained: 262131, env_steps_trained: 155648, entropy: 0.7519185308067278, learning_rate: 0.0005000000000000002\n",
      "iteration [20] => episode_reward_mean: -20.57121212121212, episode_len_mean: 12.398484848484848, agent_steps_trained: 277687, env_steps_trained: 163840, entropy: 0.6998647769954469, learning_rate: 0.0005000000000000002\n",
      "iteration [21] => episode_reward_mean: -20.17593984962406, episode_len_mean: 12.330827067669173, agent_steps_trained: 293044, env_steps_trained: 172032, entropy: 0.6753439705262239, learning_rate: 0.0005000000000000002\n",
      "iteration [22] => episode_reward_mean: -18.220744680851062, episode_len_mean: 10.892287234042554, agent_steps_trained: 308980, env_steps_trained: 180224, entropy: 0.626677607272261, learning_rate: 0.0005000000000000002\n",
      "iteration [23] => episode_reward_mean: -18.202479338842974, episode_len_mean: 11.294765840220386, agent_steps_trained: 324330, env_steps_trained: 188416, entropy: 0.6185753326991509, learning_rate: 0.0005000000000000002\n",
      "iteration [24] => episode_reward_mean: -17.541131105398456, episode_len_mean: 10.510282776349614, agent_steps_trained: 340308, env_steps_trained: 196608, entropy: 0.5703079536717425, learning_rate: 0.0005000000000000002\n",
      "iteration [25] => episode_reward_mean: -17.28369704749679, episode_len_mean: 10.525032092426187, agent_steps_trained: 356081, env_steps_trained: 204800, entropy: 0.5579331944717302, learning_rate: 0.0005000000000000002\n",
      "iteration [26] => episode_reward_mean: -17.401617250673855, episode_len_mean: 11.04177897574124, agent_steps_trained: 371188, env_steps_trained: 212992, entropy: 0.5624810807321263, learning_rate: 0.0005000000000000002\n",
      "iteration [27] => episode_reward_mean: -16.517835178351785, episode_len_mean: 10.079950799507994, agent_steps_trained: 387015, env_steps_trained: 221184, entropy: 0.5417046360505952, learning_rate: 0.0005000000000000002\n",
      "iteration [28] => episode_reward_mean: -16.253012048192772, episode_len_mean: 9.86987951807229, agent_steps_trained: 402971, env_steps_trained: 229376, entropy: 0.519673115463667, learning_rate: 0.0005000000000000002\n",
      "iteration [29] => episode_reward_mean: -16.33534378769602, episode_len_mean: 9.8854041013269, agent_steps_trained: 418957, env_steps_trained: 237568, entropy: 0.5125473991837552, learning_rate: 0.0005000000000000002\n",
      "iteration [30] => episode_reward_mean: -16.264812575574364, episode_len_mean: 9.896009673518742, agent_steps_trained: 434876, env_steps_trained: 245760, entropy: 0.49800399493786596, learning_rate: 0.0005000000000000002\n",
      "iteration [31] => episode_reward_mean: -16.03680981595092, episode_len_mean: 10.062576687116565, agent_steps_trained: 450347, env_steps_trained: 253952, entropy: 0.5053460363547008, learning_rate: 0.0005000000000000002\n",
      "iteration [32] => episode_reward_mean: -16.20298879202989, episode_len_mean: 10.20049813200498, agent_steps_trained: 465739, env_steps_trained: 262144, entropy: 0.49184300127956604, learning_rate: 0.0005000000000000002\n",
      "iteration [33] => episode_reward_mean: -15.776839565741858, episode_len_mean: 9.879372738238843, agent_steps_trained: 481288, env_steps_trained: 270336, entropy: 0.4882601131333245, learning_rate: 0.0005000000000000002\n",
      "iteration [34] => episode_reward_mean: -16.109876543209875, episode_len_mean: 10.11358024691358, agent_steps_trained: 496724, env_steps_trained: 278528, entropy: 0.48317304614517426, learning_rate: 0.0005000000000000002\n",
      "iteration [35] => episode_reward_mean: -15.291866028708133, episode_len_mean: 9.788277511961722, agent_steps_trained: 511994, env_steps_trained: 286720, entropy: 0.4795684683596951, learning_rate: 0.0005000000000000002\n",
      "iteration [36] => episode_reward_mean: -16.302354399008674, episode_len_mean: 10.156133828996282, agent_steps_trained: 527532, env_steps_trained: 294912, entropy: 0.4692450633313921, learning_rate: 0.0005000000000000002\n",
      "iteration [37] => episode_reward_mean: -15.641843971631205, episode_len_mean: 9.684397163120567, agent_steps_trained: 543268, env_steps_trained: 303104, entropy: 0.46364187134636775, learning_rate: 0.0005000000000000002\n",
      "iteration [38] => episode_reward_mean: -15.324766355140186, episode_len_mean: 9.572429906542055, agent_steps_trained: 558919, env_steps_trained: 311296, entropy: 0.46034537351793714, learning_rate: 0.0005000000000000002\n",
      "iteration [39] => episode_reward_mean: -15.052213393870602, episode_len_mean: 9.293984108967082, agent_steps_trained: 574790, env_steps_trained: 319488, entropy: 0.4343314301305347, learning_rate: 0.0005000000000000002\n",
      "iteration [40] => episode_reward_mean: -15.546967895362663, episode_len_mean: 9.74435196195006, agent_steps_trained: 590351, env_steps_trained: 327680, entropy: 0.4559255969855521, learning_rate: 0.0005000000000000002\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.tune.registry import register_env\n",
    "from gymnasium.wrappers.time_limit import TimeLimit\n",
    "\n",
    "observations_memory = 2\n",
    "register_env(\"my_env\", lambda _: PointCoverageEnv({\"height\": 10, \"width\": 10, \"n_agents\": 3, \"n_targets\": 3, \"max_steps\": 100, \"observations_memory\": observations_memory}))\n",
    "\n",
    "train_batch_size = 4096*2\n",
    "sgd_minibatch_size = 256*2\n",
    "num_sgd_iter = 30\n",
    "trainings = 40\n",
    "\n",
    "total_env_steps = trainings*train_batch_size\n",
    "\n",
    "print(f\"number of different environment steps: {total_env_steps}\")\n",
    "\n",
    "#def my_policy_mapping_fn(agent_id, episode):\n",
    "    # return \"agent-policy\"\n",
    "\n",
    "algo3 = (\n",
    "    PPOConfig()\n",
    "    .training(gamma=0.95, \n",
    "              lr=0.0005,\n",
    "              kl_coeff=0.2, \n",
    "              train_batch_size=train_batch_size, \n",
    "              sgd_minibatch_size=sgd_minibatch_size, \n",
    "              num_sgd_iter=num_sgd_iter)\n",
    "    .env_runners(num_env_runners=1)\n",
    "    #.multi_agent(policies={\"agent-policy\": (None, env.observation_space, env.action_space, {})},\n",
    "    #             policy_mapping_fn=my_policy_mapping_fn)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"my_env\")\n",
    "    .build()\n",
    ")\n",
    "\n",
    "#print(algo2.config.is_multi_agent())\n",
    "\n",
    "for i in range(trainings):\n",
    "    result = algo3.train()\n",
    "    customResultPrint(result)\n",
    "    #if i % 5 == 0:\n",
    "    #    checkpoint_dir = algo2.save().checkpoint.path\n",
    "    #    print(f\"Checkpoint saved in directory {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20]\n",
      "____________\n",
      "|          |\n",
      "|          |\n",
      "|          |\n",
      "|         *|\n",
      "|          |\n",
      "| *        |\n",
      "|          |\n",
      "|         *|\n",
      "|          |\n",
      "|          |\n",
      "‾‾‾‾‾‾‾‾‾‾‾‾\n",
      "{}\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "observations_memory = 2\n",
    "env = PointCoverageEnv({\"height\": 10, \"width\": 10, \"n_agents\": 3, \"n_targets\": 3, \"observations_memory\":observations_memory})\n",
    "obs, _ = env.reset()\n",
    "env.render()\n",
    "\n",
    "for i in range(100):\n",
    "    actions = algo2.compute_actions(obs)\n",
    "    print(actions, \"\\n\")\n",
    "    obs, reward, terminated, truncated, info = env.step(actions)\n",
    "    clear_output()\n",
    "    print(f\"[{i}]\")\n",
    "    env.render()\n",
    "    print(obs)\n",
    "    print(reward)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    if terminated['__all__'] or truncated['__all__']:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two agents, DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "ray.shutdown()\n",
    "#ray.init(object_store_memory=(10**9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-05-20 12:29:38</td></tr>\n",
       "<tr><td>Running for: </td><td>00:07:40.53        </td></tr>\n",
       "<tr><td>Memory:      </td><td>3.6/3.7 GiB        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 6.0/8 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  : ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
       "  ... 52 more trials not shown (5 PENDING, 47 ERROR)\n",
       "  Number of errored trials: 57<br>Table truncated to 20 rows (37 overflow)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                                                                                                             </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_my_env_ca129_00000</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-05-20_12-21-53_440403_856/artifacts/2024-05-20_12-21-58/DQN_2024-05-20_12-21-53/driver_artifacts/DQN_my_env_ca129_00000_0_double_q=True,dueling=True,gamma=0.9000,lr=0.0100,train_batch_size=128_2024-05-20_12-21-58/error.txt   </td></tr>\n",
       "<tr><td>DQN_my_env_ca129_00001</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-05-20_12-21-53_440403_856/artifacts/2024-05-20_12-21-58/DQN_2024-05-20_12-21-53/driver_artifacts/DQN_my_env_ca129_00001_1_double_q=False,dueling=True,gamma=0.9000,lr=0.0100,train_batch_size=128_2024-05-20_12-21-59/error.txt  </td></tr>\n",
       "<tr><td>DQN_my_env_ca129_00002</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-05-20_12-21-53_440403_856/artifacts/2024-05-20_12-21-58/DQN_2024-05-20_12-21-53/driver_artifacts/DQN_my_env_ca129_00002_2_double_q=True,dueling=False,gamma=0.9000,lr=0.0100,train_batch_size=128_2024-05-20_12-21-59/error.txt  </td></tr>\n",
       "<tr><td>DQN_my_env_ca129_00003</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-05-20_12-21-53_440403_856/artifacts/2024-05-20_12-21-58/DQN_2024-05-20_12-21-53/driver_artifacts/DQN_my_env_ca129_00003_3_double_q=False,dueling=False,gamma=0.9000,lr=0.0100,train_batch_size=128_2024-05-20_12-21-59/error.txt </td></tr>\n",
       "<tr><td>DQN_my_env_ca129_00004</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-05-20_12-21-53_440403_856/artifacts/2024-05-20_12-21-58/DQN_2024-05-20_12-21-53/driver_artifacts/DQN_my_env_ca129_00004_4_double_q=True,dueling=True,gamma=0.9500,lr=0.0100,train_batch_size=128_2024-05-20_12-21-59/error.txt   </td></tr>\n",
       "<tr><td>DQN_my_env_ca129_00005</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-05-20_12-21-53_440403_856/artifacts/2024-05-20_12-21-58/DQN_2024-05-20_12-21-53/driver_artifacts/DQN_my_env_ca129_00005_5_double_q=False,dueling=True,gamma=0.9500,lr=0.0100,train_batch_size=128_2024-05-20_12-21-59/error.txt  </td></tr>\n",
       "<tr><td>DQN_my_env_ca129_00006</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-05-20_12-21-53_440403_856/artifacts/2024-05-20_12-21-58/DQN_2024-05-20_12-21-53/driver_artifacts/DQN_my_env_ca129_00006_6_double_q=True,dueling=False,gamma=0.9500,lr=0.0100,train_batch_size=128_2024-05-20_12-21-59/error.txt  </td></tr>\n",
       "<tr><td>DQN_my_env_ca129_00007</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-05-20_12-21-53_440403_856/artifacts/2024-05-20_12-21-58/DQN_2024-05-20_12-21-53/driver_artifacts/DQN_my_env_ca129_00007_7_double_q=False,dueling=False,gamma=0.9500,lr=0.0100,train_batch_size=128_2024-05-20_12-21-59/error.txt </td></tr>\n",
       "<tr><td>DQN_my_env_ca129_00008</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-05-20_12-21-53_440403_856/artifacts/2024-05-20_12-21-58/DQN_2024-05-20_12-21-53/driver_artifacts/DQN_my_env_ca129_00008_8_double_q=True,dueling=True,gamma=0.9900,lr=0.0100,train_batch_size=128_2024-05-20_12-21-59/error.txt   </td></tr>\n",
       "<tr><td>DQN_my_env_ca129_00009</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-05-20_12-21-53_440403_856/artifacts/2024-05-20_12-21-58/DQN_2024-05-20_12-21-53/driver_artifacts/DQN_my_env_ca129_00009_9_double_q=False,dueling=True,gamma=0.9900,lr=0.0100,train_batch_size=128_2024-05-20_12-21-59/error.txt  </td></tr>\n",
       "<tr><td>DQN_my_env_ca129_00010</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-05-20_12-21-53_440403_856/artifacts/2024-05-20_12-21-58/DQN_2024-05-20_12-21-53/driver_artifacts/DQN_my_env_ca129_00010_10_double_q=True,dueling=False,gamma=0.9900,lr=0.0100,train_batch_size=128_2024-05-20_12-21-59/error.txt </td></tr>\n",
       "<tr><td>DQN_my_env_ca129_00011</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-05-20_12-21-53_440403_856/artifacts/2024-05-20_12-21-58/DQN_2024-05-20_12-21-53/driver_artifacts/DQN_my_env_ca129_00011_11_double_q=False,dueling=False,gamma=0.9900,lr=0.0100,train_batch_size=128_2024-05-20_12-21-59/error.txt</td></tr>\n",
       "<tr><td>DQN_my_env_ca129_00012</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-05-20_12-21-53_440403_856/artifacts/2024-05-20_12-21-58/DQN_2024-05-20_12-21-53/driver_artifacts/DQN_my_env_ca129_00012_12_double_q=True,dueling=True,gamma=0.9000,lr=0.0010,train_batch_size=128_2024-05-20_12-21-59/error.txt  </td></tr>\n",
       "<tr><td>DQN_my_env_ca129_00013</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-05-20_12-21-53_440403_856/artifacts/2024-05-20_12-21-58/DQN_2024-05-20_12-21-53/driver_artifacts/DQN_my_env_ca129_00013_13_double_q=False,dueling=True,gamma=0.9000,lr=0.0010,train_batch_size=128_2024-05-20_12-21-59/error.txt </td></tr>\n",
       "<tr><td>DQN_my_env_ca129_00014</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-05-20_12-21-53_440403_856/artifacts/2024-05-20_12-21-58/DQN_2024-05-20_12-21-53/driver_artifacts/DQN_my_env_ca129_00014_14_double_q=True,dueling=False,gamma=0.9000,lr=0.0010,train_batch_size=128_2024-05-20_12-21-59/error.txt </td></tr>\n",
       "<tr><td>DQN_my_env_ca129_00015</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-05-20_12-21-53_440403_856/artifacts/2024-05-20_12-21-58/DQN_2024-05-20_12-21-53/driver_artifacts/DQN_my_env_ca129_00015_15_double_q=False,dueling=False,gamma=0.9000,lr=0.0010,train_batch_size=128_2024-05-20_12-21-59/error.txt</td></tr>\n",
       "<tr><td>DQN_my_env_ca129_00016</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-05-20_12-21-53_440403_856/artifacts/2024-05-20_12-21-58/DQN_2024-05-20_12-21-53/driver_artifacts/DQN_my_env_ca129_00016_16_double_q=True,dueling=True,gamma=0.9500,lr=0.0010,train_batch_size=128_2024-05-20_12-21-59/error.txt  </td></tr>\n",
       "<tr><td>DQN_my_env_ca129_00017</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-05-20_12-21-53_440403_856/artifacts/2024-05-20_12-21-58/DQN_2024-05-20_12-21-53/driver_artifacts/DQN_my_env_ca129_00017_17_double_q=False,dueling=True,gamma=0.9500,lr=0.0010,train_batch_size=128_2024-05-20_12-21-59/error.txt </td></tr>\n",
       "<tr><td>DQN_my_env_ca129_00018</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-05-20_12-21-53_440403_856/artifacts/2024-05-20_12-21-58/DQN_2024-05-20_12-21-53/driver_artifacts/DQN_my_env_ca129_00018_18_double_q=True,dueling=False,gamma=0.9500,lr=0.0010,train_batch_size=128_2024-05-20_12-21-59/error.txt </td></tr>\n",
       "<tr><td>DQN_my_env_ca129_00019</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-05-20_12-21-53_440403_856/artifacts/2024-05-20_12-21-58/DQN_2024-05-20_12-21-53/driver_artifacts/DQN_my_env_ca129_00019_19_double_q=False,dueling=False,gamma=0.9500,lr=0.0010,train_batch_size=128_2024-05-20_12-21-59/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th><th>double_q  </th><th>dueling  </th><th style=\"text-align: right;\">  gamma</th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  train_batch_size</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_my_env_ca129_00055</td><td>PENDING </td><td>     </td><td>False     </td><td>False    </td><td style=\"text-align: right;\">   0.95</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">               256</td></tr>\n",
       "<tr><td>DQN_my_env_ca129_00057</td><td>PENDING </td><td>     </td><td>False     </td><td>True     </td><td style=\"text-align: right;\">   0.99</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">               256</td></tr>\n",
       "<tr><td>DQN_my_env_ca129_00059</td><td>PENDING </td><td>     </td><td>False     </td><td>False    </td><td style=\"text-align: right;\">   0.99</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">               256</td></tr>\n",
       "<tr><td>DQN_my_env_ca129_00060</td><td>PENDING </td><td>     </td><td>True      </td><td>True     </td><td style=\"text-align: right;\">   0.9 </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">               256</td></tr>\n",
       "<tr><td>DQN_my_env_ca129_00061</td><td>PENDING </td><td>     </td><td>False     </td><td>True     </td><td style=\"text-align: right;\">   0.9 </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">               256</td></tr>\n",
       "<tr><td>DQN_my_env_ca129_00062</td><td>PENDING </td><td>     </td><td>True      </td><td>False    </td><td style=\"text-align: right;\">   0.9 </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">               256</td></tr>\n",
       "<tr><td>DQN_my_env_ca129_00063</td><td>PENDING </td><td>     </td><td>False     </td><td>False    </td><td style=\"text-align: right;\">   0.9 </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">               256</td></tr>\n",
       "<tr><td>DQN_my_env_ca129_00064</td><td>PENDING </td><td>     </td><td>True      </td><td>True     </td><td style=\"text-align: right;\">   0.95</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">               256</td></tr>\n",
       "<tr><td>DQN_my_env_ca129_00065</td><td>PENDING </td><td>     </td><td>False     </td><td>True     </td><td style=\"text-align: right;\">   0.95</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">               256</td></tr>\n",
       "<tr><td>DQN_my_env_ca129_00066</td><td>PENDING </td><td>     </td><td>True      </td><td>False    </td><td style=\"text-align: right;\">   0.95</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">               256</td></tr>\n",
       "<tr><td>DQN_my_env_ca129_00000</td><td>ERROR   </td><td>     </td><td>True      </td><td>True     </td><td style=\"text-align: right;\">   0.9 </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">               128</td></tr>\n",
       "<tr><td>DQN_my_env_ca129_00001</td><td>ERROR   </td><td>     </td><td>False     </td><td>True     </td><td style=\"text-align: right;\">   0.9 </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">               128</td></tr>\n",
       "<tr><td>DQN_my_env_ca129_00002</td><td>ERROR   </td><td>     </td><td>True      </td><td>False    </td><td style=\"text-align: right;\">   0.9 </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">               128</td></tr>\n",
       "<tr><td>DQN_my_env_ca129_00003</td><td>ERROR   </td><td>     </td><td>False     </td><td>False    </td><td style=\"text-align: right;\">   0.9 </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">               128</td></tr>\n",
       "<tr><td>DQN_my_env_ca129_00004</td><td>ERROR   </td><td>     </td><td>True      </td><td>True     </td><td style=\"text-align: right;\">   0.95</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">               128</td></tr>\n",
       "<tr><td>DQN_my_env_ca129_00005</td><td>ERROR   </td><td>     </td><td>False     </td><td>True     </td><td style=\"text-align: right;\">   0.95</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">               128</td></tr>\n",
       "<tr><td>DQN_my_env_ca129_00006</td><td>ERROR   </td><td>     </td><td>True      </td><td>False    </td><td style=\"text-align: right;\">   0.95</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">               128</td></tr>\n",
       "<tr><td>DQN_my_env_ca129_00007</td><td>ERROR   </td><td>     </td><td>False     </td><td>False    </td><td style=\"text-align: right;\">   0.95</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">               128</td></tr>\n",
       "<tr><td>DQN_my_env_ca129_00008</td><td>ERROR   </td><td>     </td><td>True      </td><td>True     </td><td style=\"text-align: right;\">   0.99</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">               128</td></tr>\n",
       "<tr><td>DQN_my_env_ca129_00009</td><td>ERROR   </td><td>     </td><td>False     </td><td>True     </td><td style=\"text-align: right;\">   0.99</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">               128</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-20 12:22:02,373\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00005\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: fffffffffffffffff7c4a5057430e79fb755469901000000, name=DQN.__init__, pid=12552, memory used=0.04GB) was running was 3.62GB / 3.70GB (0.976848), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 15643c4f3a42d5a2d8e684bf29d1bc716ba1ef4527aba46ab7706d72) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-15643c4f3a42d5a2d8e684bf29d1bc716ba1ef4527aba46ab7706d72*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.42\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.36\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "349\t0.16\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11401\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11515\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "11658\t0.10\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:22:02,777\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00004\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffffb156df766bb74cbfb2bc11b501000000, name=DQN.__init__, pid=12551, memory used=0.05GB) was running was 3.64GB / 3.70GB (0.981767), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 192f78a9d09ea7cd9ea1f7eda4c683d7f70cc0030b64125a8386cae3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-192f78a9d09ea7cd9ea1f7eda4c683d7f70cc0030b64125a8386cae3*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.42\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.36\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "349\t0.15\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11401\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11658\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "11515\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:23:39,822\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffffee06c37547d6abb9ac9c4ec801000000, name=DQN.__init__, pid=12413, memory used=0.11GB) was running was 3.66GB / 3.70GB (0.989216), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: af65934e8ef7bdd8f900e64c42b0cfe950203349611439cca1b030df) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-af65934e8ef7bdd8f900e64c42b0cfe950203349611439cca1b030df*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.42\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.36\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "349\t0.15\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11401\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "12414\t0.11\tray::IDLE\n",
      "12413\t0.11\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff981bac298066d7330a605f4601000000 Worker ID: 94ac3c3d43a31140af4113acf7fcb810a8e59a24f994235471d8e42e Node ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a Worker IP address: 172.23.82.135 Worker port: 42089 Worker PID: 12416 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2024-05-20 12:23:39,747 E 12008 12008] (raylet) node_manager.cc:3002: 13 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a, IP: 172.23.82.135) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.23.82.135`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-05-20 12:23:39,747 E 12008 12008] (raylet) worker_pool.cc:549: Some workers of the worker process(12730) have not registered within the timeout. The process is still alive, probably it's hanging during start.\n",
      "2024-05-20 12:23:39,917\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00003\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: DQN\n",
      "\tactor_id: 981bac298066d7330a605f4601000000\n",
      "\tpid: 12416\n",
      "\tnamespace: bf3b2f4c-4ec3-4648-87da-595bb7e030f0\n",
      "\tip: 172.23.82.135\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "The actor never ran - it was cancelled before it started running.\n",
      "2024-05-20 12:23:39,927\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00001\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: DQN\n",
      "\tactor_id: 73a23c0bf571374e7b17de9901000000\n",
      "\tpid: 12414\n",
      "\tnamespace: bf3b2f4c-4ec3-4648-87da-595bb7e030f0\n",
      "\tip: 172.23.82.135\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "The actor never ran - it was cancelled before it started running.\n",
      "2024-05-20 12:23:40,266\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00002\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffff24cc3e670052280db1541ffb01000000, name=DQN.__init__, pid=12415, memory used=0.16GB) was running was 3.57GB / 3.70GB (0.964441), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 1efeed5201a30ca5730cfee1a75b9af351dc9237580e003f41dd9b00) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-1efeed5201a30ca5730cfee1a75b9af351dc9237580e003f41dd9b00*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.43\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.38\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "12415\t0.16\tray::IDLE\n",
      "349\t0.15\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11515\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "11401\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:23:44,196\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00006\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffff559ae3ac31429d1d3302e47a01000000, name=DQN.__init__, pid=12863, memory used=0.22GB) was running was 3.55GB / 3.70GB (0.959467), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 63faeadac14967ab8fc5ee24ea3a6733dae9c45bac511a9c31c11bcb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-63faeadac14967ab8fc5ee24ea3a6733dae9c45bac511a9c31c11bcb*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.35\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "12863\t0.22\tray::IDLE\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "349\t0.12\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11515\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "11401\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:23:44,640\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00013\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffff0f72826224b127a3d3da99b601000000, name=DQN.__init__, pid=13152, memory used=0.05GB) was running was 3.56GB / 3.70GB (0.960152), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a8cd745a931813b285e13de7ccd4b0295e4b1cae6f63062d9051e90a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-a8cd745a931813b285e13de7ccd4b0295e4b1cae6f63062d9051e90a*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.35\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "349\t0.12\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11515\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "11401\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11658\t0.10\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:23:44,980\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00007\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffffa7fcbaed0160010bb0831be001000000, name=DQN.__init__, pid=13143, memory used=0.05GB) was running was 3.57GB / 3.70GB (0.965108), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7ccde8dae804bb376b0f334314e51fc18f3bb10c30d5b136d26cef5f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-7ccde8dae804bb376b0f334314e51fc18f3bb10c30d5b136d26cef5f*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.35\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "349\t0.12\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11515\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "11401\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11658\t0.10\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:23:45,284\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00009\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffff9166d662c4858c35e661a3c901000000, name=DQN.__init__, pid=13145, memory used=0.06GB) was running was 3.58GB / 3.70GB (0.967182), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 98e619990d337ffc4792b6089216fde5dd00cbab79d51422b6010250) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-98e619990d337ffc4792b6089216fde5dd00cbab79d51422b6010250*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.35\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "349\t0.12\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11515\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "11401\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11658\t0.10\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff73a23c0bf571374e7b17de9901000000 Worker ID: 0a8fc328081b8029871a7e182fb6920d1637a0b6773659bcc8688c42 Node ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a Worker IP address: 172.23.82.135 Worker port: 46203 Worker PID: 12414 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffee41d1f0eea0a7d6d32693dd01000000 Worker ID: e28e23148134fd8e87059180d6bc116c74be42c967094566289d398e Node ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a Worker IP address: 172.23.82.135 Worker port: 44819 Worker PID: 13146 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2024-05-20 12:27:14,031 E 12008 12008] (raylet) node_manager.cc:3002: 13 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a, IP: 172.23.82.135) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.23.82.135`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-05-20 12:27:14,036 E 12008 12008] (raylet) worker_pool.cc:549: Some workers of the worker process(13389) have not registered within the timeout. The process is still alive, probably it's hanging during start.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2024-05-20 12:27:14,998\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00011\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffff3a3b378a4de54208e0de419201000000, name=DQN.__init__, pid=13147, memory used=0.11GB) was running was 3.68GB / 3.70GB (0.993545), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c4a93c60ed04d84f130d74aa4addc0376cb82f2c59dd6e2ccc6f9924) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-c4a93c60ed04d84f130d74aa4addc0376cb82f2c59dd6e2ccc6f9924*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.35\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "349\t0.12\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "13147\t0.11\tray::IDLE\n",
      "13146\t0.11\tray::IDLE\n",
      "13151\t0.11\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:27:15,006\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00008\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffffe132749c98e29a91bf0b30d101000000, name=DQN.__init__, pid=13144, memory used=0.15GB) was running was 3.59GB / 3.70GB (0.968051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d60b7137bf576054475be45b8e6b241f4aace8ab7dc1efa1c530b027) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-d60b7137bf576054475be45b8e6b241f4aace8ab7dc1efa1c530b027*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.39\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.35\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "13151\t0.15\tray::IDLE\n",
      "13144\t0.15\tray::IDLE\n",
      "349\t0.12\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11401\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:27:15,012\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00010\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: DQN\n",
      "\tactor_id: ee41d1f0eea0a7d6d32693dd01000000\n",
      "\tpid: 13146\n",
      "\tnamespace: bf3b2f4c-4ec3-4648-87da-595bb7e030f0\n",
      "\tip: 172.23.82.135\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "The actor never ran - it was cancelled before it started running.\n",
      "2024-05-20 12:27:15,137\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00012\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffff3a44153899fb061cfbaf6fe501000000, name=DQN.__init__, pid=13151, memory used=0.22GB) was running was 3.53GB / 3.70GB (0.95412), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2ff77efb1b2088de404144a517c3e9a1598887d774ce5d5136d9fb9e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-2ff77efb1b2088de404144a517c3e9a1598887d774ce5d5136d9fb9e*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.39\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.35\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "13151\t0.22\tray::IDLE\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "349\t0.12\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11401\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11515\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:27:17,299\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00014\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffff76516697365a82989aa30e5001000000, name=DQN.__init__, pid=13585, memory used=0.05GB) was running was 3.54GB / 3.70GB (0.956159), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 79018a95d2cb88001a1ba0bec43e1e8b33206ba9a66b6e76f26c4e4b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-79018a95d2cb88001a1ba0bec43e1e8b33206ba9a66b6e76f26c4e4b*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.34\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "349\t0.13\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11401\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11515\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "11658\t0.10\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:27:17,617\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00020\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffffa48e0e40ba667fef3c818d9a01000000, name=DQN.__init__, pid=13591, memory used=0.05GB) was running was 3.54GB / 3.70GB (0.955625), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 74b9c74a8cb1d082ce0ab0636b7b19374cbc7d0731d018f80db26c3c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-74b9c74a8cb1d082ce0ab0636b7b19374cbc7d0731d018f80db26c3c*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.34\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "349\t0.13\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11401\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11515\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "11658\t0.10\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:27:17,946\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00018\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffffdca463762a8aa5b8f33c7d2701000000, name=DQN.__init__, pid=13589, memory used=0.06GB) was running was 3.58GB / 3.70GB (0.967809), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8e8a09ca9307baa0163632923f5db2e06060681a4502c12a5ee3c293) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-8e8a09ca9307baa0163632923f5db2e06060681a4502c12a5ee3c293*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.34\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "349\t0.13\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11401\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11515\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "11658\t0.10\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: fffffffffffffffff7e76abbcea1234c653db2f201000000 Worker ID: 13965c09980b1db13464c0eaf739080d9e51be37137e66a5dc85b634 Node ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a Worker IP address: 172.23.82.135 Worker port: 33353 Worker PID: 13586 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-20 12:27:19,463\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00015\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: DQN\n",
      "\tactor_id: f7e76abbcea1234c653db2f201000000\n",
      "\tpid: 13586\n",
      "\tnamespace: bf3b2f4c-4ec3-4648-87da-595bb7e030f0\n",
      "\tip: 172.23.82.135\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "The actor never ran - it was cancelled before it started running.\n",
      "2024-05-20 12:27:19,843\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00021\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffffe6d9b19baf992b69d0d3b57201000000, name=DQN.__init__, pid=13592, memory used=0.11GB) was running was 3.66GB / 3.70GB (0.988529), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dbc6ca086f9777e6610d369addb92099a13d696b15fdbc90d48c733d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-dbc6ca086f9777e6610d369addb92099a13d696b15fdbc90d48c733d*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.34\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "349\t0.13\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11401\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11515\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "13590\t0.11\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:27:20,107\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00019\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffff2f55e8e625665b8ba2a7112201000000, name=DQN.__init__, pid=13590, memory used=0.13GB) was running was 3.64GB / 3.70GB (0.981561), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7c79df50abeebc9a6479a4778490a265e5d751db09820cab0136f417) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-7c79df50abeebc9a6479a4778490a265e5d751db09820cab0136f417*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.34\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "349\t0.13\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "13590\t0.13\tray::IDLE\n",
      "13588\t0.13\tray::IDLE\n",
      "13587\t0.13\tray::IDLE\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:27:20,396\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00016\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: fffffffffffffffff9a799e8213b13174313026a01000000, name=DQN.__init__, pid=13587, memory used=0.15GB) was running was 3.60GB / 3.70GB (0.971451), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 90fd4873bf30a4da48bd2725de1cc06a2deb722b9301e5ec2482ac29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-90fd4873bf30a4da48bd2725de1cc06a2deb722b9301e5ec2482ac29*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.34\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "13587\t0.15\tray::IDLE\n",
      "13588\t0.15\tray::IDLE\n",
      "349\t0.13\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11401\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:27:22,541\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00017\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffff9d6cb475c96d63072f92501301000000, name=DQN.__init__, pid=13588, memory used=0.22GB) was running was 3.52GB / 3.70GB (0.950884), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 486bd2f498a9a268012630cd9035851bf04e98c01bf54baa87f56f8d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-486bd2f498a9a268012630cd9035851bf04e98c01bf54baa87f56f8d*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.34\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "13588\t0.22\tray::IDLE\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "349\t0.14\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11401\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11515\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:27:23,670\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00022\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffff4d505ed902877a64bb29988301000000, name=DQN.__init__, pid=14010, memory used=0.06GB) was running was 3.62GB / 3.70GB (0.976785), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 9e40cb08776c296f9f7acb60edade3ab2bc7d595941adf7980b95b0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-9e40cb08776c296f9f7acb60edade3ab2bc7d595941adf7980b95b0c*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.34\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "349\t0.14\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11401\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11515\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "11658\t0.10\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:27:41,353\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00024\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: fffffffffffffffff734ad97d47a8b521863017001000000, name=DQN.__init__, pid=14014, memory used=0.11GB) was running was 3.67GB / 3.70GB (0.991063), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 549bf6bd83ebc3c4d498c468ba2e1a8d14f8344a87987d48fc01f329) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-549bf6bd83ebc3c4d498c468ba2e1a8d14f8344a87987d48fc01f329*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.34\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "349\t0.13\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11401\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11515\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "14017\t0.11\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffec6c290bae20cdb9589132a001000000 Worker ID: b9faf86994356a42e4c56b2c842a534daa93d4ea61807e6f910c2978 Node ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a Worker IP address: 172.23.82.135 Worker port: 34921 Worker PID: 14011 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-20 12:27:41,396\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00023\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: DQN\n",
      "\tactor_id: ec6c290bae20cdb9589132a001000000\n",
      "\tpid: 14011\n",
      "\tnamespace: bf3b2f4c-4ec3-4648-87da-595bb7e030f0\n",
      "\tip: 172.23.82.135\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "The actor never ran - it was cancelled before it started running.\n",
      "2024-05-20 12:27:42,199\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00027\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffff44e889908043d400047a123101000000, name=DQN.__init__, pid=14017, memory used=0.15GB) was running was 3.63GB / 3.70GB (0.978895), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df0be71f05350a80ce1a9940a92d4b707d0a6852fe12fca8b049a082) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-df0be71f05350a80ce1a9940a92d4b707d0a6852fe12fca8b049a082*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.34\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "14017\t0.15\tray::IDLE\n",
      "14016\t0.14\tray::IDLE\n",
      "349\t0.13\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11515\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:27:42,426\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00025\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffff5078adf70d971f60d84b047b01000000, name=DQN.__init__, pid=14015, memory used=0.12GB) was running was 3.56GB / 3.70GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 921eab7b1cd2e92c113b630b8b01fdc327e9417700519ef0fe0f8e16) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-921eab7b1cd2e92c113b630b8b01fdc327e9417700519ef0fe0f8e16*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.34\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "14016\t0.19\tray::IDLE\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "349\t0.13\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "14015\t0.12\tray::IDLE\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11515\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:27:43,372\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00028\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffff3ad404473c170f45f8511df901000000, name=DQN.__init__, pid=14256, memory used=0.10GB) was running was 3.55GB / 3.70GB (0.959019), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: de964205c374d76ff9198f492421a17b7c20d379440bd222003f89b2) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-de964205c374d76ff9198f492421a17b7c20d379440bd222003f89b2*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.34\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "14016\t0.20\tray::IDLE\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "349\t0.12\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11515\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "11401\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:27:43,688\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00026\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffffe1c1eff22dd5baace4c3941201000000, name=DQN.__init__, pid=14016, memory used=0.20GB) was running was 3.60GB / 3.70GB (0.971928), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: b0f996f109d56d47181aaa9cf7098962a69e77300dfd5dce53580a4e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-b0f996f109d56d47181aaa9cf7098962a69e77300dfd5dce53580a4e*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.34\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "14016\t0.20\tray::IDLE\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "349\t0.12\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11515\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "11401\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:27:45,303\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00029\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: DQN\n",
      "\tactor_id: 76e74455381610b0f458d85b01000000\n",
      "\tpid: 14303\n",
      "\tnamespace: bf3b2f4c-4ec3-4648-87da-595bb7e030f0\n",
      "\tip: 172.23.82.135\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "The actor never ran - it was cancelled before it started running.\n",
      "2024-05-20 12:27:46,052\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00030\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffff29547df5839de52e803c274301000000, name=DQN.__init__, pid=14404, memory used=0.15GB) was running was 3.63GB / 3.70GB (0.981249), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 173a08579d2a07aa25bc2d61ed665d6a359b7b8eb29f20ad2b62a136) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-173a08579d2a07aa25bc2d61ed665d6a359b7b8eb29f20ad2b62a136*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.34\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "14404\t0.15\tray::IDLE\n",
      "14408\t0.15\tray::IDLE\n",
      "349\t0.12\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11515\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:27:46,379\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00031\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffff5517e44e85304ced6da5943b01000000, name=DQN.__init__, pid=14408, memory used=0.21GB) was running was 3.56GB / 3.70GB (0.962451), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 80bb2f809948e4fbfdea4b647000da6ee57d86bdbed2a087ed84a070) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-80bb2f809948e4fbfdea4b647000da6ee57d86bdbed2a087ed84a070*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.34\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "14408\t0.21\tray::IDLE\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "349\t0.13\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11515\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "11401\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:27:47,554\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00036\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffffad8a7e7675d0ba52f3c13b4301000000, name=DQN.__init__, pid=14662, memory used=0.05GB) was running was 3.54GB / 3.70GB (0.955295), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2e35cc8a5ac86dd2a29f25ebcb2938551b367fde7244ba75fdfef9a4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-2e35cc8a5ac86dd2a29f25ebcb2938551b367fde7244ba75fdfef9a4*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.34\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "349\t0.14\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11515\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "11401\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11658\t0.10\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:29:16,950\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00035\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffffd0ced24c76c7c4480f5703b601000000, name=DQN.__init__, pid=14660, memory used=0.11GB) was running was 3.67GB / 3.70GB (0.991149), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e8f200e14b050e757932c19f9949be65b56b0912fe030081195ea95a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-e8f200e14b050e757932c19f9949be65b56b0912fe030081195ea95a*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.34\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "349\t0.13\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "14660\t0.11\tray::IDLE\n",
      "14659\t0.11\tray::IDLE\n",
      "14658\t0.11\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-05-20 12:29:16,951 E 12008 12008] (raylet) node_manager.cc:3002: 35 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a, IP: 172.23.82.135) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.23.82.135`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-05-20 12:29:16,955 E 12008 12008] (raylet) worker_pool.cc:549: Some workers of the worker process(14872) have not registered within the timeout. The process is still alive, probably it's hanging during start.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2024-05-20 12:29:17,341\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00032\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffffb7f6cc3131144413b5c6666501000000, name=DQN.__init__, pid=14657, memory used=0.13GB) was running was 3.64GB / 3.70GB (0.98367), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3b2271dac21091d6983001aa33240ff840707146258a505605d2a336) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-3b2271dac21091d6983001aa33240ff840707146258a505605d2a336*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.34\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "349\t0.14\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "14659\t0.13\tray::IDLE\n",
      "14658\t0.13\tray::IDLE\n",
      "14657\t0.13\tray::IDLE\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:29:17,598\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00034\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: fffffffffffffffffc17478587930d416e409f8001000000, name=DQN.__init__, pid=14659, memory used=0.15GB) was running was 3.58GB / 3.70GB (0.967941), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c87917fea678f039572d39b725a37dde16cfe1309b4344d9c82ba21e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-c87917fea678f039572d39b725a37dde16cfe1309b4344d9c82ba21e*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.34\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "14658\t0.15\tray::IDLE\n",
      "14659\t0.15\tray::IDLE\n",
      "349\t0.13\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11515\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:29:18,150\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00033\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffffe574e418294c2bc063e3c95001000000, name=DQN.__init__, pid=14658, memory used=0.22GB) was running was 3.57GB / 3.70GB (0.962884), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd89cd4a9ec2509292d0b8ba237ecfd1deed65c8f68650154f471b35) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-dd89cd4a9ec2509292d0b8ba237ecfd1deed65c8f68650154f471b35*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.34\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "14658\t0.22\tray::IDLE\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "349\t0.13\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11515\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "11401\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:29:20,018\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00041\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffff45095c5697d069fa801b549d01000000, name=DQN.__init__, pid=15041, memory used=0.05GB) was running was 3.60GB / 3.70GB (0.971038), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0f8a2bef1d98d1d358e17fc10624d8b834e7296c30c7eaed1efafa2e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-0f8a2bef1d98d1d358e17fc10624d8b834e7296c30c7eaed1efafa2e*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.34\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "349\t0.12\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11515\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "11401\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11658\t0.10\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:29:20,399\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00040\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffffa328f53bcb8498f0180aee6501000000, name=DQN.__init__, pid=15040, memory used=0.06GB) was running was 3.61GB / 3.70GB (0.973849), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 62301fd6be3fdc9824207a80a9a4851b0b23e4f2905578862fd3c335) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-62301fd6be3fdc9824207a80a9a4851b0b23e4f2905578862fd3c335*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.34\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "14889\t0.16\tray::IDLE\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "349\t0.14\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11515\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "11401\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:29:20,739\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00042\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: fffffffffffffffff039b2f0c3721fc94dbacd8d01000000, name=DQN.__init__, pid=15075, memory used=0.05GB) was running was 3.62GB / 3.70GB (0.977436), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5ac416725f795919cf7e516914be2519bcae2579b4e1232b87e0cc1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-5ac416725f795919cf7e516914be2519bcae2579b4e1232b87e0cc1c*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.34\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "14889\t0.15\tray::IDLE\n",
      "349\t0.13\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11515\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "11401\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:29:21,351\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00044\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffff7752b7c3b6ca095e66c0b76401000000, name=DQN.__init__, pid=15078, memory used=0.05GB) was running was 3.66GB / 3.70GB (0.988062), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5ed3006e948cb28cec52c9d03d7353c0bc521968c4cf16b5397fa4c3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-5ed3006e948cb28cec52c9d03d7353c0bc521968c4cf16b5397fa4c3*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.34\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "14889\t0.15\tray::IDLE\n",
      "349\t0.13\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "15038\t0.11\tray::IDLE\n",
      "15039\t0.11\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:29:23,203\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00043\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffff4271c347b98f9794eedd101301000000, name=DQN.__init__, pid=15077, memory used=0.05GB) was running was 3.66GB / 3.70GB (0.988466), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 263f8c4c6e6a9ddf34695610cb2b0530fa63f9a3f77c6d71f30ca204) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-263f8c4c6e6a9ddf34695610cb2b0530fa63f9a3f77c6d71f30ca204*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.34\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "14889\t0.16\tray::IDLE\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "349\t0.13\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "15038\t0.12\tray::IDLE\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "15039\t0.12\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:29:25,664\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00038\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffff41d0c23a8c72c8b8742e8dba01000000, name=DQN.__init__, pid=15038, memory used=0.14GB) was running was 3.67GB / 3.70GB (0.989876), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4fd59ffe2d45b4eb742a44cca22c332f691d9011b7e8eb3b6f624283) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-4fd59ffe2d45b4eb742a44cca22c332f691d9011b7e8eb3b6f624283*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.34\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "14889\t0.17\tray::IDLE\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "15038\t0.14\tray::IDLE\n",
      "15039\t0.14\tray::IDLE\n",
      "349\t0.13\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:29:25,956\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00039\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: fffffffffffffffff9911b7a7b0322a03105372e01000000, name=DQN.__init__, pid=15039, memory used=0.16GB) was running was 3.63GB / 3.70GB (0.980479), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c26ed703f00a0f7520dcb0b7f0456fcab6194ac4e03996f612c421a6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-c26ed703f00a0f7520dcb0b7f0456fcab6194ac4e03996f612c421a6*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.34\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "14889\t0.18\tray::IDLE\n",
      "15039\t0.16\tray::IDLE\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "349\t0.14\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11515\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:29:26,244\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00037\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffff93051ccb164db7a370a0c6e701000000, name=DQN.__init__, pid=14889, memory used=0.19GB) was running was 3.55GB / 3.70GB (0.957898), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f1236a6353efda2ffc0194434f4b7057d63726e1581ec6619b24362f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-f1236a6353efda2ffc0194434f4b7057d63726e1581ec6619b24362f*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.34\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "14889\t0.19\tray::IDLE\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "349\t0.14\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11515\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "11401\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:29:28,473\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00046\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffff5a141517407cee6859bf0dbf01000000, name=DQN.__init__, pid=15507, memory used=0.04GB) was running was 3.62GB / 3.70GB (0.978436), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ddc276fe2a7cb2a67b89fbf659dedc0e9f088aaa9cc97581030ed882) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-ddc276fe2a7cb2a67b89fbf659dedc0e9f088aaa9cc97581030ed882*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.34\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "349\t0.14\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11515\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "11401\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11658\t0.10\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:29:28,777\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00050\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffff11294afca516fb94e8b1fff701000000, name=DQN.__init__, pid=15511, memory used=0.05GB) was running was 3.55GB / 3.70GB (0.957586), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2384e95c1faee5152086bf9098fa273a47d6332b871543fafe0477f7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-2384e95c1faee5152086bf9098fa273a47d6332b871543fafe0477f7*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.34\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "349\t0.14\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11515\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "11401\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11658\t0.10\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:29:29,081\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00052\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffff0dba5c6668382accfa3cc97301000000, name=DQN.__init__, pid=15513, memory used=0.06GB) was running was 3.60GB / 3.70GB (0.971552), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 799685b8681a374c06ac34e76523b75133ad16dae82827e52f41af1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-799685b8681a374c06ac34e76523b75133ad16dae82827e52f41af1c*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.34\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "349\t0.14\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11515\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "11401\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11658\t0.10\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:29:30,736\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00051\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffff947e59b7388631680a99a52c01000000, name=DQN.__init__, pid=15512, memory used=0.11GB) was running was 3.62GB / 3.70GB (0.97829), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f20b60ee2b50b7f9e77381481554c1cc1318c095c4009772e27e1e67) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-f20b60ee2b50b7f9e77381481554c1cc1318c095c4009772e27e1e67*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.34\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "349\t0.13\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11515\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "15510\t0.11\tray::IDLE\n",
      "15512\t0.11\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:29:30,756\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00047\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: DQN\n",
      "\tactor_id: b5b7fe6e58e5b275d09d562c01000000\n",
      "\tpid: 15508\n",
      "\tnamespace: bf3b2f4c-4ec3-4648-87da-595bb7e030f0\n",
      "\tip: 172.23.82.135\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "The actor never ran - it was cancelled before it started running.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffb5b7fe6e58e5b275d09d562c01000000 Worker ID: bffbf7a906f71c779b1e34320b890df41b35d5891194e541c0776d47 Node ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a Worker IP address: 172.23.82.135 Worker port: 35917 Worker PID: 15508 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-20 12:29:31,124\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00048\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffff99916c0b372aeb23713c2d6201000000, name=DQN.__init__, pid=15509, memory used=0.13GB) was running was 3.65GB / 3.70GB (0.985146), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 1e23c6bb06fe8a75601ffe8157531c5efd4a38253cb4d3d07346a267) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-1e23c6bb06fe8a75601ffe8157531c5efd4a38253cb4d3d07346a267*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.34\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "349\t0.14\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "15510\t0.13\tray::IDLE\n",
      "15506\t0.13\tray::IDLE\n",
      "15509\t0.13\tray::IDLE\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:29:31,346\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00049\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffff82927c02935685aa2ffe9ff501000000, name=DQN.__init__, pid=15510, memory used=0.15GB) was running was 3.62GB / 3.70GB (0.976928), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fc431e27a1360eb68b9cb5c4af7af7ae484322f50f07a83cd19be9f9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-fc431e27a1360eb68b9cb5c4af7af7ae484322f50f07a83cd19be9f9*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.34\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "15510\t0.15\tray::IDLE\n",
      "15506\t0.15\tray::IDLE\n",
      "349\t0.13\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11515\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:29:33,224\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00053\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffff13289f337340978dd941473f01000000, name=DQN.__init__, pid=15878, memory used=0.05GB) was running was 3.64GB / 3.70GB (0.983289), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e3e3dc917b61f78a7704284734752c4b1b99e74f5c0a4e8d93b00c85) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-e3e3dc917b61f78a7704284734752c4b1b99e74f5c0a4e8d93b00c85*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.34\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "15506\t0.23\tray::IDLE\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "349\t0.13\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11515\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "11401\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:29:33,856\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00054\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffffda4198d8b0ff14bb08c5edef01000000, name=DQN.__init__, pid=15897, memory used=0.07GB) was running was 3.60GB / 3.70GB (0.972455), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 542159281d9a08d2e790c8dc1262453e791cae9c97273cab210ffa81) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-542159281d9a08d2e790c8dc1262453e791cae9c97273cab210ffa81*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.34\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "15506\t0.24\tray::IDLE\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "349\t0.15\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11515\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "11401\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:29:34,166\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00045\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffff58360cf3caf8b4d0cef123df01000000, name=DQN.__init__, pid=15506, memory used=0.25GB) was running was 3.62GB / 3.70GB (0.977807), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a86dab2f6e17d2ca452c662e490660bc24b0c0123a3e9a1c6635293d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-a86dab2f6e17d2ca452c662e490660bc24b0c0123a3e9a1c6635293d*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.34\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "15506\t0.25\tray::IDLE\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "349\t0.15\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11515\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "11401\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:29:36,358\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00058\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffff3b28bb26affda7af4c9c718201000000, name=DQN.__init__, pid=16043, memory used=0.06GB) was running was 3.66GB / 3.70GB (0.988875), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 9491f24e388f17170e9cba836571da90d418c298a151592569dafb19) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-9491f24e388f17170e9cba836571da90d418c298a151592569dafb19*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.34\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "349\t0.14\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "15938\t0.11\tray::IDLE\n",
      "15937\t0.11\tray::IDLE\n",
      "15935\t0.11\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:29:38,861\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00056\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffff3ff97c77e7c00bde96cc026201000000, name=DQN.__init__, pid=15937, memory used=0.12GB) was running was 3.66GB / 3.70GB (0.988504), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 9f4efb0e703480790d2c0aed30fba7f988708ce6272c7ccc972f557f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-9f4efb0e703480790d2c0aed30fba7f988708ce6272c7ccc972f557f*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.34\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "349\t0.15\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "15938\t0.13\tray::IDLE\n",
      "15937\t0.12\tray::IDLE\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "15935\t0.12\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:29:39,146\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00057\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffff2f0211c5aff520e5728c102801000000, name=DQN.__init__, pid=15938, memory used=0.14GB) was running was 3.64GB / 3.70GB (0.981714), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4626ed11856d9caf0932e16949f4939bac64390d6f36d8d87fd8740a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-4626ed11856d9caf0932e16949f4939bac64390d6f36d8d87fd8740a*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.34\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "349\t0.15\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "15938\t0.14\tray::IDLE\n",
      "15935\t0.14\tray::IDLE\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11515\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-05-20 12:29:39,432\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_my_env_ca129_00055\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.23.82.135, ID: 710b910025519a0350de60aae0726b0cd1496778ebf0af8e8d6b654a) where the task (task ID: ffffffffffffffff0e8a3aaba23277dbf6c431b201000000, name=DQN.__init__, pid=15935, memory used=0.19GB) was running was 3.54GB / 3.70GB (0.956445), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 1e5e595d20a36f009b845c79581a96c2e5f9cebe2253f11a46b6af01) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.23.82.135`. To see the logs of the worker, use `ray logs worker-1e5e595d20a36f009b845c79581a96c2e5f9cebe2253f11a46b6af01*out -ip 172.23.82.135. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "856\t0.38\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -m ipykernel_launcher --f=/home/nicolo/.local/share/j...\n",
      "758\t0.34\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node /home/nicolo/.vscode-s...\n",
      "15935\t0.19\tray::IDLE\n",
      "11553\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11626\t0.16\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11483\t0.15\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "349\t0.15\t/home/nicolo/.vscode-server/bin/dc96b837cf6bb4af9cd736aa3af08cf8279f7685/node --dns-result-order=ipv...\n",
      "11718\t0.12\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "11515\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/bin/python -u /home/nicolo/anaconda3/envs/tianEnv/lib/python3.11...\n",
      "11401\t0.11\t/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.dqn.dqn import DQNConfig\n",
    "from ray.rllib.algorithms import DQN\n",
    "from ray.tune.registry import register_env\n",
    "from ray import air\n",
    "from ray import tune\n",
    "\n",
    "observations_memory = 2\n",
    "register_env(\"my_env\", lambda _: PointCoverageEnv({\"height\": 10, \"width\": 10, \"n_agents\": 2, \"n_targets\": 2, \"max_steps\": 100, \"observations_memory\": observations_memory}))\n",
    "\n",
    "replay_config = {\n",
    "        \"capacity\": 50000,\n",
    "    }\n",
    "\n",
    "config = (DQNConfig()\n",
    "    .training(\n",
    "        replay_buffer_config=replay_config,\n",
    "        gamma = tune.grid_search([0.90, 0.95, 0.99]),                # Discount factor for future rewards\n",
    "        lr = tune.grid_search([0.01, 0.001, 0.0005]),                 # Learning rate\n",
    "        train_batch_size = tune.grid_search([128, 256]),       # Batch size for training\n",
    "        dueling=tune.grid_search([True, False]),\n",
    "        double_q=tune.grid_search([True, False]),\n",
    "        #model={\"fcnet_hiddens\": [16], \"fcnet_activation\": \"relu\"},  # Model architecture\n",
    "        #adam_epsilon=0.5\n",
    "    )\n",
    "    .environment(\"my_env\")\n",
    ")\n",
    "\n",
    "tune.Tuner(\n",
    "    \"DQN\",\n",
    "    run_config=air.RunConfig(stop={\"training_iteration\":5}),\n",
    "    param_space=config.to_dict()\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-20 11:46:37,336\tWARNING deprecation.py:50 -- DeprecationWarning: `WorkerSet(num_workers=... OR local_worker=...)` has been deprecated. Use `EnvRunnerGroup(num_env_runners=... AND local_env_runner=...)` instead. This will raise an error in the future!\n",
      "2024-05-20 11:46:37,400\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] mean_reward: -144.6, mean_len: 99.2\n",
      "[1] mean_reward: -150.52631578947367, mean_len: 100.05263157894737\n",
      "[2] mean_reward: -140.29032258064515, mean_len: 96.16129032258064\n",
      "[3] mean_reward: -135.0952380952381, mean_len: 94.30952380952381\n",
      "[4] mean_reward: -134.78846153846155, mean_len: 95.4423076923077\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.dqn.dqn import DQNConfig\n",
    "from ray.rllib.algorithms import DQN\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "observations_memory = 2\n",
    "register_env(\"my_env\", lambda _: PointCoverageEnv({\"height\": 10, \"width\": 10, \"n_agents\": 2, \"n_targets\": 2, \"max_steps\": 100, \"observations_memory\": observations_memory}))\n",
    "\n",
    "replay_config = {\n",
    "        \"capacity\": 50000,\n",
    "    }\n",
    "\n",
    "config = (DQNConfig()\n",
    "    .training(\n",
    "        replay_buffer_config=replay_config,\n",
    "        gamma=0.90,                # Discount factor for future rewards\n",
    "        lr=0.001,                 # Learning rate\n",
    "        train_batch_size=64,       # Batch size for training\n",
    "        #model={\"fcnet_hiddens\": [16], \"fcnet_activation\": \"relu\"},  # Model architecture\n",
    "        #dueling=True,              # Use dueling DQN\n",
    "        #double_q=True,             # Use double Q-learning\n",
    "        #adam_epsilon=0.5\n",
    "        )\n",
    "    .environment(\"my_env\")\n",
    ")\n",
    "config.sample_timeout_s *= 5 \n",
    "\n",
    "algoDQN = config.build()\n",
    "\n",
    "for i in range(5):\n",
    "    result = algoDQN.train()\n",
    "    print(f\"[{i}] mean_reward: {result['sampler_results']['episode_reward_mean']}, mean_len: {result['sampler_results']['episode_len_mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19]\n",
      "____________\n",
      "|      x   |\n",
      "|          |\n",
      "|          |\n",
      "|     o    |\n",
      "|       x  |\n",
      "|          |\n",
      "|          |\n",
      "|          |\n",
      "|   o      |\n",
      "|          |\n",
      "‾‾‾‾‾‾‾‾‾‾‾‾\n",
      "{'agent-0': array([0.3, 0.8, 0.5, 0.3, 0.7, 0.4, 0.6, 0. , 0.3, 0.9, 0.5, 0.2, 0.7,\n",
      "       0.4, 0.6, 0. ], dtype=float32), 'agent-1': array([0.5, 0.3, 0.3, 0.8, 0.7, 0.4, 0.6, 0. , 0.5, 0.2, 0.3, 0.9, 0.7,\n",
      "       0.4, 0.6, 0. ], dtype=float32)}\n",
      "{'agent-0': -1, 'agent-1': -1}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[147], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(obs)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(reward)\n\u001b[0;32m---> 18\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__all__\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m truncated[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__all__\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "observations_memory = 2\n",
    "env = PointCoverageEnv({\"height\": 10, \"width\": 10, \"n_agents\": 2, \"n_targets\": 2, \"observations_memory\":observations_memory})\n",
    "obs, _ = env.reset()\n",
    "env.render()\n",
    "\n",
    "for i in range(100):\n",
    "    actions = algoDQN.compute_actions(obs)\n",
    "    print(actions, \"\\n\")\n",
    "    obs, reward, terminated, truncated, info = env.step(actions)\n",
    "    clear_output()\n",
    "    print(f\"[{i}]\")\n",
    "    env.render()\n",
    "    print(obs)\n",
    "    print(reward)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    if terminated['__all__'] or truncated['__all__']:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tianshou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PointCoverageEnv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mPointCoverageEnv\u001b[49m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m10\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m10\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_agents\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_targets\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m100\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_nested_observation\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m})\n\u001b[1;32m      2\u001b[0m obs, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(obs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PointCoverageEnv' is not defined"
     ]
    }
   ],
   "source": [
    "env = PointCoverageEnv({\"height\": 10, \"width\": 10, \"n_agents\": 2, \"n_targets\": 2, \"max_steps\": 100, \"use_nested_observation\": False})\n",
    "obs, _ = env.reset()\n",
    "print(obs)\n",
    "print(Batch(obs)) \n",
    "print(env.observation_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'agent_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 52\u001b[0m\n\u001b[1;32m     34\u001b[0m train_collector \u001b[38;5;241m=\u001b[39m Collector(\n\u001b[1;32m     35\u001b[0m     policy\u001b[38;5;241m=\u001b[39mmapolicy_manager,\n\u001b[1;32m     36\u001b[0m     env\u001b[38;5;241m=\u001b[39mtrain_envs,\n\u001b[1;32m     37\u001b[0m     buffer\u001b[38;5;241m=\u001b[39mVectorReplayBuffer(\u001b[38;5;241m20000\u001b[39m, \u001b[38;5;28mlen\u001b[39m(train_envs)),\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     39\u001b[0m test_collector \u001b[38;5;241m=\u001b[39m Collector(policy\u001b[38;5;241m=\u001b[39mmapolicy_manager, env\u001b[38;5;241m=\u001b[39mtest_envs)\n\u001b[1;32m     41\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mOnpolicyTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmapolicy_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_collector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_collector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_collector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_collector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepeat_per_collect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepisode_per_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_per_collect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmean_reward\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean_reward\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m195\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m---> 52\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m result\u001b[38;5;241m.\u001b[39mpprint_asdict()\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/tianshou/trainer/base.py:557\u001b[0m, in \u001b[0;36mBaseTrainer.run\u001b[0;34m(self, reset_prior_to_run)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Consume iterator.\u001b[39;00m\n\u001b[1;32m    552\u001b[0m \n\u001b[1;32m    553\u001b[0m \u001b[38;5;124;03mSee itertools - recipes. Use functions that consume iterators at C speed\u001b[39;00m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;124;03m(feed the entire iterator into a zero-length deque).\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reset_prior_to_run:\n\u001b[0;32m--> 557\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/tianshou/trainer/base.py:261\u001b[0m, in \u001b[0;36mBaseTrainer.reset\u001b[0;34m(self, reset_collectors, reset_buffer)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_per_test \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_collector, AsyncCollector)  \u001b[38;5;66;03m# Issue 700\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m test_result \u001b[38;5;241m=\u001b[39m \u001b[43mtest_episode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_collector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepisode_per_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreward_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m test_result\u001b[38;5;241m.\u001b[39mreturns_stat \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# for mypy\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_epoch\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/tianshou/trainer/utils.py:30\u001b[0m, in \u001b[0;36mtest_episode\u001b[0;34m(collector, test_fn, epoch, n_episode, logger, global_step, reward_metric)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m test_fn:\n\u001b[1;32m     29\u001b[0m     test_fn(epoch, global_step)\n\u001b[0;32m---> 30\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mcollector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_episode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_episode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reward_metric:  \u001b[38;5;66;03m# TODO: move into collector\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     rew \u001b[38;5;241m=\u001b[39m reward_metric(result\u001b[38;5;241m.\u001b[39mreturns)\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/tianshou/data/collector.py:304\u001b[0m, in \u001b[0;36mBaseCollector.collect\u001b[0;34m(self, n_step, n_episode, random, render, reset_before_collect, gym_reset_kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset(reset_buffer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, gym_reset_kwargs\u001b[38;5;241m=\u001b[39mgym_reset_kwargs)\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch_train_mode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_episode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_episode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgym_reset_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgym_reset_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/tianshou/data/collector.py:507\u001b[0m, in \u001b[0;36mCollector._collect\u001b[0;34m(self, n_step, n_episode, random, render, gym_reset_kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m last_hidden_state_RH \u001b[38;5;241m=\u001b[39m _nullable_slice(\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre_collect_hidden_state_RH,\n\u001b[1;32m    489\u001b[0m     ready_env_ids_R,\n\u001b[1;32m    490\u001b[0m )\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;66;03m# todo check if we need this when using cur_rollout_batch\u001b[39;00m\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;66;03m# if len(cur_rollout_batch) != len(ready_env_ids):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    500\u001b[0m \n\u001b[1;32m    501\u001b[0m     \u001b[38;5;66;03m# get the next action\u001b[39;00m\n\u001b[1;32m    502\u001b[0m     (\n\u001b[1;32m    503\u001b[0m         act_RA,\n\u001b[1;32m    504\u001b[0m         act_normalized_RA,\n\u001b[1;32m    505\u001b[0m         policy_R,\n\u001b[1;32m    506\u001b[0m         hidden_state_RH,\n\u001b[0;32m--> 507\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_action_policy_hidden\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mready_env_ids_R\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mready_env_ids_R\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlast_obs_RO\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_obs_RO\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlast_info_R\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_info_R\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlast_hidden_state_RH\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_hidden_state_RH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    515\u001b[0m     obs_next_RO, rew_R, terminated_R, truncated_R, info_R \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(\n\u001b[1;32m    516\u001b[0m         act_normalized_RA,\n\u001b[1;32m    517\u001b[0m         ready_env_ids_R,\n\u001b[1;32m    518\u001b[0m     )\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(info_R, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# type: ignore[unreachable]\u001b[39;00m\n\u001b[1;32m    520\u001b[0m         \u001b[38;5;66;03m# This can happen if the env is an envpool env. Then the info returned by step is a dict\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/tianshou/data/collector.py:419\u001b[0m, in \u001b[0;36mCollector._compute_action_policy_hidden\u001b[0;34m(self, random, ready_env_ids_R, last_obs_RO, last_info_R, last_hidden_state_RH)\u001b[0m\n\u001b[1;32m    416\u001b[0m info_batch \u001b[38;5;241m=\u001b[39m _HACKY_create_info_batch(last_info_R)\n\u001b[1;32m    417\u001b[0m obs_batch_R \u001b[38;5;241m=\u001b[39m cast(ObsBatchProtocol, Batch(obs\u001b[38;5;241m=\u001b[39mlast_obs_RO, info\u001b[38;5;241m=\u001b[39minfo_batch))\n\u001b[0;32m--> 419\u001b[0m act_batch_RA \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobs_batch_R\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlast_hidden_state_RH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    424\u001b[0m act_RA \u001b[38;5;241m=\u001b[39m to_numpy(act_batch_RA\u001b[38;5;241m.\u001b[39mact)\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexploration_noise:\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/tianshou/policy/multiagent/mapolicy.py:222\u001b[0m, in \u001b[0;36mMultiAgentPolicyManager.forward\u001b[0;34m(self, batch, state, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m results: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mbool\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, Batch, np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m|\u001b[39m Batch, Batch]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent_id, policy \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicies\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# This part of code is difficult to understand.\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# Let's follow an example with two agents\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# agent_index for agent 2 is [1, 3, 5]\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# we separate the transition of each agent according to agent_id\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m     agent_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnonzero(\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_id\u001b[49m \u001b[38;5;241m==\u001b[39m agent_id)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(agent_index) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;66;03m# (has_data, agent_index, out, act, state)\u001b[39;00m\n\u001b[1;32m    225\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;28;01mFalse\u001b[39;00m, np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), Batch(), Batch(), Batch()))\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/tianshou/data/batch.py:474\u001b[0m, in \u001b[0;36mBatch.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    473\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return self.key. The \"Any\" return type is needed for mypy.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m, key)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'agent_id'"
     ]
    }
   ],
   "source": [
    "train_size, test_size = (20, 10)\n",
    "device = \"cpu\"\n",
    "\n",
    "env = PointCoverageEnv({\"height\": 10, \"width\": 10, \"n_agents\": 2, \"n_targets\": 2, \"max_steps\": 100, \"use_nested_observation\": False})\n",
    "train_envs = DummyVectorEnv([lambda: PointCoverageEnv({\"height\": 10, \"width\": 10, \"n_agents\": 2, \"n_targets\": 2, \"max_steps\": 100, \"use_nested_observation\": False}) for _ in range(train_size)])\n",
    "test_envs = DummyVectorEnv([lambda: PointCoverageEnv({\"height\": 10, \"width\": 10, \"n_agents\": 2, \"n_targets\": 2, \"max_steps\": 100, \"use_nested_observation\": False}) for _ in range(test_size)])\n",
    "\n",
    "assert env.observation_space.shape is not None\n",
    "assert isinstance(env.action_space, Discrete) \n",
    "\n",
    "net = Net(state_shape=env.observation_space.shape, hidden_sizes=[64, 64], device=device)\n",
    "actor = Actor(preprocess_net=net, action_shape=env.action_space.n, device=device).to(device)\n",
    "critic = Critic(preprocess_net=net, device=device).to(device)\n",
    "actor_critic = ActorCritic(actor=actor, critic=critic)\n",
    "\n",
    "# optimizer of the actor and the critic\n",
    "optim = torch.optim.Adam(actor_critic.parameters(), lr=0.0003)\n",
    "\n",
    "dist = torch.distributions.Categorical\n",
    "policy: BasePolicy\n",
    "policy = PPOPolicy(\n",
    "    actor=actor,\n",
    "    critic=critic,\n",
    "    optim=optim,\n",
    "    dist_fn=dist,\n",
    "    action_space=env.action_space,\n",
    "    deterministic_eval=True,\n",
    "    action_scaling=False,\n",
    ")\n",
    "\n",
    "mapolicy_manager = MultiAgentPolicyManager(policies=[policy, policy], env=env)\n",
    "\n",
    "\n",
    "train_collector = Collector(\n",
    "    policy=mapolicy_manager,\n",
    "    env=train_envs,\n",
    "    buffer=VectorReplayBuffer(20000, len(train_envs)),\n",
    ")\n",
    "test_collector = Collector(policy=mapolicy_manager, env=test_envs)\n",
    "\n",
    "result = OnpolicyTrainer(\n",
    "    policy=mapolicy_manager,\n",
    "    train_collector=train_collector,\n",
    "    test_collector=test_collector,\n",
    "    max_epoch=10,\n",
    "    step_per_epoch=50000,\n",
    "    repeat_per_collect=10,\n",
    "    episode_per_test=10,\n",
    "    batch_size=256,\n",
    "    step_per_collect=2000,\n",
    "    stop_fn=lambda mean_reward: mean_reward >= 195,\n",
    ").run()\n",
    "\n",
    "result.pprint_asdict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rayEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
